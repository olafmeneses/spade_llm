{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"SPADE_LLM : SPADE with Large Language Models","text":"<p>Extension for SPADE that integrates Large Language Models into multi-agent systems.</p>"},{"location":"#features","title":"Features","text":"<ul> <li>Multi-Provider Support: OpenAI, Ollama, LM Studio, vLLM</li> <li>Tool System: Function calling with async execution</li> <li>Context Management: Multi-conversation support with automatic cleanup</li> <li>Message Routing: Conditional routing based on LLM responses</li> <li>Guardrails System: Content filtering and safety controls for input/output</li> <li>MCP Integration: Model Context Protocol server support</li> <li>Production Ready: Comprehensive error handling and logging</li> </ul>"},{"location":"#quick-start","title":"Quick Start","text":"<pre><code>import spade\nfrom spade_llm import LLMAgent, LLMProvider\n\nasync def main():\n    provider = LLMProvider.create_openai(\n        api_key=\"your-api-key\",\n        model=\"gpt-4o-mini\"\n    )\n\n    agent = LLMAgent(\n        jid=\"assistant@example.com\",\n        password=\"password\",\n        provider=provider,\n        system_prompt=\"You are a helpful assistant\"\n    )\n\n    await agent.start()\n\nif __name__ == \"__main__\":\n    spade.run(main())\n</code></pre>"},{"location":"#architecture","title":"Architecture","text":"<pre><code>graph TB\n    A[LLMAgent] --&gt; B[LLMBehaviour]\n    B --&gt; C[ContextManager]\n    B --&gt; D[LLMProvider]\n    B --&gt; E[LLMTool]\n    B --&gt; I[Guardrails System]\n\n    D --&gt; F[OpenAI/Ollama/etc]\n    E --&gt; G[Python Functions]\n    E --&gt; H[MCP Servers]\n    I --&gt; J[Input Filters]\n    I --&gt; K[Output Filters]</code></pre>"},{"location":"#documentation-structure","title":"Documentation Structure","text":""},{"location":"#getting-started","title":"Getting Started","text":"<ul> <li>Installation - Setup and requirements</li> <li>Quick Start - Basic usage examples</li> </ul>"},{"location":"#core-guides","title":"Core Guides","text":"<ul> <li>Architecture - SPADE_LLM general structure</li> <li>Providers - LLM provider configuration</li> <li>Tools System - Function calling capabilities</li> <li>Guardrails - Content filtering and safety controls</li> <li>Message Routing - Conditional message routing</li> <li>Context Management - Conversation handling</li> </ul>"},{"location":"#reference","title":"Reference","text":"<ul> <li>API Reference - Complete API documentation</li> <li>Examples - Working code examples</li> </ul>"},{"location":"#examples","title":"Examples","text":"<p>Explore the examples directory for complete working examples:</p> <ul> <li><code>multi_provider_chat_example.py</code> - Chat with different LLM providers</li> <li><code>ollama_with_tools_example.py</code> - Local models with tool calling</li> <li><code>guardrails_example.py</code> - Content filtering and safety controls</li> <li><code>langchain_tools_example.py</code> - LangChain tool integration</li> <li><code>valencia_multiagent_trip_planner.py</code> - Multi-agent workflow</li> </ul>"},{"location":"contributing/","title":"Contributing","text":"<p>We welcome contributions to SPADE_LLM! This guide will help you get started.</p>"},{"location":"contributing/#quick-start","title":"Quick Start","text":"<ol> <li>Fork the repository on GitHub</li> <li>Clone your fork locally</li> <li>Create a branch for your changes</li> <li>Make your changes and test them</li> <li>Submit a pull request</li> </ol>"},{"location":"contributing/#development-setup","title":"Development Setup","text":""},{"location":"contributing/#prerequisites","title":"Prerequisites","text":"<ul> <li>Python 3.10+</li> <li>Git</li> <li>Virtual environment tool (venv, conda, etc.)</li> <li>A XMPP server connection </li> </ul>"},{"location":"contributing/#installation","title":"Installation","text":"<pre><code># Clone your fork\ngit clone https://github.com/your-username/spade_llm.git\ncd spade_llm\n\n# Create virtual environment\npython -m venv .venv\nsource .venv/bin/activate  # On Windows: .venv\\Scripts\\activate\n\n# Install in development mode\npip install -e \".[dev]\"\n\n# Install pre-commit hooks\npre-commit install\n</code></pre>"},{"location":"contributing/#running-tests","title":"Running Tests","text":"<pre><code># Run all tests\npytest\n\n# Run with coverage\npytest --cov=spade_llm\n\n# Run specific test file\npytest tests/test_agent/test_llm_agent.py\n\n# Run with verbose output\npytest -v\n</code></pre>"},{"location":"contributing/#types-of-contributions","title":"Types of Contributions","text":""},{"location":"contributing/#bug-reports","title":"Bug Reports","text":"<p>When reporting bugs, please include:</p> <ul> <li>Clear description of the problem</li> <li>Steps to reproduce the issue</li> <li>Expected vs actual behavior</li> <li>Environment details (Python version, OS, etc.)</li> <li>Minimal code example that reproduces the bug</li> </ul>"},{"location":"contributing/#feature-requests","title":"Feature Requests","text":"<p>For new features, please:</p> <ul> <li>Check existing issues to avoid duplicates</li> <li>Describe the use case and motivation</li> <li>Propose implementation approach if possible</li> <li>Consider backward compatibility</li> </ul>"},{"location":"contributing/#submitting-changes","title":"Submitting Changes","text":""},{"location":"contributing/#pull-request-process","title":"Pull Request Process","text":"<ol> <li> <p>Create a feature branch <pre><code>git checkout -b feature/your-feature-name\n</code></pre></p> </li> <li> <p>Make your changes</p> </li> <li>Write code</li> <li>Add tests</li> <li> <p>Update documentation</p> </li> <li> <p>Test your changes <pre><code>pytest\nflake8 spade_llm tests\nmypy spade_llm\n</code></pre></p> </li> <li> <p>Commit with clear messages <pre><code>git add .\ngit commit -m \"Add feature: clear description of changes\"\n</code></pre></p> </li> <li> <p>Push to your fork <pre><code>git push origin feature/your-feature-name\n</code></pre></p> </li> <li> <p>Create pull request on GitHub</p> </li> </ol>"},{"location":"contributing/#pull-request-guidelines","title":"Pull Request Guidelines","text":"<ul> <li>Clear title describing the change</li> <li>Detailed description of what and why</li> <li>Link related issues using keywords (fixes #123)</li> <li>Include screenshots for UI changes</li> <li>Check that CI passes</li> </ul>"},{"location":"contributing/#development-guidelines","title":"Development Guidelines","text":""},{"location":"contributing/#architecture-principles","title":"Architecture Principles","text":"<ul> <li>Extend, don't replace SPADE functionality</li> <li>Maintain compatibility with existing SPADE agents</li> <li>Use async/await throughout</li> <li>Keep interfaces simple and consistent</li> <li>Favor composition over inheritance</li> </ul>"},{"location":"contributing/#code-organization","title":"Code Organization","text":"<pre><code>spade_llm/\n\u251c\u2500\u2500 agent/          # Agent classes\n\u251c\u2500\u2500 behaviour/      # Behaviour implementations\n\u251c\u2500\u2500 context/        # Context management\n\u251c\u2500\u2500 providers/      # LLM provider interfaces\n\u251c\u2500\u2500 tools/          # Tool system\n\u251c\u2500\u2500 routing/        # Message routing\n\u251c\u2500\u2500 mcp/           # MCP integration\n\u2514\u2500\u2500 utils/         # Utility functions\n</code></pre>"},{"location":"contributing/#release-process","title":"Release Process","text":""},{"location":"contributing/#version-numbering","title":"Version Numbering","text":"<p>We use semantic versioning (semver):</p> <ul> <li>Major (X.0.0) - Breaking changes</li> <li>Minor (0.X.0) - New features, backward compatible</li> <li>Patch (0.0.X) - Bug fixes, backward compatible</li> </ul>"},{"location":"contributing/#release-checklist","title":"Release Checklist","text":"<ul> <li>[ ] All tests pass</li> <li>[ ] Documentation updated</li> <li>[ ] Changelog updated</li> <li>[ ] Version bumped</li> <li>[ ] Release notes prepared</li> <li>[ ] Tagged release created</li> </ul>"},{"location":"contributing/#community","title":"Community","text":""},{"location":"contributing/#getting-help","title":"Getting Help","text":"<ul> <li>GitHub Issues - Bug reports and feature requests</li> <li>Discussions - General questions and ideas</li> <li>Documentation - Comprehensive guides and API reference</li> </ul>"},{"location":"contributing/#code-of-conduct","title":"Code of Conduct","text":"<ul> <li>Be respectful and inclusive</li> <li>Help others learn and contribute</li> <li>Focus on constructive feedback</li> <li>Follow project guidelines and standards</li> </ul>"},{"location":"contributing/#recognition","title":"Recognition","text":"<p>Contributors are recognized in:</p> <ul> <li>CONTRIBUTORS.md file</li> <li>Release notes for significant contributions</li> <li>Documentation acknowledgments</li> </ul> <p>Thank you for contributing to SPADE_LLM! \ud83d\ude80</p>"},{"location":"contributing/development/","title":"Development Guide","text":"<p>Detailed guide for SPADE_LLM development and testing.</p>"},{"location":"contributing/development/#development-environment","title":"Development Environment","text":""},{"location":"contributing/development/#setup","title":"Setup","text":"<pre><code># Clone repository\ngit clone https://github.com/sosanzma/spade_llm.git\ncd spade_llm\n\n# Create virtual environment\npython -m venv .venv\nsource .venv/bin/activate\n\n# Install development dependencies\npip install -e \".[dev,docs]\"\n\n# Setup pre-commit hooks\npre-commit install\n</code></pre>"},{"location":"contributing/development/#project-structure","title":"Project Structure","text":"<pre><code>spade_llm/\n\u251c\u2500\u2500 spade_llm/          # Main package\n\u2502   \u251c\u2500\u2500 agent/          # Agent implementations\n\u2502   \u251c\u2500\u2500 behaviour/      # Behaviour classes\n\u2502   \u251c\u2500\u2500 context/        # Context management\n\u2502   \u251c\u2500\u2500 providers/      # LLM providers\n\u2502   \u251c\u2500\u2500 tools/          # Tool system\n\u2502   \u251c\u2500\u2500 routing/        # Message routing\n\u2502   \u251c\u2500\u2500 mcp/           # MCP integration\n\u2502   \u2514\u2500\u2500 utils/         # Utilities\n\u251c\u2500\u2500 tests/             # Test suite\n\u251c\u2500\u2500 examples/          # Usage examples\n\u251c\u2500\u2500 docs/             # Documentation\n\u2514\u2500\u2500 requirements*.txt  # Dependencies\n</code></pre>"},{"location":"contributing/development/#testing","title":"Testing","text":""},{"location":"contributing/development/#test-organization","title":"Test Organization","text":"<pre><code>tests/\n\u251c\u2500\u2500 test_agent/        # Agent tests\n\u251c\u2500\u2500 test_behaviour/    # Behaviour tests\n\u251c\u2500\u2500 test_context/      # Context tests\n\u251c\u2500\u2500 test_providers/    # Provider tests\n\u251c\u2500\u2500 test_tools/        # Tool tests\n\u251c\u2500\u2500 test_routing/      # Routing tests\n\u251c\u2500\u2500 conftest.py        # Test configuration\n\u2514\u2500\u2500 factories.py       # Test factories\n</code></pre>"},{"location":"contributing/development/#running-tests","title":"Running Tests","text":"<pre><code># Run all tests\npytest\n\n# Run with coverage\npytest --cov=spade_llm --cov-report=html\n\n# Run specific test module\npytest tests/test_agent/\n\n# Run specific test\npytest tests/test_agent/test_llm_agent.py::test_agent_creation\n\n# Run with verbose output\npytest -v -s\n</code></pre>"},{"location":"contributing/development/#documentation-standards","title":"Documentation Standards","text":""},{"location":"contributing/development/#docstring-format","title":"Docstring Format","text":"<pre><code>def example_function(param1: str, param2: int = 0) -&gt; str:\n    \"\"\"Brief description of the function.\n\n    Longer description if needed. Explain the purpose,\n    behavior, and any important details.\n\n    Args:\n        param1: Description of first parameter\n        param2: Description of second parameter with default value\n\n    Returns:\n        Description of return value\n\n    Raises:\n        ValueError: When invalid input is provided\n        ConnectionError: When service is unavailable\n\n    Example:\n        ```python\n        result = example_function(\"hello\", 42)\n        print(result)  # Output: processed result\n        ```\n    \"\"\"\n    # Implementation here\n    pass\n</code></pre>"},{"location":"contributing/development/#class-documentation","title":"Class Documentation","text":"<pre><code>class ExampleClass:\n    \"\"\"Brief description of the class.\n\n    Longer description explaining the class purpose,\n    usage patterns, and important behavior.\n\n    Attributes:\n        attribute1: Description of attribute\n        attribute2: Description of another attribute\n\n    Example:\n        ```python\n        instance = ExampleClass(param=\"value\")\n        result = instance.method()\n        ```\n    \"\"\"\n\n    def __init__(self, param: str):\n        \"\"\"Initialize the class.\n\n        Args:\n            param: Configuration parameter\n        \"\"\"\n        self.attribute1 = param\n</code></pre> <p>This development guide should help you contribute effectively to SPADE_LLM. For specific questions, check the existing issues or create a new one.</p>"},{"location":"getting-started/","title":"Getting Started","text":"<p>Learn how to install and use SPADE_LLM to create LLM-powered agents.</p>"},{"location":"getting-started/#prerequisites","title":"Prerequisites","text":"<ul> <li>Python 3.10+</li> <li>Basic knowledge of Python async/await</li> <li>XMPP server</li> <li>Access to an LLM provider (OpenAI API key or local model)</li> </ul>"},{"location":"getting-started/#learning-path","title":"Learning Path","text":"<ol> <li>Installation - Install SPADE_LLM and dependencies</li> <li>Quick Start - Create your first agent</li> <li>First Agent - Complete tutorial</li> </ol>"},{"location":"getting-started/#need-help","title":"Need Help?","text":"<ul> <li>Check the API Reference for detailed documentation</li> <li>Browse Examples for working code</li> <li>Open an issue on GitHub</li> </ul>"},{"location":"getting-started/first-agent/","title":"Your First Agent","text":"<p>Step-by-step tutorial for creating a complete LLM-powered agent.</p>"},{"location":"getting-started/first-agent/#prerequisites","title":"Prerequisites","text":"<ul> <li>SPADE_LLM installed</li> <li>OpenAI API key or local model running</li> <li>XMPP server access</li> </ul>"},{"location":"getting-started/first-agent/#step-1-basic-agent","title":"Step 1: Basic Agent","text":"<p>Create <code>my_agent.py</code>:</p> <pre><code>import spade\nfrom spade_llm import LLMAgent, LLMProvider\n\nasync def main():\n    # Configure LLM provider\n    provider = LLMProvider.create_openai(\n        api_key=\"your-api-key\",\n        model=\"gpt-4o-mini\",\n        temperature=0.7\n    )\n\n    # Create agent\n    agent = LLMAgent(\n        jid=\"myagent@jabber.at\", \n        password=\"mypassword\",\n        provider=provider,\n        system_prompt=\"You are a helpful coding assistant\"\n    )\n\n    await agent.start()\n    print(\"Agent started successfully!\")\n\n    # Keep running\n    import asyncio\n    await asyncio.sleep(60)\n\n    await agent.stop()\n    print(\"Agent stopped\")\n\nif __name__ == \"__main__\":\n    spade.run(main())\n</code></pre> <p>Run with: <code>python my_agent.py</code></p>"},{"location":"getting-started/first-agent/#step-2-add-tools","title":"Step 2: Add Tools","text":"<p>Add function calling capabilities:</p> <pre><code>from spade_llm import LLMTool\nfrom datetime import datetime\n\n# Define tool function\nasync def get_current_time() -&gt; str:\n    \"\"\"Get the current date and time.\"\"\"\n    return datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n\n# Create tool\ntime_tool = LLMTool(\n    name=\"get_current_time\",\n    description=\"Get current date and time\",\n    parameters={\"type\": \"object\", \"properties\": {}, \"required\": []},\n    func=get_current_time\n)\n\n# Add to agent\nagent = LLMAgent(\n    jid=\"myagent@jabber.at\",\n    password=\"mypassword\", \n    provider=provider,\n    system_prompt=\"You are a helpful assistant with access to time information\",\n    tools=[time_tool]  # Add tools here\n)\n</code></pre>"},{"location":"getting-started/first-agent/#step-3-interactive-chat","title":"Step 3: Interactive Chat","text":"<p>Create interactive chat interface:</p> <pre><code>from spade_llm import ChatAgent\n\nasync def main():\n    # LLM Agent\n    llm_agent = LLMAgent(\n        jid=\"assistant@jabber.at\",\n        password=\"password1\",\n        provider=provider,\n        system_prompt=\"You are a helpful assistant\"\n    )\n\n    # Chat Agent for human interaction\n    chat_agent = ChatAgent(\n        jid=\"human@jabber.at\",\n        password=\"password2\", \n        target_agent_jid=\"assistant@jabber.at\"\n    )\n\n    await llm_agent.start()\n    await chat_agent.start()\n\n    print(\"Chat system ready! Type messages below.\")\n    print(\"Type 'exit' to quit.\")\n\n    # Start interactive chat\n    await chat_agent.run_interactive()\n\n    await chat_agent.stop()\n    await llm_agent.stop()\n\nif __name__ == \"__main__\":\n    spade.run(main())\n</code></pre>"},{"location":"getting-started/first-agent/#step-4-conversation-management","title":"Step 4: Conversation Management","text":"<p>Add conversation limits and callbacks:</p> <pre><code>def on_conversation_end(conversation_id: str, reason: str):\n    print(f\"Conversation {conversation_id} ended: {reason}\")\n\nagent = LLMAgent(\n    jid=\"assistant@jabber.at\",\n    password=\"password\",\n    provider=provider,\n    system_prompt=\"You are a helpful assistant\",\n    max_interactions_per_conversation=10,\n    termination_markers=[\"&lt;DONE&gt;\", \"goodbye\"],\n    on_conversation_end=on_conversation_end\n)\n</code></pre>"},{"location":"getting-started/first-agent/#step-5-message-routing","title":"Step 5: Message Routing","text":"<p>Route responses to different recipients:</p> <pre><code>def my_router(msg, response, context):\n    \"\"\"Route based on response content.\"\"\"\n    if \"technical\" in response.lower():\n        return \"tech-support@jabber.at\"\n    elif \"sales\" in response.lower():\n        return \"sales@jabber.at\"\n    else:\n        return str(msg.sender)  # Reply to sender\n\nagent = LLMAgent(\n    jid=\"router@jabber.at\",\n    password=\"password\",\n    provider=provider,\n    routing_function=my_router\n)\n</code></pre>"},{"location":"getting-started/first-agent/#step-6-adding-safety-with-guardrails","title":"Step 6: Adding Safety with Guardrails","text":"<p>Protect your agent with content filtering and safety controls:</p> <pre><code>from spade_llm.guardrails import KeywordGuardrail, GuardrailAction\n\n# Create content filter\nsafety_filter = KeywordGuardrail(\n    name=\"basic_safety\",\n    blocked_keywords=[\"hack\", \"exploit\", \"malware\", \"virus\"],\n    action=GuardrailAction.BLOCK,\n    blocked_message=\"I cannot help with potentially harmful activities.\"\n)\n\n# Profanity filter\nprofanity_filter = KeywordGuardrail(\n    name=\"profanity_filter\",\n    blocked_keywords=[\"damn\", \"hell\", \"stupid\"],\n    action=GuardrailAction.MODIFY,\n    replacement=\"[FILTERED]\"\n)\n\n# Add to your agent\nagent = LLMAgent(\n    jid=\"safe-assistant@jabber.at\",\n    password=\"password\",\n    provider=provider,\n    system_prompt=\"You are a helpful and safe assistant\",\n    input_guardrails=[safety_filter, profanity_filter]  # Filter incoming messages\n)\n</code></pre> <p>Guardrail monitoring: <pre><code>def on_guardrail_trigger(result):\n    \"\"\"Monitor guardrail activity.\"\"\"\n    if result.action == GuardrailAction.BLOCK:\n        print(f\"\ud83d\udeab Blocked: {result.reason}\")\n    elif result.action == GuardrailAction.MODIFY:\n        print(f\"\u270f\ufe0f Modified: {result.reason}\")\n\nagent = LLMAgent(\n    jid=\"monitored-assistant@jabber.at\",\n    password=\"password\",\n    provider=provider,\n    input_guardrails=[safety_filter],\n    on_guardrail_trigger=on_guardrail_trigger  # Monitor events\n)\n</code></pre></p>"},{"location":"getting-started/first-agent/#complete-example","title":"Complete Example","text":"<p>Here's a full-featured agent combining all concepts:</p> <pre><code>import spade\nfrom spade_llm import LLMAgent, ChatAgent, LLMProvider, LLMTool\nfrom datetime import datetime\n\n# Tool function\nasync def get_time() -&gt; str:\n    \"\"\"Get current time.\"\"\"\n    return datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n\n# Routing function\ndef smart_router(msg, response, context):\n    \"\"\"Route based on content.\"\"\"\n    if \"time\" in response.lower():\n        return \"time-service@jabber.at\"\n    return str(msg.sender)\n\n# Conversation callback\ndef on_end(conv_id: str, reason: str):\n    print(f\"Conversation ended: {conv_id} ({reason})\")\n\nasync def main():\n    # Create provider\n    provider = LLMProvider.create_openai(\n        api_key=\"your-api-key\",\n        model=\"gpt-4o-mini\"\n    )\n\n    # Create tool\n    time_tool = LLMTool(\n        name=\"get_time\",\n        description=\"Get current time\",\n        parameters={\"type\": \"object\", \"properties\": {}, \"required\": []},\n        func=get_time\n    )\n\n    # Create LLM agent\n    llm_agent = LLMAgent(\n        jid=\"smart-assistant@jabber.at\",\n        password=\"password1\",\n        provider=provider,\n        system_prompt=\"You are a smart assistant with time access\",\n        tools=[time_tool],\n        routing_function=smart_router,\n        max_interactions_per_conversation=20,\n        on_conversation_end=on_end\n    )\n\n    # Create chat agent\n    chat_agent = ChatAgent(\n        jid=\"human@jabber.at\",\n        password=\"password2\",\n        target_agent_jid=\"smart-assistant@jabber.at\"\n    )\n\n    # Start agents\n    await llm_agent.start()\n    await chat_agent.start()\n\n    print(\"Smart assistant ready!\")\n    print(\"Try asking: 'What time is it?' or 'Help me with Python'\")\n\n    # Interactive chat\n    await chat_agent.run_interactive()\n\n    # Cleanup\n    await chat_agent.stop()\n    await llm_agent.stop()\n\nif __name__ == \"__main__\":\n    spade.run(main())\n</code></pre>"},{"location":"getting-started/first-agent/#testing-your-agent","title":"Testing Your Agent","text":"<ol> <li>Run the agent: <code>python my_agent.py</code></li> <li>Send test messages using an XMPP client</li> <li>Check agent responses and tool execution</li> <li>Monitor conversation limits and termination</li> </ol>"},{"location":"getting-started/first-agent/#troubleshooting","title":"Troubleshooting","text":"<p>Agent won't start: - Check XMPP credentials - Verify server connectivity - Try <code>verify_security=False</code> for development</p> <p>No LLM responses: - Verify API key - Check provider configuration - Test with simple queries first</p> <p>Tools not working: - Ensure tool functions are async - Check parameter schema - Verify tool registration</p>"},{"location":"getting-started/first-agent/#next-steps","title":"Next Steps","text":"<ul> <li>Tools System - Advanced tool development</li> <li>Message Routing - Complex routing patterns</li> <li>Examples - More complete examples</li> <li>API Reference - Detailed documentation</li> </ul>"},{"location":"getting-started/installation/","title":"Installation","text":""},{"location":"getting-started/installation/#requirements","title":"Requirements","text":"<ul> <li>Python 3.10+</li> <li>4GB+ RAM (8GB+ for local models)</li> </ul>"},{"location":"getting-started/installation/#install","title":"Install","text":"<pre><code>pip install spade_llm\n</code></pre>"},{"location":"getting-started/installation/#verify-installation","title":"Verify Installation","text":"<pre><code>import spade_llm\nfrom spade_llm import LLMAgent, LLMProvider\n\nprint(f\"SPADE_LLM version: {spade_llm.__version__}\")\n</code></pre>"},{"location":"getting-started/installation/#llm-provider-setup","title":"LLM Provider Setup","text":"<p>Choose one provider:</p>"},{"location":"getting-started/installation/#openai","title":"OpenAI","text":"<pre><code>export OPENAI_API_KEY=\"your-api-key\"\n</code></pre>"},{"location":"getting-started/installation/#ollama-local","title":"Ollama (Local)","text":"<pre><code># Install Ollama\ncurl -fsSL https://ollama.ai/install.sh | sh\n\n# Download a model\nollama pull llama3.1:8b\nollama serve\n</code></pre>"},{"location":"getting-started/installation/#lm-studio-local","title":"LM Studio (Local)","text":"<ol> <li>Download LM Studio</li> <li>Download a model through the GUI</li> <li>Start the local server</li> </ol>"},{"location":"getting-started/installation/#xmpp-server","title":"XMPP Server","text":"<p>For development, use a public XMPP server like <code>jabber.at</code> or set up Prosody locally.</p>"},{"location":"getting-started/installation/#development-install","title":"Development Install","text":"<pre><code>git clone https://github.com/sosanzma/spade_llm.git\ncd spade_llm\npip install -e \".[dev]\"\n</code></pre>"},{"location":"getting-started/installation/#troubleshooting","title":"Troubleshooting","text":"<p>Import errors: Ensure you're in the correct Python environment <pre><code>python -m pip install spade_llm\n</code></pre></p> <p>SSL errors: For development only, disable SSL verification: <pre><code>agent = LLMAgent(..., verify_security=False)\n</code></pre></p> <p>Ollama connection: Check if Ollama is running: <pre><code>curl http://localhost:11434/v1/models\n</code></pre></p>"},{"location":"getting-started/quickstart/","title":"Quick Start","text":"<p>Get your first SPADE_LLM agent running in minutes.</p>"},{"location":"getting-started/quickstart/#setup","title":"Setup","text":""},{"location":"getting-started/quickstart/#1-install","title":"1. Install","text":"<pre><code>pip install spade_llm\n</code></pre>"},{"location":"getting-started/quickstart/#2-get-llm-access","title":"2. Get LLM Access","text":"<p>OpenAI (easiest): <pre><code>export OPENAI_API_KEY=\"your-api-key\"\n</code></pre></p> <p>Ollama (free): <pre><code>ollama pull llama3.1:8b\nollama serve\n</code></pre></p>"},{"location":"getting-started/quickstart/#3-create-agent","title":"3. Create Agent","text":"<pre><code>import spade\nfrom spade_llm import LLMAgent, LLMProvider\n\nasync def main():\n    # Configure provider\n    provider = LLMProvider.create_openai(\n        api_key=\"your-api-key\",\n        model=\"gpt-4o-mini\"\n    )\n\n    # Create agent\n    agent = LLMAgent(\n        jid=\"assistant@jabber.at\",\n        password=\"your-password\",\n        provider=provider,\n        system_prompt=\"You are a helpful assistant\"\n    )\n\n    await agent.start()\n    print(\"Agent started!\")\n\n    # Keep running\n    import asyncio\n    await asyncio.sleep(60)\n    await agent.stop()\n\nif __name__ == \"__main__\":\n    spade.run(main())\n</code></pre>"},{"location":"getting-started/quickstart/#4-run","title":"4. Run","text":"<pre><code>python my_agent.py\n</code></pre>"},{"location":"getting-started/quickstart/#alternative-providers","title":"Alternative Providers","text":""},{"location":"getting-started/quickstart/#ollama","title":"Ollama","text":"<pre><code>provider = LLMProvider.create_ollama(\n    model=\"llama3.1:8b\"\n)\n</code></pre>"},{"location":"getting-started/quickstart/#lm-studio","title":"LM Studio","text":"<pre><code>provider = LLMProvider.create_lm_studio(\n    model=\"local-model\",\n    base_url=\"http://localhost:1234/v1\"\n)\n</code></pre>"},{"location":"getting-started/quickstart/#chat-example","title":"Chat Example","text":"<p>Interactive chat agent:</p> <pre><code>import spade\nfrom spade_llm import LLMAgent, ChatAgent, LLMProvider\n\nasync def main():\n    # LLM Agent\n    provider = LLMProvider.create_openai(\n        api_key=\"your-key\",\n        model=\"gpt-4o-mini\"\n    )\n\n    llm_agent = LLMAgent(\n        jid=\"assistant@jabber.at\",\n        password=\"password1\",\n        provider=provider\n    )\n\n    # Chat Agent (for human interaction)\n    chat_agent = ChatAgent(\n        jid=\"human@jabber.at\", \n        password=\"password2\",\n        target_agent_jid=\"assistant@jabber.at\"\n    )\n\n    await llm_agent.start()\n    await chat_agent.start()\n\n    # Start interactive chat\n    await chat_agent.run_interactive()\n\n    await chat_agent.stop()\n    await llm_agent.stop()\n\nif __name__ == \"__main__\":\n    spade.run(main())\n</code></pre>"},{"location":"getting-started/quickstart/#next-steps","title":"Next Steps","text":"<ul> <li>First Agent Tutorial - Detailed walkthrough</li> <li>Providers Guide - LLM provider configuration</li> <li>Tools System - Add function calling</li> <li>Examples - Complete working examples</li> </ul>"},{"location":"guides/","title":"Guides","text":"<p>Comprehensive guides for SPADE_LLM features and concepts.</p>"},{"location":"guides/#core-concepts","title":"Core Concepts","text":"<ul> <li>Architecture - System components and design</li> <li>Providers - LLM provider configuration and usage</li> <li>Tools System - Function calling and tool integration</li> <li>Context Management - Advanced context control and memory optimization</li> <li>Memory System - Agent memory and learning capabilities</li> <li>Conversations - Conversation lifecycle and management</li> <li>MCP - Model context protocol integration</li> <li>Human-in-the-Loop - Human expert consultation and workflows</li> <li>Guardrails - Content filtering and safety controls</li> <li>Routing - Message routing and multi-agent workflows</li> </ul>"},{"location":"guides/#usage-patterns","title":"Usage Patterns","text":"<p>Each guide covers: - Core concepts and configuration - Common usage patterns - Best practices and troubleshooting - Complete code examples</p>"},{"location":"guides/#next-steps","title":"Next Steps","text":"<ul> <li>API Reference - Detailed API documentation</li> <li>Examples - Working code examples</li> </ul>"},{"location":"guides/architecture/","title":"Architecture","text":"<p>SPADE_LLM extends SPADE's multi-agent framework with LLM capabilities while maintaining full compatibility.</p>"},{"location":"guides/architecture/#component-overview","title":"Component Overview","text":"<pre><code>graph TB\n    A[LLMAgent] --&gt; B[LLMBehaviour]\n    B --&gt; C[ContextManager]\n    B --&gt; D[LLMProvider]\n    B --&gt; E[LLMTool]\n    B --&gt; I[Guardrails System]\n\n    D --&gt; F[OpenAI/Ollama/etc]\n    E --&gt; G[Python Functions]\n    E --&gt; H[MCP Servers]\n    I --&gt; J[Input Filters]\n    I --&gt; K[Output Filters]</code></pre>"},{"location":"guides/architecture/#core-components","title":"\ud83c\udfd7\ufe0f Core Components","text":""},{"location":"guides/architecture/#llmagent","title":"\ud83e\udd16 LLMAgent","text":"<p>The main agent class that extends SPADE's <code>Agent</code> with LLM capabilities:</p> <ul> <li>Manages LLM provider connection and configuration</li> <li>Registers tools and handles their lifecycle</li> <li>Controls conversation limits and termination conditions</li> <li>Provides the bridge between SPADE's XMPP messaging and LLM processing</li> </ul>"},{"location":"guides/architecture/#llmbehaviour","title":"\u26a1 LLMBehaviour","text":"<p>The core processing engine that orchestrates the entire LLM workflow:</p> <ol> <li>Receives XMPP messages from other agents</li> <li>Updates conversation context with new information</li> <li>Calls LLM provider for intelligent responses</li> <li>Executes tools when requested by the LLM</li> <li>Routes responses to appropriate recipients</li> </ol> <p>This is where the magic happens - transforming simple messages into intelligent interactions.</p>"},{"location":"guides/architecture/#contextmanager","title":"\ud83e\udde0 ContextManager","text":"<p>Manages conversation state across multiple concurrent discussions:</p> <ul> <li>Tracks multiple conversations simultaneously by thread ID</li> <li>Formats messages appropriately for different LLM providers</li> <li>Handles context windowing to manage token limits efficiently</li> <li>Ensures each conversation maintains its own context and history</li> </ul>"},{"location":"guides/architecture/#llmprovider","title":"\ud83d\udd0c LLMProvider","text":"<p>Unified interface for different LLM services, providing consistency:</p> <ul> <li>Abstracts provider-specific APIs (OpenAI, Ollama, Anthropic, etc.)</li> <li>Handles tool calling formats across different providers</li> <li>Provides consistent error handling and retry mechanisms</li> <li>Makes it easy to switch between different LLM services</li> </ul>"},{"location":"guides/architecture/#llmtool","title":"\ud83d\udee0\ufe0f LLMTool","text":"<p>Framework for executable functions that extend LLM capabilities:</p> <ul> <li>Async execution support for non-blocking operations</li> <li>JSON Schema parameter validation for type safety</li> <li>Integration with LangChain and MCP for ecosystem compatibility</li> <li>Enables LLMs to perform real actions beyond conversation</li> </ul>"},{"location":"guides/architecture/#message-flow","title":"\ud83d\udce8 Message Flow","text":"<pre><code>sequenceDiagram\n    participant A as External Agent\n    participant B as LLMBehaviour\n    participant C as LLMProvider\n    participant D as LLM Service\n    participant E as LLMTool\n\n    A-&gt;&gt;B: XMPP Message\n    B-&gt;&gt;C: Get Response\n    C-&gt;&gt;D: API Call\n    D-&gt;&gt;C: Tool Calls\n    C-&gt;&gt;B: Tool Requests\n    loop For Each Tool\n        B-&gt;&gt;E: Execute\n        E-&gt;&gt;B: Result\n    end\n    B-&gt;&gt;C: Get Final Response\n    C-&gt;&gt;D: API Call\n    D-&gt;&gt;C: Final Response\n    B-&gt;&gt;A: Response Message</code></pre>"},{"location":"guides/architecture/#conversation-lifecycle","title":"\ud83d\udd04 Conversation Lifecycle","text":"<p>The conversation lifecycle follows a well-defined process:</p> <ol> <li>Initialization: New conversation created from message thread</li> <li>Processing: Messages processed through LLM with tool execution</li> <li>Termination: Ends via markers, limits, or manual control</li> <li>Cleanup: Resources freed and callbacks executed</li> </ol> <p>Each stage is designed to be robust and efficient, ensuring conversations can handle complex, multi-turn interactions while maintaining system stability.</p>"},{"location":"guides/architecture/#integration-points","title":"\ud83d\udd27 Integration Points","text":"<p>The architecture provides multiple integration points for customization:</p> <ul> <li>Custom Providers: Add new LLM services</li> <li>Tool Extensions: Create domain-specific tools</li> <li>Routing Logic: Implement custom message routing</li> <li>Context Management: Customize conversation handling</li> <li>MCP Integration: Connect to external servers</li> </ul> <p>This flexible design ensures SPADE_LLM can adapt to various use cases while maintaining its core multi-agent capabilities.</p>"},{"location":"guides/architecture/#next-steps","title":"Next Steps","text":"<ul> <li>Providers - Configure LLM providers</li> <li>Tools System - Add tool capabilities</li> <li>Routing - Implement message routing</li> <li>MCP - Connect to external services</li> </ul>"},{"location":"guides/context-management/","title":"Context Management","text":"<p>SPADE_LLM provides sophisticated context management to control conversation memory and optimize LLM performance across multi-turn interactions.</p>"},{"location":"guides/context-management/#overview","title":"Overview","text":"<p>The context management system handles how conversation history is maintained and filtered:</p> <ul> <li>Memory Control: Prevents token overflow while preserving important information</li> <li>Multi-Strategy Support: Choose from basic, windowed, or intelligent strategies</li> <li>Tool-Aware: Maintains critical tool execution context</li> <li>Conversation Isolation: Each conversation maintains separate context</li> </ul>"},{"location":"guides/context-management/#context-strategies","title":"Context Strategies","text":""},{"location":"guides/context-management/#nocontextmanagement-default","title":"NoContextManagement (Default)","text":"<p>Behavior: Preserves all messages without any filtering or limitations.</p> <pre><code>from spade_llm.context import NoContextManagement\n\n# Keep all conversation history\ncontext = NoContextManagement()\n\nagent = LLMAgent(\n    jid=\"assistant@example.com\",\n    password=\"password\",\n    provider=provider,\n    context_management=context\n)\n</code></pre> <p>Characteristics: - \u2705 Preserves complete conversation history - \u2705 No context loss - \u274c Unlimited memory growth - \u274c Potential LLM context overflow</p> <p>Use Cases: - Short conversations (&lt; 10 exchanges) - Debugging sessions requiring complete history - Post-conversation analysis</p>"},{"location":"guides/context-management/#windowsizecontext-basic","title":"WindowSizeContext (Basic)","text":"<p>Behavior: Implements a sliding window maintaining only the last N messages.</p> <pre><code>from spade_llm.context import WindowSizeContext\n\n# Keep last 20 messages\ncontext = WindowSizeContext(max_messages=20)\n\nagent = LLMAgent(\n    jid=\"assistant@example.com\",\n    password=\"password\",\n    provider=provider,\n    context_management=context\n)\n</code></pre> <p>Visual Example:</p> <p></p> <p>Basic sliding window keeps only the most recent N messages, dropping older ones as new messages arrive.</p> <p>Characteristics: - \u2705 Predictable memory control - \u2705 Prevents context overflow - \u274c Loses important initial context - \u274c No message type differentiation</p> <p>Use Cases: - Long conversations with memory constraints - Resource-limited environments - Continuous monitoring sessions</p>"},{"location":"guides/context-management/#smartwindowsizecontext-advanced","title":"SmartWindowSizeContext (Advanced) \ud83c\udd95","text":"<p>Behavior: Intelligent management combining sliding window with selective retention of critical messages.</p>"},{"location":"guides/context-management/#basic-configuration","title":"Basic Configuration","text":"<pre><code>from spade_llm.context import SmartWindowSizeContext\n\n# Standard behavior (equivalent to WindowSizeContext)\nbasic_context = SmartWindowSizeContext(max_messages=20)\n\n# With initial message preservation\ninitial_preserve = SmartWindowSizeContext(\n    max_messages=20,\n    preserve_initial=3\n)\n\n# With tool prioritization\ntool_priority = SmartWindowSizeContext(\n    max_messages=20,\n    prioritize_tools=True\n)\n\n# Full configuration\nsmart_context = SmartWindowSizeContext(\n    max_messages=20,\n    preserve_initial=3,\n    prioritize_tools=True\n)\n</code></pre>"},{"location":"guides/context-management/#configuration-parameters","title":"Configuration Parameters","text":"Parameter Type Default Description <code>max_messages</code> <code>int</code> 20 Maximum messages in context <code>preserve_initial</code> <code>int</code> 0 Number of initial messages to always preserve <code>prioritize_tools</code> <code>bool</code> False Whether to prioritize tool results"},{"location":"guides/context-management/#intelligent-retention-algorithm","title":"Intelligent Retention Algorithm","text":"<p>The SmartWindowSizeContext uses a sophisticated algorithm:</p> <ol> <li>If <code>total_messages \u2264 max_messages</code> \u2192 Return all messages</li> <li>If <code>preserve_initial = 0</code> and <code>prioritize_tools = False</code> \u2192 Basic sliding window</li> <li>If <code>preserve_initial &gt; 0</code> \u2192 Preserve initial messages + fill with recent ones</li> <li>If <code>prioritize_tools = True</code> \u2192 Prioritize tool results + fill remaining space</li> <li>If both enabled \u2192 Combine preservation + prioritization strategies</li> </ol>"},{"location":"guides/context-management/#advanced-features","title":"Advanced Features","text":""},{"location":"guides/context-management/#initial-message-preservation","title":"Initial Message Preservation","text":"<pre><code># Example: 30 total messages, window=10, preserve_initial=3\n# Result: [msg1, msg2, msg3] + [msg24, msg25, ..., msg30]\n\ncontext = SmartWindowSizeContext(\n    max_messages=10, \n    preserve_initial=3\n)\n</code></pre> <p>Visual Example:</p> <p></p> <p>Smart window preserves the first N messages (objectives, instructions) while filling remaining space with recent messages.</p> <p>Benefits: Preserves conversation objectives and fundamental context.</p>"},{"location":"guides/context-management/#tool-result-prioritization","title":"Tool Result Prioritization","text":"<pre><code># Prioritizes all messages with role=\"tool\"\ncontext = SmartWindowSizeContext(\n    max_messages=15, \n    prioritize_tools=True\n)\n</code></pre> <p>Algorithm: 1. Extract all tool result messages 2. If tool messages \u2265 max_messages \u2192 Keep recent tool messages 3. Otherwise \u2192 tool messages + recent messages to fill window 4. Reorder chronologically</p>"},{"location":"guides/context-management/#tool-callresult-pair-preservation","title":"Tool Call/Result Pair Preservation","text":"<p>The system automatically detects and preserves tool call/result pairs:</p> <pre><code># Automatically preserves:\n# Assistant: \"I'll check the weather\" [tool_calls: get_weather]\n# Tool: \"22\u00b0C, sunny\" [tool_call_id: matching_id]\n\ncontext = SmartWindowSizeContext(\n    max_messages=20,\n    prioritize_tools=True\n)\n</code></pre> <p>Visual Example:</p> <p></p> <p>Smart window with initial preservation + tool prioritization maintains both conversation objectives and critical tool execution context.</p> <p>Benefits: Maintains execution context for complex tool workflows.</p>"},{"location":"guides/context-management/#integration-with-llmagent","title":"Integration with LLMAgent","text":""},{"location":"guides/context-management/#constructor-configuration","title":"Constructor Configuration","text":"<pre><code>from spade_llm.agent import LLMAgent\nfrom spade_llm.context import SmartWindowSizeContext\n\n# Create context strategy\nsmart_context = SmartWindowSizeContext(\n    max_messages=20,\n    preserve_initial=3,\n    prioritize_tools=True\n)\n\n# Integrate with agent\nagent = LLMAgent(\n    jid=\"agent@example.com\",\n    password=\"password\",\n    provider=provider,\n    context_management=smart_context,\n    system_prompt=\"You are an assistant with intelligent context management...\"\n)\n</code></pre>"},{"location":"guides/context-management/#context-statistics","title":"Context Statistics","text":""},{"location":"guides/context-management/#getting-statistics","title":"Getting Statistics","text":"<pre><code>context = SmartWindowSizeContext(\n    max_messages=20,\n    preserve_initial=3,\n    prioritize_tools=True\n)\n\n# Get stats for current conversation\nstats = context.get_stats(total_messages=50)\n</code></pre>"},{"location":"guides/context-management/#statistics-format","title":"Statistics Format","text":"<pre><code>{\n    \"strategy\": \"smart_window_size\",\n    \"max_messages\": 20,\n    \"preserve_initial\": 3,\n    \"prioritize_tools\": True,\n    \"total_messages\": 50,\n    \"messages_in_context\": 20,\n    \"messages_dropped\": 30\n}\n</code></pre>"},{"location":"guides/context-management/#strategy-comparison","title":"Strategy Comparison","text":"Feature NoContext WindowSize SmartWindowSize Memory Control \u274c \u2705 \u2705 Preserves Initial Context \u2705 \u274c \u2705 (optional) Tool Prioritization \u2705 \u274c \u2705 (optional) Configuration Complexity None Low Medium Performance O(1) O(1) O(n log n) Memory Usage Unlimited Limited Limited"},{"location":"guides/context-management/#complete-example","title":"Complete Example","text":"<pre><code>import asyncio\nfrom spade_llm.agent import LLMAgent\nfrom spade_llm.context import SmartWindowSizeContext\nfrom spade_llm.providers import LLMProvider\n\nasync def main():\n    # Configure intelligent context management\n    smart_context = SmartWindowSizeContext(\n        max_messages=20,\n        preserve_initial=3,\n        prioritize_tools=True\n    )\n\n    # Create LLM provider\n    provider = LLMProvider.create_openai(\n        api_key=\"your-api-key\",\n        model=\"gpt-4\"\n    )\n\n    # Create agent with smart context\n    agent = LLMAgent(\n        jid=\"smart_agent@example.com\",\n        password=\"password\",\n        provider=provider,\n        context_management=smart_context,\n        system_prompt=\"You are an intelligent assistant with context management.\"\n    )\n\n    await agent.start()\n\n    # Monitor context during conversation\n    while True:\n        # ... agent processes messages ...\n\n        # Check context stats periodically\n        stats = agent.get_context_stats()\n        if stats['messages_in_context'] &gt; 15:\n            print(\"Context approaching limit\")\n\n        await asyncio.sleep(1)\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n</code></pre>"},{"location":"guides/context-management/#next-steps","title":"Next Steps","text":"<ul> <li>Conversations - Learn about conversation lifecycle</li> <li>Memory System - Explore agent memory capabilities</li> <li>Tools System - Add tool capabilities</li> <li>API Reference - Detailed API documentation</li> </ul>"},{"location":"guides/conversations/","title":"Conversation Management","text":"<p>SPADE_LLM automatically manages conversation context across multi-turn interactions, enabling intelligent, stateful dialogues between agents.</p>"},{"location":"guides/conversations/#conversation-flow","title":"Conversation Flow","text":"<pre><code>graph TD\n    A[New Message Arrives] --&gt; B{Conversation Exists?}\n    B --&gt;|No| C[Create New Conversation]\n    B --&gt;|Yes| D[Load Existing Context]\n    C --&gt; E[Initialize Context with System Prompt]\n    D --&gt; F[Add Message to Context]\n    E --&gt; F\n    F --&gt; G[Check Interaction Count]\n    G --&gt; H{Max Interactions Reached?}\n    H --&gt;|Yes| I[Mark for Termination]\n    H --&gt;|No| J[Process with LLM]\n    J --&gt; K[Check Response for Termination Markers]\n    K --&gt; L{Termination Marker Found?}\n    L --&gt;|Yes| M[End Conversation]\n    L --&gt;|No| N[Send Response]\n    I --&gt; M\n    M --&gt; O[Execute Cleanup Callbacks]\n    N --&gt; P[Update Context]\n    P --&gt; Q[Continue Conversation]</code></pre>"},{"location":"guides/conversations/#overview","title":"Overview","text":"<p>The conversation system provides comprehensive management for multi-agent interactions:</p> <ul> <li>Multi-turn Context: Maintains complete conversation history across interactions</li> <li>Concurrent Conversations: Supports multiple simultaneous conversations per agent  </li> <li>Automatic Lifecycle: Manages memory and conversation cleanup efficiently</li> <li>Flexible Termination: Controls interaction limits and ending conditions</li> </ul>"},{"location":"guides/conversations/#basic-configuration","title":"Basic Configuration","text":"<p>Conversation management is automatic. Each message thread creates a separate conversation with configurable parameters:</p> <pre><code>from spade_llm import LLMAgent, LLMProvider\n\nagent = LLMAgent(\n    jid=\"assistant@example.com\",\n    password=\"password\",\n    provider=provider,\n    system_prompt=\"You are a helpful assistant\",\n    max_interactions_per_conversation=10,\n    termination_markers=[\"&lt;DONE&gt;\", \"&lt;END&gt;\"]\n)\n</code></pre>"},{"location":"guides/conversations/#conversation-lifecycle","title":"Conversation Lifecycle","text":"<p>The conversation lifecycle follows a structured process:</p> <ol> <li>Initialization: New conversation created from message thread ID</li> <li>Processing: Messages added to context, LLM generates responses</li> <li>Monitoring: System tracks interactions and checks termination conditions</li> <li>Termination: Conversation ends through various mechanisms</li> <li>Cleanup: Resources freed and callbacks executed</li> </ol>"},{"location":"guides/conversations/#lifecycle-states","title":"Lifecycle States","text":"<ul> <li>Active: Conversation is processing messages normally</li> <li>Approaching Limit: Near maximum interaction count</li> <li>Terminated: Conversation has ended and is being cleaned up</li> <li>Cleanup Complete: All resources have been freed</li> </ul>"},{"location":"guides/conversations/#termination-markers","title":"Termination Markers","text":"<p>Termination markers provide a way for LLMs to explicitly signal when a conversation should end. This enables intelligent conversation closure based on content rather than just interaction counts.</p>"},{"location":"guides/conversations/#how-termination-markers-work","title":"How Termination Markers Work","text":"<p>When the LLM includes a termination marker in its response:</p> <ol> <li>Detection: The system scans the LLM response for configured markers</li> <li>Immediate Termination: Conversation is marked for closure regardless of interaction count</li> <li>Response Processing: The marker is typically removed from the final response sent to users</li> <li>Cleanup Execution: Standard termination procedures are triggered</li> </ol>"},{"location":"guides/conversations/#configuration-examples","title":"Configuration Examples","text":"<pre><code># Single termination marker\nagent = LLMAgent(\n    jid=\"assistant@example.com\",\n    password=\"password\",\n    provider=provider,\n    termination_markers=[\"&lt;CONVERSATION_END&gt;\"]\n)\n\n# Multiple termination markers\nagent = LLMAgent(\n    jid=\"assistant@example.com\", \n    password=\"password\",\n    provider=provider,\n    termination_markers=[\"&lt;DONE&gt;\", \"&lt;END&gt;\", \"&lt;GOODBYE&gt;\", \"&lt;TERMINATE&gt;\"]\n)\n</code></pre>"},{"location":"guides/conversations/#llm-integration","title":"LLM Integration","text":"<p>Train your LLM to use termination markers appropriately:</p> <pre><code>system_prompt = \"\"\"\nYou are a helpful assistant. When a conversation naturally comes to an end \nor the user says goodbye, include &lt;DONE&gt; at the end of your response to \nproperly close the conversation.\n\nExample:\nUser: \"Thanks for your help, goodbye!\"\nAssistant: \"You're welcome! Have a great day! &lt;DONE&gt;\"\n\"\"\"\n\nagent = LLMAgent(\n    jid=\"assistant@example.com\",\n    password=\"password\",\n    provider=provider,\n    system_prompt=system_prompt,\n    termination_markers=[\"&lt;DONE&gt;\"]\n)\n</code></pre>"},{"location":"guides/conversations/#termination-marker-best-practices","title":"Termination Marker Best Practices","text":"<ul> <li>Choose Unique Markers: Use markers unlikely to appear in normal conversation</li> <li>Document for LLMs: Clearly instruct the LLM when and how to use markers</li> <li>Multiple Options: Provide several markers for different termination scenarios</li> <li>Consistent Format: Use consistent marker syntax across your system</li> </ul>"},{"location":"guides/conversations/#context-management","title":"Context Management","text":"<p>SPADE_LLM provides sophisticated context management to control conversation memory and optimize performance. See the Context Management Guide for detailed information.</p>"},{"location":"guides/conversations/#accessing-conversation-state","title":"Accessing Conversation State","text":"<p>Programmatic access to conversation information:</p> <pre><code># Get conversation state\nconversation_id = \"user1_session\"\nstate = agent.get_conversation_state(conversation_id)\n\n# Check conversation status\nis_active = state.get(\"active\", False)\ninteraction_count = state.get(\"interaction_count\", 0)\nmax_interactions = state.get(\"max_interactions\", 10)\n\n# Reset conversation (removes limits)\nsuccess = agent.reset_conversation(conversation_id)\n</code></pre>"},{"location":"guides/conversations/#context-strategy-configuration","title":"Context Strategy Configuration","text":"<p>Configure different context management strategies:</p> <pre><code>from spade_llm.context import SmartWindowSizeContext\n\n# Intelligent context management\nsmart_context = SmartWindowSizeContext(\n    max_messages=20,\n    preserve_initial=3,\n    prioritize_tools=True\n)\n\n# Use with agent\nagent = LLMAgent(\n    jid=\"coder@example.com\",\n    password=\"password\",\n    provider=provider,\n    context_management=smart_context\n)\n</code></pre>"},{"location":"guides/conversations/#conversation-control","title":"Conversation Control","text":""},{"location":"guides/conversations/#termination-callbacks","title":"Termination Callbacks","text":"<p>Handle conversation endings with custom logic:</p> <pre><code>def on_conversation_end(conversation_id: str, reason: str):\n    \"\"\"Called when conversation terminates.\"\"\"\n    print(f\"Conversation {conversation_id} ended: {reason}\")\n\n    # Possible reasons:\n    # - \"max_interactions_reached\"\n    # - \"termination_marker_found\" \n    # - \"manual_termination\"\n\n    # Custom cleanup logic\n    save_conversation_log(conversation_id)\n    send_completion_notification(conversation_id)\n\nagent = LLMAgent(\n    jid=\"assistant@example.com\",\n    password=\"password\", \n    provider=provider,\n    max_interactions_per_conversation=5,\n    on_conversation_end=on_conversation_end\n)\n</code></pre>"},{"location":"guides/conversations/#best-practices","title":"Best Practices","text":""},{"location":"guides/conversations/#context-design","title":"Context Design","text":"<ul> <li>Use clear, focused system prompts that explain the agent's role</li> <li>Set appropriate interaction limits based on expected conversation length</li> <li>Implement proper cleanup callbacks for resource management</li> <li>Handle termination gracefully with user-friendly messages</li> </ul>"},{"location":"guides/conversations/#performance-optimization","title":"Performance Optimization","text":"<ul> <li>Monitor memory usage for long-running conversations</li> <li>Implement context compression when approaching token limits</li> <li>Use conversation limits to prevent runaway interactions</li> <li>Clean up inactive conversations regularly to free resources</li> </ul>"},{"location":"guides/conversations/#termination-strategy","title":"Termination Strategy","text":"<ul> <li>Train LLMs to recognize natural conversation endings</li> <li>Use multiple termination markers for different scenarios</li> <li>Combine markers with interaction limits for robust termination</li> <li>Test termination behavior thoroughly in different conversation contexts</li> </ul>"},{"location":"guides/conversations/#next-steps","title":"Next Steps","text":"<ul> <li>Context Management - Advanced context control strategies</li> <li>Memory System - Agent memory and learning capabilities</li> <li>Tools System - Add capabilities to conversations</li> <li>Message Routing - Control conversation flow between agents</li> <li>Architecture - Understanding conversation management internals</li> <li>Providers - Configure LLM providers for conversations</li> </ul>"},{"location":"guides/guardrails/","title":"Guardrails System","text":"<p>Protect your agents with configurable content filtering and safety controls.</p>"},{"location":"guides/guardrails/#content-flow","title":"Content Flow","text":"<pre><code>flowchart TD\n    A[Input Message] --&gt; B[Input Guardrails]\n    B --&gt;|Pass| C[LLM Processing]  \n    B --&gt;|Block| D[Send Block Response]\n    B --&gt;|Modify| E[Process Modified Content]\n    E --&gt; C\n    C --&gt; F[LLM Response]\n    F --&gt; G[Output Guardrails]\n    G --&gt;|Pass| H[Send Response]\n    G --&gt;|Block| I[Send Safe Response]\n    G --&gt;|Modify| J[Send Modified Response]</code></pre>"},{"location":"guides/guardrails/#overview","title":"Overview","text":"<p>The Guardrails System provides multi-layer content protection for your LLM agents. It enables you to:</p> <ul> <li>\ud83d\udee1\ufe0f Filter harmful content before it reaches the LLM</li> <li>\ud83d\udd0d Validate LLM responses before sending to users</li> <li>\u270f\ufe0f Automatically modify inappropriate content</li> <li>\ud83d\udcca Monitor and log security events</li> <li>\ud83d\udd17 Chain multiple filters in sequence</li> </ul>"},{"location":"guides/guardrails/#how-guardrails-work","title":"How Guardrails Work","text":"<p>Guardrails operate at two critical points in the agent workflow:</p> <ol> <li>Input Guardrails: Process incoming messages before LLM processing</li> <li>Output Guardrails: Validate LLM responses before sending to users</li> </ol> <p>Each guardrail can take one of four actions:</p> <ul> <li>PASS: Allow content without changes</li> <li>MODIFY: Transform content and continue processing</li> <li>BLOCK: Stop processing and send rejection message</li> <li>WARNING: Log concern but allow content</li> </ul>"},{"location":"guides/guardrails/#basic-usage","title":"Basic Usage","text":"<pre><code>from spade_llm import LLMAgent, LLMProvider\nfrom spade_llm.guardrails import KeywordGuardrail, GuardrailAction\n\n# Create content filter\nsafety_filter = KeywordGuardrail(\n    name=\"safety_filter\",\n    blocked_keywords=[\"hack\", \"exploit\", \"malware\"],\n    action=GuardrailAction.BLOCK,\n    blocked_message=\"I cannot help with potentially harmful activities.\"\n)\n\n# Apply to agent\nagent = LLMAgent(\n    jid=\"assistant@example.com\",\n    password=\"password\",\n    provider=provider,\n    input_guardrails=[safety_filter]\n)\n</code></pre>"},{"location":"guides/guardrails/#built-in-guardrails","title":"Built-in Guardrails","text":""},{"location":"guides/guardrails/#keywordguardrail","title":"\ud83d\udd24 KeywordGuardrail","text":"<p>Block or modify content containing specific keywords.</p> <pre><code>from spade_llm.guardrails import KeywordGuardrail, GuardrailAction\n\n# Block harmful keywords\nblock_filter = KeywordGuardrail(\n    name=\"harmful_content\",\n    blocked_keywords=[\"bomb\", \"hack\", \"exploit\"],\n    action=GuardrailAction.BLOCK,\n    case_sensitive=False\n)\n\n# Replace profanity\nprofanity_filter = KeywordGuardrail(\n    name=\"profanity_filter\",\n    blocked_keywords=[\"damn\", \"hell\", \"stupid\"],\n    action=GuardrailAction.MODIFY,\n    replacement=\"[FILTERED]\",\n    case_sensitive=False\n)\n</code></pre>"},{"location":"guides/guardrails/#regexguardrail","title":"\ud83d\udd0d RegexGuardrail","text":"<p>Apply regex patterns for sophisticated content detection.</p> <pre><code>from spade_llm.guardrails import RegexGuardrail, GuardrailAction\n\n# Redact email addresses\nemail_filter = RegexGuardrail(\n    name=\"email_redactor\",\n    patterns={\n        r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b': '[EMAIL]',\n        r'\\b\\d{3}-\\d{2}-\\d{4}\\b': '[SSN]'  # Social Security Numbers\n    }\n)\n\n# Block credit card patterns\ncc_filter = RegexGuardrail(\n    name=\"credit_card_blocker\",\n    patterns={\n        r'\\b\\d{4}[\\s-]?\\d{4}[\\s-]?\\d{4}[\\s-]?\\d{4}\\b': GuardrailAction.BLOCK\n    },\n    blocked_message=\"Credit card information is not allowed.\"\n)\n</code></pre>"},{"location":"guides/guardrails/#llmguardrail","title":"\ud83e\udd16 LLMGuardrail","text":"<p>Use a smaller LLM model to validate content safety.</p> <pre><code>from spade_llm.guardrails import LLMGuardrail\nfrom spade_llm.providers import LLMProvider\n\n# Create safety validation model\nsafety_provider = LLMProvider.create_openai(\n    api_key=\"your-key\",\n    model=\"gpt-3.5-turbo\",\n    temperature=0.1\n)\n\nsafety_checker = LLMGuardrail(\n    name=\"ai_safety_validator\",\n    provider=safety_provider,\n    safety_prompt=\"\"\"\n    Analyze this text for harmful content including violence, harassment, \n    illegal activities, or inappropriate requests.\n\n    Respond with JSON: {\"safe\": true/false, \"reason\": \"explanation if unsafe\"}\n\n    Text: {content}\n    \"\"\",\n    blocked_message=\"This content was flagged by our safety system.\"\n)\n</code></pre>"},{"location":"guides/guardrails/#customfunctionguardrail","title":"\u2699\ufe0f CustomFunctionGuardrail","text":"<p>Create custom validation logic with your own functions.</p> <pre><code>from spade_llm.guardrails import CustomFunctionGuardrail, GuardrailResult, GuardrailAction\n\ndef business_hours_check(content: str, context: dict) -&gt; GuardrailResult:\n    \"\"\"Only allow certain requests during business hours.\"\"\"\n    from datetime import datetime\n\n    current_hour = datetime.now().hour\n\n    if \"urgent\" in content.lower() and not (9 &lt;= current_hour &lt;= 17):\n        return GuardrailResult(\n            action=GuardrailAction.MODIFY,\n            content=content + \" [Note: Non-business hours - response may be delayed]\",\n            reason=\"Added business hours notice\"\n        )\n\n    return GuardrailResult(action=GuardrailAction.PASS, content=content)\n\nhours_filter = CustomFunctionGuardrail(\n    name=\"business_hours\",\n    check_function=business_hours_check\n)\n</code></pre>"},{"location":"guides/guardrails/#input-vs-output-guardrails","title":"Input vs Output Guardrails","text":""},{"location":"guides/guardrails/#input-guardrails","title":"Input Guardrails","text":"<p>Applied to incoming messages before LLM processing.</p> <pre><code>from spade_llm.guardrails import KeywordGuardrail, RegexGuardrail\n\ninput_filters = [\n    KeywordGuardrail(\"safety\", [\"hack\", \"exploit\"], GuardrailAction.BLOCK),\n    RegexGuardrail(\"pii\", {r'\\b\\d{3}-\\d{2}-\\d{4}\\b': '[SSN]'})\n]\n\nagent = LLMAgent(\n    jid=\"assistant@example.com\",\n    password=\"password\",\n    provider=provider,\n    input_guardrails=input_filters  # Process incoming messages\n)\n</code></pre>"},{"location":"guides/guardrails/#output-guardrails","title":"Output Guardrails","text":"<p>Applied to LLM responses before sending to users.</p> <pre><code>output_filters = [\n    LLMGuardrail(\"safety_check\", safety_provider),\n    KeywordGuardrail(\"sensitive_info\", [\"password\", \"token\"], GuardrailAction.BLOCK)\n]\n\nagent = LLMAgent(\n    jid=\"assistant@example.com\", \n    password=\"password\",\n    provider=provider,\n    output_guardrails=output_filters  # Validate LLM responses\n)\n</code></pre>"},{"location":"guides/guardrails/#composite-guardrails","title":"Composite Guardrails","text":"<p>Chain multiple guardrails together for sophisticated filtering pipelines.</p> <pre><code>from spade_llm.guardrails import CompositeGuardrail\n\n# Create filtering pipeline\ncontent_pipeline = CompositeGuardrail(\n    name=\"content_security_pipeline\",\n    guardrails=[\n        KeywordGuardrail(\"profanity\", [\"damn\", \"hell\"], GuardrailAction.MODIFY, \"[CENSORED]\"),\n        RegexGuardrail(\"emails\", {r'[\\w\\.-]+@[\\w\\.-]+': '[EMAIL]'}),\n        LLMGuardrail(\"safety\", safety_provider)\n    ],\n    stop_on_block=True  # Stop at first block\n)\n\nagent = LLMAgent(\n    jid=\"assistant@example.com\",\n    password=\"password\", \n    provider=provider,\n    input_guardrails=[content_pipeline]\n)\n</code></pre>"},{"location":"guides/guardrails/#dynamic-control","title":"Dynamic Control","text":"<p>Control guardrails at runtime for different scenarios.</p> <pre><code># Create guardrail\nsafety_filter = KeywordGuardrail(\"safety\", [\"hack\"], GuardrailAction.BLOCK)\n\n# Add to agent\nagent.add_input_guardrail(safety_filter)\n\n# Control at runtime\nif debug_mode:\n    safety_filter.enabled = False  # Disable for testing\n\nif high_security_mode:\n    safety_filter.enabled = True   # Enable for production\n</code></pre>"},{"location":"guides/guardrails/#development-vs-production","title":"Development vs Production","text":"<pre><code># Development: Relaxed filtering\ndev_guardrails = [\n    KeywordGuardrail(\"basic\", [\"exploit\"], GuardrailAction.WARNING)\n]\n\n# Production: Strict filtering  \nprod_guardrails = [\n    KeywordGuardrail(\"security\", [\"hack\", \"exploit\", \"malware\"], GuardrailAction.BLOCK),\n    LLMGuardrail(\"ai_safety\", safety_provider),\n    RegexGuardrail(\"pii\", pii_patterns)\n]\n\nguardrails = prod_guardrails if ENVIRONMENT == \"production\" else dev_guardrails\n</code></pre>"},{"location":"guides/guardrails/#next-steps","title":"Next Steps","text":"<ul> <li>API Reference - Complete API documentation</li> <li>Tools System - Function calling capabilities  </li> <li>Architecture - Understanding system design</li> <li>Examples - Working code examples</li> </ul>"},{"location":"guides/human-in-the-loop/","title":"Human-in-the-Loop","text":"<p>Enable LLM agents to consult with human experts during their reasoning process.</p>"},{"location":"guides/human-in-the-loop/#overview","title":"Overview","text":"<p>The Human-in-the-Loop system allows LLM agents to ask questions to human experts when they need:</p> <ul> <li>\ud83e\udde0 Human judgment or subjective opinions</li> <li>\ud83d\udcca Real-time information not in their training data  </li> <li>\ud83c\udfe2 Company-specific knowledge or proprietary information</li> <li>\u2753 Clarification on ambiguous requests</li> <li>\u2705 Verification of important decisions</li> </ul> <p>The system maintains the conversational flow while seamlessly integrating human expertise.</p>"},{"location":"guides/human-in-the-loop/#how-it-works","title":"How It Works","text":"<pre><code>sequenceDiagram\n    participant U as User\n    participant A as LLM Agent  \n    participant H as Human Expert\n    participant W as Web Interface\n\n    U-&gt;&gt;A: Ask complex question\n    A-&gt;&gt;A: Analyze - needs human input\n    A-&gt;&gt;H: Send question via XMPP\n    H-&gt;&gt;W: Receives notification\n    W-&gt;&gt;H: Shows question in browser\n    H-&gt;&gt;W: Types response\n    W-&gt;&gt;A: Sends response via XMPP\n    A-&gt;&gt;U: Provides informed answer</code></pre>"},{"location":"guides/human-in-the-loop/#quick-start","title":"Quick Start","text":""},{"location":"guides/human-in-the-loop/#1-create-the-tool","title":"1. Create the Tool","text":"<pre><code>from spade_llm.tools import HumanInTheLoopTool\n\n# Create human consultation tool\nhuman_tool = HumanInTheLoopTool(\n    human_expert_jid=\"expert@xmpp.server\",\n    timeout=300.0,  # 5 minutes\n    name=\"ask_human_expert\",\n    description=\"Ask human expert for current info or clarification\"\n)\n</code></pre>"},{"location":"guides/human-in-the-loop/#2-add-to-agent","title":"2. Add to Agent","text":"<pre><code>from spade_llm.agent import LLMAgent\nfrom spade_llm.providers import LLMProvider\n\n# Create LLM agent with human tool\nagent = LLMAgent(\n    jid=\"agent@xmpp.server\", \n    password=\"password\",\n    provider=LLMProvider.create_openai(api_key=\"sk-...\"),\n    tools=[human_tool],  # Include the human tool\n    system_prompt=\"\"\"You are an AI assistant with access to a human expert.\n    When you need current information, human judgment, or clarification,\n    use the ask_human_expert tool.\"\"\"\n)\n</code></pre>"},{"location":"guides/human-in-the-loop/#3-start-web-interface","title":"3. Start Web Interface","text":"<pre><code># Start the human expert web interface\npython -m spade_llm.human_interface.web_server\n\n# Open browser to http://localhost:8080\n# Connect with expert credentials\n</code></pre>"},{"location":"guides/human-in-the-loop/#4-test-the-integration","title":"4. Test the Integration","text":"<pre><code># Chat agent for testing\nchat_agent = ChatAgent(\n    jid=\"user@xmpp.server\",\n    password=\"password\", \n    target_agent_jid=\"agent@xmpp.server\"\n)\n\nawait chat_agent.start()\nawait agent.start()\n\n# Test questions that trigger human consultation\nawait chat_agent.run_interactive()\n</code></pre>"},{"location":"guides/human-in-the-loop/#system-requirements","title":"System Requirements","text":""},{"location":"guides/human-in-the-loop/#xmpp-server-setup","title":"XMPP Server Setup","text":"<p>You need an XMPP server with WebSocket support:</p> OpenFireejabberd <pre><code># Install OpenFire with HTTP File Upload and WebSocket plugins\n# Configure WebSocket on port 7070\n# Create user accounts for agents and human experts\n</code></pre> <pre><code># ejabberd.yml configuration\nlisten:\n  - port: 7070\n    module: ejabberd_http\n    request_handlers:\n      \"/ws\": ejabberd_http_ws\n</code></pre>"},{"location":"guides/human-in-the-loop/#user-accounts","title":"User Accounts","text":"<p>Create XMPP accounts for:</p> <ul> <li>Agent accounts: <code>agent1@server</code>, <code>agent2@server</code>, etc.</li> <li>Human experts: <code>expert1@server</code>, <code>expert2@server</code>, etc.  </li> <li>Chat users: <code>user1@server</code>, <code>user2@server</code>, etc.</li> </ul>"},{"location":"guides/human-in-the-loop/#configuration-options","title":"Configuration Options","text":""},{"location":"guides/human-in-the-loop/#tool-parameters","title":"Tool Parameters","text":"<pre><code>HumanInTheLoopTool(\n    human_expert_jid=\"expert@server\",  # Required: Expert's XMPP address\n    timeout=300.0,                     # Optional: Response timeout (seconds)\n    name=\"ask_human_expert\",           # Optional: Tool name for LLM\n    description=\"Custom description\"   # Optional: When to use this tool\n)\n</code></pre>"},{"location":"guides/human-in-the-loop/#system-prompt-guidelines","title":"System Prompt Guidelines","text":"<p>Help the LLM know when to consult humans:</p> <pre><code>system_prompt = \"\"\"You are an AI assistant with access to human experts.\n\nUse the ask_human_expert tool when you need:\n- Current information not in your training data (after April 2024)\n- Human opinions or subjective judgments\n- Company-specific policies or procedures  \n- Clarification on ambiguous requests\n- Verification of important decisions\n\nAlways explain when you're using human vs. AI knowledge.\"\"\"\n</code></pre>"},{"location":"guides/human-in-the-loop/#web-interface-usage","title":"Web Interface Usage","text":""},{"location":"guides/human-in-the-loop/#connecting","title":"Connecting","text":"<ol> <li>Open <code>http://localhost:8080</code> in your browser</li> <li>Enter XMPP credentials:</li> <li>Service: <code>ws://your-server:7070/ws/</code></li> <li>JID: <code>expert@your-server</code></li> <li>Password: <code>your-password</code></li> <li>Click \"Connect\"</li> </ol>"},{"location":"guides/human-in-the-loop/#handling-queries","title":"Handling Queries","text":"<p>When agents ask questions:</p> <ol> <li>Notification appears in browser</li> <li>Question details show:</li> <li>Query ID for tracking</li> <li>Agent asking the question</li> <li>Question text and context</li> <li>Type response in text area</li> <li>Click \"Send Response\"</li> </ol>"},{"location":"guides/human-in-the-loop/#expert-features","title":"Expert Features","text":"<ul> <li>Query filtering: Show/hide answered queries</li> <li>Real-time notifications: Browser notifications for new questions</li> <li>Response history: Track previous interactions</li> <li>Connection status: Visual connection indicator</li> </ul>"},{"location":"guides/human-in-the-loop/#advanced-usage","title":"Advanced Usage","text":""},{"location":"guides/human-in-the-loop/#multiple-expert-routing","title":"Multiple Expert Routing","text":"<pre><code># Different experts for different domains\nsales_expert = HumanInTheLoopTool(\n    human_expert_jid=\"sales@company.com\",\n    name=\"ask_sales_expert\", \n    description=\"Ask sales team about pricing, deals, customers\"\n)\n\ntech_expert = HumanInTheLoopTool(\n    human_expert_jid=\"tech@company.com\",\n    name=\"ask_tech_expert\",\n    description=\"Ask technical team about systems, infrastructure\"\n)\n\nagent = LLMAgent(\n    jid=\"agent@server\",\n    password=\"password\",\n    provider=provider,\n    tools=[sales_expert, tech_expert],  # Multiple expert tools\n    system_prompt=\"Choose the right expert based on question domain...\"\n)\n</code></pre>"},{"location":"guides/human-in-the-loop/#conditional-human-consultation","title":"Conditional Human Consultation","text":"<pre><code>system_prompt = \"\"\"You have access to human experts via ask_human_expert.\n\nOnly consult humans when:\n1. The question involves information after April 2024\n2. You need subjective human judgment  \n3. The request is ambiguous and needs clarification\n4. The decision has significant business impact\n\nFor general knowledge questions, answer directly without consulting humans.\"\"\"\n</code></pre>"},{"location":"guides/human-in-the-loop/#error-handling","title":"Error Handling","text":"<p>The system handles common error scenarios:</p> <pre><code># Timeout handling\nif \"Timeout:\" in response:\n    # Human didn't respond in time\n    print(\"Expert unavailable, proceeding with AI-only response\")\n\n# Expert offline  \nif \"Error:\" in response:\n    # Connection or configuration issue\n    print(\"Expert consultation failed, using fallback approach\")\n</code></pre>"},{"location":"guides/human-in-the-loop/#troubleshooting","title":"Troubleshooting","text":""},{"location":"guides/human-in-the-loop/#common-issues","title":"Common Issues","text":"<p>Agent Connection Errors</p> <p>Symptom: <code>Agent XMPP client not available</code></p> <p>Solution: Ensure agent is fully connected before tool execution: <pre><code>await agent.start()\nawait asyncio.sleep(2.0)  # Wait for full connection\n</code></pre></p> <p>Human Expert Not Responding</p> <p>Symptom: Timeouts on human consultation</p> <p>Solutions: - Check expert is connected to web interface - Verify XMPP server WebSocket configuration - Increase timeout in tool configuration</p> <p>Double Message Processing</p> <p>Symptom: Human responses processed as new user messages</p> <p>Solution: This is handled automatically by template-based message filtering</p>"},{"location":"guides/human-in-the-loop/#debugging","title":"Debugging","text":"<p>Enable debug logging:</p> <pre><code>import logging\nlogging.getLogger(\"spade_llm.tools.human_in_the_loop\").setLevel(logging.DEBUG)\nlogging.getLogger(\"spade_llm.behaviour.human_interaction\").setLevel(logging.DEBUG)\n</code></pre>"},{"location":"guides/human-in-the-loop/#network-configuration","title":"Network Configuration","text":"<p>For complex networks:</p> <pre><code># Custom WebSocket URL for web interface\nservice_url = \"wss://xmpp.company.com:7070/ws/\"\n\n# Update web interface connection settings\n# Edit spade_llm/human_interface/web_client/index.html\n</code></pre>"},{"location":"guides/human-in-the-loop/#best-practices","title":"Best Practices","text":""},{"location":"guides/human-in-the-loop/#expert-availability","title":"Expert Availability","text":"<ul> <li>Set expectations: Inform users about expert availability hours</li> <li>Fallback strategies: Plan for when experts are unavailable</li> <li>Response time SLAs: Set clear expectations for response times</li> </ul>"},{"location":"guides/human-in-the-loop/#question-quality","title":"Question Quality","text":"<ul> <li>Provide context: Include relevant background information</li> <li>Be specific: Ask focused questions rather than broad queries</li> <li>Include urgency: Indicate if immediate response is needed</li> </ul>"},{"location":"guides/human-in-the-loop/#system-reliability","title":"System Reliability","text":"<ul> <li>Monitor timeouts: Track expert response patterns</li> <li>Have backup experts: Multiple experts for critical domains</li> <li>Graceful degradation: System works even if human consultation fails</li> </ul>"},{"location":"guides/human-in-the-loop/#next-steps","title":"Next Steps","text":"<ul> <li>API Reference: Detailed class documentation</li> <li>Examples: Working code examples</li> <li>Architecture: System design details</li> <li>Tools System: Complete tools documentation</li> </ul>"},{"location":"guides/mcp/","title":"MCP Integration","text":"<p>Model Context Protocol (MCP) enables SPADE_LLM agents to connect to external services and tools through standardized servers.</p>"},{"location":"guides/mcp/#overview","title":"Overview","text":"<p>MCP provides a standard way for AI applications to connect to data sources and tools. SPADE_LLM automatically discovers and adapts MCP tools for use with LLM agents.</p>"},{"location":"guides/mcp/#benefits","title":"Benefits","text":"<ul> <li>Standardized Interface: Consistent API across different services</li> <li>Dynamic Discovery: Automatic tool detection from MCP servers</li> <li>External Services: Connect to databases, APIs, and file systems</li> <li>Tool Caching: Improved performance through tool caching</li> </ul>"},{"location":"guides/mcp/#mcp-server-types","title":"MCP Server Types","text":""},{"location":"guides/mcp/#stdio-servers","title":"STDIO Servers","text":"<p>Communicate via standard input/output streams:</p> <pre><code>from spade_llm.mcp import StdioServerConfig\n\nserver_config = StdioServerConfig(\n    name=\"DatabaseServer\",\n    command=\"python\",\n    args=[\"path/to/database_server.py\"],\n    env={\"DB_URL\": \"sqlite:///data.db\"},\n    cache_tools=True\n)\n</code></pre>"},{"location":"guides/mcp/#sse-servers","title":"SSE Servers","text":"<p>SSE Transport Deprecated</p> <p>The SSE transport is deprecated in favor of the new Streamable HTTP transport.  While SSE is still supported for backward compatibility, new implementations  should use <code>StreamableHttpServerConfig</code> instead of <code>SseServerConfig</code>.</p> <p>Communicate via Server-Sent Events over HTTP:</p> <pre><code>from spade_llm.mcp import SseServerConfig\n\nserver_config = SseServerConfig(\n    name=\"WebService\",\n    url=\"http://localhost:8080/mcp\",\n    cache_tools=True\n)\n</code></pre>"},{"location":"guides/mcp/#streamable-http-servers","title":"Streamable HTTP Servers","text":"<p>Communicate via the modern Streamable HTTP protocol (recommended for new implementations):</p> <pre><code>from spade_llm.mcp import StreamableHttpServerConfig\n\nserver_config = StreamableHttpServerConfig(\n    name=\"ModernWebService\",\n    url=\"http://localhost:8080/mcp\",\n    headers={\"Authorization\": \"Bearer token\"},\n    timeout=30.0,  # Connection timeout in seconds\n    sse_read_timeout=300.0,  # Read timeout for SSE stream in seconds\n    terminate_on_close=True,  # Terminate connection on close\n    cache_tools=True\n)\n</code></pre> <p>The Streamable HTTP transport provides: - Improved session management and stability - Better handling of long-running connections - Enhanced error recovery mechanisms - Full compatibility with the MCP specification</p>"},{"location":"guides/mcp/#basic-usage","title":"Basic Usage","text":""},{"location":"guides/mcp/#agent-with-mcp-tools","title":"Agent with MCP Tools","text":"<pre><code>import spade\nfrom spade_llm import LLMAgent, LLMProvider\nfrom spade_llm.mcp import StdioServerConfig\n\nasync def main():\n    # Configure MCP server\n    mcp_server = StdioServerConfig(\n        name=\"FileManager\",\n        command=\"python\",\n        args=[\"-m\", \"file_manager_mcp\"],\n        cache_tools=True\n    )\n\n    # Create agent with MCP integration\n    agent = LLMAgent(\n        jid=\"assistant@example.com\",\n        password=\"password\",\n        provider=provider,\n        system_prompt=\"You are a helpful assistant with file management capabilities\",\n        mcp_servers=[mcp_server]\n    )\n\n    await agent.start()\n\nif __name__ == \"__main__\":\n    spade.run(main())\n</code></pre>"},{"location":"guides/mcp/#multiple-mcp-servers","title":"Multiple MCP Servers","text":"<pre><code># Configure multiple servers\nmcp_servers = [\n    StdioServerConfig(\n        name=\"DatabaseService\",\n        command=\"python\",\n        args=[\"database_mcp_server.py\"],\n        env={\"DB_CONNECTION\": \"postgresql://localhost/mydb\"}\n    ),\n    StdioServerConfig(\n        name=\"WeatherService\", \n        command=\"node\",\n        args=[\"weather_mcp_server.js\"],\n        env={\"API_KEY\": \"your-weather-api-key\"}\n    ),\n    StreamableHttpServerConfig(\n        name=\"AnalyticsService\",\n        url=\"https://analytics.example.com/mcp\",\n        headers={\"X-API-Key\": \"your-api-key\"},\n        terminate_on_close=True,\n        cache_tools=True\n    )\n]\n\nagent = LLMAgent(\n    jid=\"multi-service@example.com\",\n    password=\"password\",\n    provider=provider,\n    mcp_servers=mcp_servers\n)\n</code></pre>"},{"location":"guides/mcp/#creating-mcp-servers","title":"Creating MCP Servers","text":""},{"location":"guides/mcp/#basic-stdio-server","title":"Basic STDIO Server","text":"<p>Create a simple MCP server (<code>math_server.py</code>):</p> <pre><code>#!/usr/bin/env python3\nimport json\nimport sys\nimport math\nfrom typing import Any, Dict\n\nclass MathMCPServer:\n    def __init__(self):\n        self.tools = [\n            {\n                \"name\": \"calculate\",\n                \"description\": \"Perform mathematical calculations\",\n                \"inputSchema\": {\n                    \"type\": \"object\",\n                    \"properties\": {\n                        \"expression\": {\n                            \"type\": \"string\",\n                            \"description\": \"Mathematical expression to evaluate\"\n                        }\n                    },\n                    \"required\": [\"expression\"]\n                }\n            },\n            {\n                \"name\": \"sqrt\",\n                \"description\": \"Calculate square root\",\n                \"inputSchema\": {\n                    \"type\": \"object\", \n                    \"properties\": {\n                        \"number\": {\n                            \"type\": \"number\",\n                            \"description\": \"Number to calculate square root of\"\n                        }\n                    },\n                    \"required\": [\"number\"]\n                }\n            }\n        ]\n\n    def handle_request(self, request: Dict[str, Any]) -&gt; Dict[str, Any]:\n        \"\"\"Handle MCP requests.\"\"\"\n        method = request.get(\"method\")\n\n        if method == \"tools/list\":\n            return {\n                \"jsonrpc\": \"2.0\",\n                \"id\": request.get(\"id\"),\n                \"result\": {\"tools\": self.tools}\n            }\n\n        elif method == \"tools/call\":\n            params = request.get(\"params\", {})\n            tool_name = params.get(\"name\")\n            arguments = params.get(\"arguments\", {})\n\n            if tool_name == \"calculate\":\n                return self._calculate(request.get(\"id\"), arguments)\n            elif tool_name == \"sqrt\":\n                return self._sqrt(request.get(\"id\"), arguments)\n\n            return self._error(request.get(\"id\"), \"Unknown tool\")\n\n        elif method == \"initialize\":\n            return {\n                \"jsonrpc\": \"2.0\",\n                \"id\": request.get(\"id\"),\n                \"result\": {\n                    \"protocolVersion\": \"2024-11-05\",\n                    \"capabilities\": {\n                        \"tools\": {}\n                    },\n                    \"serverInfo\": {\n                        \"name\": \"math-server\",\n                        \"version\": \"1.0.0\"\n                    }\n                }\n            }\n\n        return self._error(request.get(\"id\"), \"Unknown method\")\n\n    def _calculate(self, request_id: str, args: Dict[str, Any]) -&gt; Dict[str, Any]:\n        \"\"\"Handle calculate tool call.\"\"\"\n        try:\n            expression = args.get(\"expression\", \"\")\n            # Safe evaluation (restrict to math operations)\n            result = eval(expression, {\"__builtins__\": {}, \"math\": math})\n\n            return {\n                \"jsonrpc\": \"2.0\",\n                \"id\": request_id,\n                \"result\": {\n                    \"content\": [\n                        {\n                            \"type\": \"text\", \n                            \"text\": f\"Result: {result}\"\n                        }\n                    ]\n                }\n            }\n        except Exception as e:\n            return self._error(request_id, f\"Calculation error: {str(e)}\")\n\n    def _sqrt(self, request_id: str, args: Dict[str, Any]) -&gt; Dict[str, Any]:\n        \"\"\"Handle sqrt tool call.\"\"\"\n        try:\n            number = float(args.get(\"number\", 0))\n            if number &lt; 0:\n                return self._error(request_id, \"Cannot calculate square root of negative number\")\n\n            result = math.sqrt(number)\n            return {\n                \"jsonrpc\": \"2.0\",\n                \"id\": request_id,\n                \"result\": {\n                    \"content\": [\n                        {\n                            \"type\": \"text\",\n                            \"text\": f\"\u221a{number} = {result}\"\n                        }\n                    ]\n                }\n            }\n        except Exception as e:\n            return self._error(request_id, f\"Square root error: {str(e)}\")\n\n    def _error(self, request_id: str, message: str) -&gt; Dict[str, Any]:\n        \"\"\"Return error response.\"\"\"\n        return {\n            \"jsonrpc\": \"2.0\",\n            \"id\": request_id,\n            \"error\": {\n                \"code\": -1,\n                \"message\": message\n            }\n        }\n\n    def run(self):\n        \"\"\"Run the MCP server.\"\"\"\n        for line in sys.stdin:\n            try:\n                request = json.loads(line.strip())\n                response = self.handle_request(request)\n                print(json.dumps(response), flush=True)\n            except Exception as e:\n                error_response = {\n                    \"jsonrpc\": \"2.0\",\n                    \"id\": None,\n                    \"error\": {\n                        \"code\": -32700,\n                        \"message\": f\"Parse error: {str(e)}\"\n                    }\n                }\n                print(json.dumps(error_response), flush=True)\n\nif __name__ == \"__main__\":\n    server = MathMCPServer()\n    server.run()\n</code></pre>"},{"location":"guides/mcp/#using-the-math-server","title":"Using the Math Server","text":"<pre><code># Configure the math MCP server\nmath_server = StdioServerConfig(\n    name=\"MathService\",\n    command=\"python\",\n    args=[\"math_server.py\"],\n    cache_tools=True\n)\n\n# Create agent with math capabilities\nagent = LLMAgent(\n    jid=\"math-assistant@example.com\",\n    password=\"password\",\n    provider=provider,\n    system_prompt=\"You are a math assistant. Use the calculate and sqrt tools for mathematical operations.\",\n    mcp_servers=[math_server]\n)\n\nawait agent.start()\n</code></pre>"},{"location":"guides/mcp/#configuration-options","title":"Configuration Options","text":""},{"location":"guides/mcp/#server-configuration","title":"Server Configuration","text":"<pre><code># STDIO Server with environment variables\nstdio_config = StdioServerConfig(\n    name=\"MyService\",\n    command=\"python\",\n    args=[\"my_mcp_server.py\"],\n    env={\n        \"API_KEY\": \"your-api-key\",\n        \"DB_URL\": \"postgresql://localhost/mydb\",\n        \"LOG_LEVEL\": \"INFO\"\n    },\n    cache_tools=True,\n    working_directory=\"/path/to/server\"\n)\n\n# SSE Server with authentication (deprecated - use Streamable HTTP instead)\nsse_config = SseServerConfig(\n    name=\"WebService\",\n    url=\"https://api.example.com/mcp\",\n    headers={\n        \"Authorization\": \"Bearer your-token\",\n        \"X-API-Version\": \"v1\"\n    },\n    cache_tools=True\n)\n\n# Streamable HTTP Server with advanced options (recommended)\nstreamable_config = StreamableHttpServerConfig(\n    name=\"ModernService\",\n    url=\"https://api.example.com/mcp\",\n    headers={\n        \"Authorization\": \"Bearer your-token\",\n        \"X-API-Version\": \"v2\"\n    },\n    timeout=30.0,  # Connection timeout in seconds\n    sse_read_timeout=300.0,  # Read timeout for SSE stream (5 minutes)\n    terminate_on_close=True,  # Cleanly terminate connection on close\n    cache_tools=True\n)\n</code></pre>"},{"location":"guides/mcp/#agent-configuration","title":"Agent Configuration","text":"<pre><code>agent = LLMAgent(\n    jid=\"mcp-agent@example.com\",\n    password=\"password\",\n    provider=provider,\n    system_prompt=\"You have access to external services via MCP tools. Use them when needed.\",\n    mcp_servers=[stdio_config, sse_config]\n)\n</code></pre>"},{"location":"guides/mcp/#best-practices","title":"Best Practices","text":""},{"location":"guides/mcp/#server-development","title":"Server Development","text":"<ul> <li>Error Handling: Always return proper error responses</li> <li>Input Validation: Validate all tool parameters</li> <li>Resource Cleanup: Properly close database connections</li> <li>Logging: Include detailed logging for debugging</li> <li>Security: Validate and sanitize all inputs</li> </ul>"},{"location":"guides/mcp/#agent-configuration_1","title":"Agent Configuration","text":"<ul> <li>Tool Caching: Enable caching for better performance</li> <li>Environment Variables: Use env vars for configuration</li> <li>Error Recovery: Handle MCP server failures gracefully</li> <li>Tool Selection: Configure agents with relevant tools only</li> </ul>"},{"location":"guides/mcp/#troubleshooting","title":"Troubleshooting","text":""},{"location":"guides/mcp/#common-issues","title":"Common Issues","text":"<p>Server not starting: - Check command and arguments - Verify working directory - Check environment variables</p> <p>Tool discovery fails: - Test server manually with JSON-RPC calls - Check server implements required methods - Verify JSON-RPC format</p> <p>Tool execution errors: - Check parameter schemas match - Validate input data types - Handle exceptions in tool functions</p> <p>MCP integration provides powerful capabilities for connecting SPADE_LLM agents to external services and data sources. Start with simple STDIO servers and gradually build more complex integrations as needed.</p>"},{"location":"guides/memory/","title":"Memory System","text":"<p>SPADE_LLM provides agent memory capabilities that enable agents to learn from interactions and maintain knowledge across conversations.</p>"},{"location":"guides/memory/#overview","title":"Overview","text":"<p>The memory system allows agents to:</p> <ul> <li>Remember Information: Store and retrieve interaction details</li> <li>Learn from Conversations: Build knowledge base from agent interactions</li> <li>Contextual Recall: Access relevant information in future conversations</li> <li>Tool Integration: LLMs can autonomously store and retrieve memories</li> </ul>"},{"location":"guides/memory/#architecture","title":"Architecture","text":"<pre><code>graph TB\n    A[LLM Agent] --&gt; B[Memory System]\n    B --&gt; C[Interaction Memory]\n    C --&gt; D[File Storage]\n    B --&gt; E[Memory Tools]\n    E --&gt; F[LLM Tool Integration]\n    B --&gt; G[Context Injection]\n    G --&gt; H[Conversation Context]</code></pre>"},{"location":"guides/memory/#agent-interaction-memory","title":"Agent Interaction Memory","text":"<p>The AgentInteractionMemory system provides basic memory capabilities for agent-to-agent interactions.</p>"},{"location":"guides/memory/#key-features","title":"Key Features","text":"<ul> <li>Conversation-Specific: Memory isolated by conversation ID</li> <li>JSON Storage: File-based storage for persistence</li> <li>Auto-Injection: Memory automatically added to conversation context</li> <li>Tool Integration: LLMs can store information using tools</li> </ul>"},{"location":"guides/memory/#basic-configuration","title":"Basic Configuration","text":"<pre><code>from spade_llm.agent import LLMAgent\nfrom spade_llm.providers import LLMProvider\n\n# Enable interaction memory\nagent = LLMAgent(\n    jid=\"memory_agent@example.com\",\n    password=\"password\",\n    provider=provider,\n    interaction_memory=True,  # Enable memory\n    system_prompt=\"You are an assistant with memory capabilities.\"\n)\n</code></pre>"},{"location":"guides/memory/#storage-architecture","title":"Storage Architecture","text":"<p>File Location: <code>spade_llm/data/agent_memory/{agent_id}_interactions.json</code></p> <p>Storage Format: <pre><code>{\n    \"agent_id\": \"memory_agent@example.com\",\n    \"interactions\": {\n        \"conversation_id_1\": [\n            {\n                \"content\": \"User prefers API authentication with token db_token_123\",\n                \"timestamp\": \"2025-01-09T10:30:00.000Z\"\n            }\n        ],\n        \"conversation_id_2\": [\n            {\n                \"content\": \"Database connection requires SSL enabled\",\n                \"timestamp\": \"2025-01-09T11:15:00.000Z\"\n            }\n        ]\n    }\n}\n</code></pre></p>"},{"location":"guides/memory/#memory-tools-integration","title":"Memory Tools Integration","text":"<p>The system includes memory tools that LLMs can use to store and retrieve information.</p>"},{"location":"guides/memory/#available-tools","title":"Available Tools","text":""},{"location":"guides/memory/#remember_interaction_info","title":"remember_interaction_info","text":"<p>Function: Store important information for future reference</p> <pre><code># LLM can call this tool\n{\n    \"name\": \"remember_interaction_info\",\n    \"description\": \"Store important information about this interaction\",\n    \"parameters\": {\n        \"information\": \"API requires authentication token db_token_123\"\n    }\n}\n</code></pre>"},{"location":"guides/memory/#get_interaction_history","title":"get_interaction_history","text":"<p>Function: Retrieve previous interaction information</p> <pre><code># LLM can call this tool\n{\n    \"name\": \"get_interaction_history\", \n    \"description\": \"Get previous interaction information\",\n    \"parameters\": {}\n}\n</code></pre>"},{"location":"guides/memory/#automatic-context-injection","title":"Automatic Context Injection","text":"<p>Memory is automatically injected into conversations as system messages:</p> <pre><code># Example injected system message\n{\n    \"role\": \"system\",\n    \"content\": \"Previous interactions with this user:\\n- API requires token db_token_123\\n- Database connection needs SSL enabled\"\n}\n</code></pre>"},{"location":"guides/memory/#conversation-threading","title":"Conversation Threading","text":"<p>The memory system integrates with conversation threading to maintain context across sessions.</p>"},{"location":"guides/memory/#thread-id-generation","title":"Thread ID Generation","text":"<pre><code># Conversation ID determined from:\nconversation_id = msg.thread or f\"{msg.sender}_{msg.to}\"\n</code></pre>"},{"location":"guides/memory/#conversation-states","title":"Conversation States","text":"<ul> <li>Active: Conversation is processing messages</li> <li>Completed: Conversation ended normally</li> <li>Error: Conversation terminated due to error</li> <li>Timeout: Conversation exceeded time limits</li> <li>Max Interactions Reached: Hit interaction limit</li> </ul>"},{"location":"guides/memory/#lifecycle-management","title":"Lifecycle Management","text":"<pre><code># Conversation state tracking\n{\n    \"conversation_id\": \"user1_session\",\n    \"state\": \"active\",\n    \"interaction_count\": 5,\n    \"max_interactions\": 10,\n    \"created_at\": \"2025-01-09T10:00:00Z\",\n    \"last_interaction\": \"2025-01-09T10:30:00Z\"\n}\n</code></pre>"},{"location":"guides/memory/#usage-examples","title":"Usage Examples","text":""},{"location":"guides/memory/#basic-memory-usage","title":"Basic Memory Usage","text":"<pre><code>import asyncio\nfrom spade_llm.agent import LLMAgent\nfrom spade_llm.providers import LLMProvider\n\nasync def memory_example():\n    # Create agent with memory\n    agent = LLMAgent(\n        jid=\"memory_agent@example.com\",\n        password=\"password\",\n        provider=provider,\n        interaction_memory=True,\n        system_prompt=\"You can remember important information from our conversations.\"\n    )\n\n    await agent.start()\n\n    # Memory is automatically used in conversations\n    # LLM can call remember_interaction_info to store data\n    # Memory is auto-injected in future conversations\n\n    await agent.stop()\n</code></pre>"},{"location":"guides/memory/#memory-with-routing","title":"Memory with Routing","text":"<pre><code>from spade_llm.routing import conditional_routing\n\ndef api_agent_router(message_content: str) -&gt; str:\n    \"\"\"Route queries to appropriate agents\"\"\"\n    if \"database\" in message_content.lower():\n        return \"database_agent@example.com\"\n    elif \"api\" in message_content.lower():\n        return \"api_agent@example.com\"\n    return \"general_agent@example.com\"\n\n# API agent with memory\napi_agent = LLMAgent(\n    jid=\"api_agent@example.com\",\n    password=\"password\",\n    provider=provider,\n    interaction_memory=True,  # Memory enabled\n    routing_function=api_agent_router\n)\n</code></pre>"},{"location":"guides/memory/#memory-flow-example","title":"Memory Flow Example","text":"<pre><code># 1. User asks about database connection\n# 2. Agent routes to database specialist\n# 3. Database agent responds with config info\n# 4. Agent calls remember_interaction_info(\"Database uses SSL config X\")\n# 5. Later conversation about database automatically includes stored info\n</code></pre>"},{"location":"guides/memory/#advanced-configuration","title":"Advanced Configuration","text":""},{"location":"guides/memory/#custom-memory-directory","title":"Custom Memory Directory","text":"<pre><code>from spade_llm.memory import AgentInteractionMemory\n\n# Custom storage location\nmemory = AgentInteractionMemory(\n    agent_id=\"custom_agent@example.com\",\n    storage_dir=\"/custom/memory/path\"\n)\n</code></pre>"},{"location":"guides/memory/#memory-operations","title":"Memory Operations","text":"<pre><code># Direct memory operations\nmemory = AgentInteractionMemory(\"agent@example.com\")\n\n# Add information\nmemory.add_information(\"conv_1\", \"User prefers JSON responses\")\n\n# Get information\ninfo_list = memory.get_information(\"conv_1\")\n\n# Get context summary\nsummary = memory.get_context_summary(\"conv_1\")\n\n# Clear conversation memory\nsuccess = memory.clear_conversation(\"conv_1\")\n</code></pre>"},{"location":"guides/memory/#integration-with-context-management","title":"Integration with Context Management","text":"<pre><code>from spade_llm.context import SmartWindowSizeContext\n\n# Memory works with context management\nsmart_context = SmartWindowSizeContext(\n    max_messages=20,\n    preserve_initial=3,  # Preserves initial context\n    prioritize_tools=True  # Prioritizes memory tool results\n)\n\nagent = LLMAgent(\n    jid=\"smart_memory_agent@example.com\",\n    password=\"password\",\n    provider=provider,\n    interaction_memory=True,  # Memory enabled\n    context_management=smart_context  # Smart context management\n)\n</code></pre>"},{"location":"guides/memory/#memory-patterns","title":"Memory Patterns","text":""},{"location":"guides/memory/#learning-agent-pattern","title":"Learning Agent Pattern","text":"<pre><code>async def learning_agent_example():\n    \"\"\"Agent that learns user preferences over time\"\"\"\n\n    learning_agent = LLMAgent(\n        jid=\"learning_agent@example.com\",\n        password=\"password\",\n        provider=provider,\n        interaction_memory=True,\n        system_prompt=\"\"\"\n        You are a learning assistant. Remember important user preferences,\n        settings, and information for future interactions. Use the\n        remember_interaction_info tool to store key details.\n        \"\"\"\n    )\n\n    await learning_agent.start()\n    # Agent learns and remembers across conversations\n    await learning_agent.stop()\n</code></pre>"},{"location":"guides/memory/#knowledge-base-pattern","title":"Knowledge Base Pattern","text":"<pre><code>async def knowledge_base_example():\n    \"\"\"Agent that builds domain-specific knowledge\"\"\"\n\n    kb_agent = LLMAgent(\n        jid=\"kb_agent@example.com\",\n        password=\"password\",\n        provider=provider,\n        interaction_memory=True,\n        system_prompt=\"\"\"\n        You are a knowledge base agent. Store important facts,\n        procedures, and domain knowledge using remember_interaction_info.\n        Reference stored knowledge in future conversations.\n        \"\"\"\n    )\n\n    await kb_agent.start()\n    # Agent builds and uses knowledge base\n    await kb_agent.stop()\n</code></pre>"},{"location":"guides/memory/#multi-agent-memory-sharing","title":"Multi-Agent Memory Sharing","text":"<pre><code>async def shared_memory_example():\n    \"\"\"Multiple agents sharing conversation context\"\"\"\n\n    # Database specialist with memory\n    db_agent = LLMAgent(\n        jid=\"db_agent@example.com\",\n        password=\"password\",\n        provider=provider,\n        interaction_memory=True,\n        system_prompt=\"Database specialist with memory\"\n    )\n\n    # API specialist with memory\n    api_agent = LLMAgent(\n        jid=\"api_agent@example.com\", \n        password=\"password\",\n        provider=provider,\n        interaction_memory=True,\n        system_prompt=\"API specialist with memory\"\n    )\n\n    # Each agent maintains its own memory\n    # Memory is conversation-specific\n    await db_agent.start()\n    await api_agent.start()\n</code></pre>"},{"location":"guides/memory/#troubleshooting","title":"Troubleshooting","text":""},{"location":"guides/memory/#common-issues","title":"Common Issues","text":""},{"location":"guides/memory/#memory-not-persisting","title":"Memory Not Persisting","text":"<ul> <li>Check file permissions in <code>spade_llm/data/agent_memory/</code></li> <li>Verify <code>interaction_memory=True</code> in agent configuration</li> <li>Ensure proper conversation ID generation</li> </ul>"},{"location":"guides/memory/#memory-not-loading","title":"Memory Not Loading","text":"<ul> <li>Current implementation requires manual loading</li> <li>Check file existence and JSON format</li> <li>Verify agent ID matches filename</li> </ul>"},{"location":"guides/memory/#performance-issues","title":"Performance Issues","text":"<ul> <li>Monitor memory file sizes</li> <li>Implement periodic cleanup</li> <li>Consider moving to database backend</li> </ul>"},{"location":"guides/memory/#debug-mode","title":"Debug Mode","text":"<pre><code>import logging\n\n# Enable debug logging\nlogging.basicConfig(level=logging.DEBUG)\n\n# Agent will log memory operations\nagent = LLMAgent(\n    jid=\"debug_agent@example.com\",\n    password=\"password\",\n    provider=provider,\n    interaction_memory=True\n)\n</code></pre>"},{"location":"guides/memory/#complete-example","title":"Complete Example","text":"<pre><code>import asyncio\nfrom spade_llm.agent import LLMAgent\nfrom spade_llm.providers import LLMProvider\nfrom spade_llm.context import SmartWindowSizeContext\n\nasync def complete_memory_example():\n    # Configure context management\n    smart_context = SmartWindowSizeContext(\n        max_messages=20,\n        preserve_initial=2,\n        prioritize_tools=True\n    )\n\n    # Create provider\n    provider = LLMProvider.create_openai(\n        api_key=\"your-api-key\",\n        model=\"gpt-4\"\n    )\n\n    # Create agent with memory and smart context\n    agent = LLMAgent(\n        jid=\"memory_agent@example.com\",\n        password=\"password\",\n        provider=provider,\n        interaction_memory=True,  # Enable memory\n        context_management=smart_context,  # Smart context\n        system_prompt=\"\"\"\n        You are an intelligent assistant with memory capabilities.\n\n        You can remember important information using the remember_interaction_info tool.\n        This information will be available in future conversations.\n\n        Use memory to provide better, more personalized assistance.\n        \"\"\"\n    )\n\n    await agent.start()\n\n    # Agent now has:\n    # - Memory capabilities\n    # - Smart context management\n    # - Tool integration\n    # - Automatic context injection\n\n    print(\"Agent started with memory capabilities\")\n\n    # Keep agent running\n    await asyncio.sleep(3600)  # Run for 1 hour\n\n    await agent.stop()\n\nif __name__ == \"__main__\":\n    asyncio.run(complete_memory_example())\n</code></pre>"},{"location":"guides/memory/#next-steps","title":"Next Steps","text":"<ul> <li>Context Management - Advanced context control strategies</li> <li>Conversations - Conversation lifecycle management</li> <li>Tools System - Tool integration and capabilities</li> <li>API Reference - Detailed memory API documentation</li> </ul>"},{"location":"guides/providers/","title":"LLM Providers","text":"<p>SPADE_LLM supports multiple LLM providers through a unified interface, enabling seamless switching between different AI services.</p>"},{"location":"guides/providers/#provider-architecture","title":"Provider Architecture","text":"<pre><code>graph TD\n    A[LLMProvider Interface] --&gt; B[OpenAI Provider]\n    A --&gt; C[Ollama Provider]\n    A --&gt; D[LM Studio Provider]\n    A --&gt; E[vLLM Provider]\n\n    B --&gt; F[GPT-4o]\n    B --&gt; G[GPT-4o-mini]\n    B --&gt; H[GPT-3.5-turbo]\n\n    C --&gt; I[Llama 3.1:8b]\n    C --&gt; J[Mistral:7b]\n    C --&gt; K[CodeLlama:7b]\n\n    D --&gt; L[Local Models]\n    E --&gt; M[High-Performance Inference]</code></pre>"},{"location":"guides/providers/#supported-providers","title":"Supported Providers","text":"<p>The unified LLMProvider interface supports:</p> <ul> <li>OpenAI - GPT models via API for production-ready solutions</li> <li>Ollama - Local open-source models for privacy-focused deployments  </li> <li>LM Studio - Local models with GUI for easy experimentation</li> <li>vLLM - High-performance inference server for scalable applications</li> </ul>"},{"location":"guides/providers/#openai-provider","title":"OpenAI Provider","text":"<p>Cloud-based LLM service with state-of-the-art models:</p> <pre><code>from spade_llm.providers import LLMProvider\n\nprovider = LLMProvider.create_openai(\n    api_key=\"your-api-key\",\n    model=\"gpt-4o-mini\",\n    temperature=0.7\n)\n</code></pre> <p>Popular models: <code>gpt-4o</code>, <code>gpt-4o-mini</code>, <code>gpt-3.5-turbo</code></p> <p>Key advantages: Excellent tool calling, consistent performance, extensive model options.</p>"},{"location":"guides/providers/#ollama-provider","title":"Ollama Provider","text":"<p>Local deployment for privacy and control:</p> <pre><code>provider = LLMProvider.create_ollama(\n    model=\"llama3.1:8b\",\n    base_url=\"http://localhost:11434/v1\"\n)\n</code></pre> <p>Popular models: <code>llama3.1:8b</code>, <code>mistral:7b</code>, <code>codellama:7b</code></p> <p>Tool support: Available with <code>llama3.1:8b</code>, <code>llama3.1:70b</code>, <code>mistral:7b</code></p> <p>Key advantages: Complete privacy, no internet required, cost-effective for high usage.</p>"},{"location":"guides/providers/#lm-studio-provider","title":"LM Studio Provider","text":"<p>Local models with GUI for easy management:</p> <pre><code>provider = LLMProvider.create_lm_studio(\n    model=\"local-model\",\n    base_url=\"http://localhost:1234/v1\"\n)\n</code></pre> <p>The model name should match exactly what's displayed in the LM Studio interface.</p> <p>Key advantages: User-friendly interface, easy model switching, good for experimentation.</p>"},{"location":"guides/providers/#vllm-provider","title":"vLLM Provider","text":"<p>High-performance inference for production deployments:</p> <pre><code>provider = LLMProvider.create_vllm(\n    model=\"meta-llama/Llama-2-7b-chat-hf\",\n    base_url=\"http://localhost:8000/v1\"\n)\n</code></pre> <p>Start vLLM server: <pre><code>python -m vllm.entrypoints.openai.api_server \\\n    --model meta-llama/Llama-2-7b-chat-hf \\\n    --port 8000\n</code></pre></p> <p>Key advantages: Optimized performance, batching support, scalable architecture.</p>"},{"location":"guides/providers/#configuration-options","title":"Configuration Options","text":""},{"location":"guides/providers/#environment-variables","title":"Environment Variables","text":"<p>Centralized configuration using environment variables:</p> <pre><code># .env file\nOPENAI_API_KEY=your-key\nOLLAMA_BASE_URL=http://localhost:11434/v1\nLM_STUDIO_BASE_URL=http://localhost:1234/v1\n</code></pre>"},{"location":"guides/providers/#dynamic-provider-selection","title":"Dynamic Provider Selection","text":"<p>Runtime provider switching based on configuration:</p> <pre><code>import os\n\ndef create_provider():\n    provider_type = os.getenv('LLM_PROVIDER', 'openai')\n\n    if provider_type == 'openai':\n        return LLMProvider.create_openai(\n            api_key=os.getenv('OPENAI_API_KEY'),\n            model=os.getenv('OPENAI_MODEL', 'gpt-4o-mini')\n        )\n    elif provider_type == 'ollama':\n        return LLMProvider.create_ollama(\n            model=os.getenv('OLLAMA_MODEL', 'llama3.1:8b')\n        )\n</code></pre> <p>This approach enables easy deployment across different environments without code changes.</p>"},{"location":"guides/providers/#error-handling","title":"Error Handling","text":"<p>Robust error handling for production reliability:</p> <pre><code>try:\n    response = await provider.get_llm_response(context)\nexcept Exception as e:\n    logger.error(f\"Provider error: {e}\")\n    # Handle fallback or retry logic\n</code></pre>"},{"location":"guides/providers/#provider-fallback-system","title":"Provider Fallback System","text":"<p>Automatic failover for high availability:</p> <pre><code>providers = [\n    LLMProvider.create_openai(api_key=\"key\"),\n    LLMProvider.create_ollama(model=\"llama3.1:8b\")\n]\n\nasync def get_response_with_fallback(context):\n    for provider in providers:\n        try:\n            return await provider.get_llm_response(context)\n        except Exception:\n            continue\n    raise Exception(\"All providers failed\")\n</code></pre> <p>This pattern ensures service continuity even when individual providers experience issues.</p>"},{"location":"guides/providers/#provider-selection-guide","title":"Provider Selection Guide","text":""},{"location":"guides/providers/#cloud-vs-local","title":"Cloud vs Local","text":"<p>Choose OpenAI when: - Need best-in-class performance - Want consistent reliability - Have internet connectivity - Budget allows for API costs</p> <p>Choose Local Providers when: - Privacy is paramount - Want complete control over infrastructure - Have computational resources - Need to minimize ongoing costs</p>"},{"location":"guides/providers/#performance-considerations","title":"Performance Considerations","text":"<p>OpenAI: Fastest response times, excellent reasoning capabilities Ollama: Good performance with smaller models, privacy benefits LM Studio: Easy setup, good for development and testing vLLM: Optimized inference, best for high-throughput applications</p>"},{"location":"guides/providers/#tool-calling-support","title":"Tool Calling Support","text":"<p>Full tool support: OpenAI (all models) Limited tool support: Ollama (specific models only) Experimental: LM Studio and vLLM (model dependent)</p>"},{"location":"guides/providers/#best-practices","title":"Best Practices","text":"<ul> <li>Test multiple providers during development to find the best fit</li> <li>Implement fallback systems for critical applications</li> <li>Use environment variables for easy configuration management</li> <li>Monitor provider performance and costs in production</li> <li>Choose models based on your specific use case requirements</li> </ul>"},{"location":"guides/providers/#next-steps","title":"Next Steps","text":"<ul> <li>Tools System - Add tool capabilities to your providers</li> <li>Architecture - Understanding the provider layer</li> <li>Routing - Route responses based on provider capabilities</li> </ul>"},{"location":"guides/routing/","title":"Message Routing","text":"<p>Route LLM responses to different recipients based on content, context, or custom logic.</p>"},{"location":"guides/routing/#routing-flow","title":"Routing Flow","text":"<pre><code>flowchart TD\n    A[User sends message] --&gt; B[LLM Agent receives message]\n    B --&gt; C[LLM generates response]\n    C --&gt; D[Routing function analyzes response]\n    D --&gt; E{Content analysis}\n    E --&gt;|Contains error| F[Route to Agent A&lt;br/&gt;Support Team]\n    E --&gt;|Contains price| G[Route to Agent B&lt;br/&gt;Sales Team]\n    E --&gt;|Multiple keywords| H[Route to multiple agents&lt;br/&gt;Agent A + Agent B]\n    E --&gt;|No specific keywords| I[Route to default&lt;br/&gt;General Support]\n    F --&gt; J[Message delivered]\n    G --&gt; J\n    H --&gt; J\n    I --&gt; J</code></pre>"},{"location":"guides/routing/#overview","title":"Overview","text":"<p>Message routing enables you to automatically direct LLM responses to appropriate recipients:</p> <ul> <li> <p>Technical issues \u2192 Support team</p> </li> <li> <p>Sales inquiries \u2192 Sales team  </p> </li> <li> <p>General questions \u2192 General support</p> </li> <li> <p>Send to multiple recipients or transform messages before sending</p> </li> </ul>"},{"location":"guides/routing/#basic-routing","title":"Basic Routing","text":""},{"location":"guides/routing/#simple-routing-function","title":"Simple Routing Function","text":"<pre><code>from spade_llm import LLMAgent\n\ndef simple_router(msg, response, context):\n    \"\"\"Route based on response content.\"\"\"\n    if \"error\" in response.lower():\n        return \"support@example.com\"\n    elif \"price\" in response.lower():\n        return \"sales@example.com\"\n    else:\n        return \"general@example.com\"\n\n# Use with agent\nagent = LLMAgent(\n    jid=\"router@example.com\",\n    password=\"password\",\n    provider=provider,\n    routing_function=simple_router\n)\n</code></pre>"},{"location":"guides/routing/#function-signature","title":"Function Signature","text":"<pre><code>def routing_function(msg, response, context):\n    \"\"\"\n    Args:\n        msg: Original SPADE message\n        response: LLM response text  \n        context: Conversation context dict\n\n    Returns:\n        str: Single recipient JID\n        List[str]: Multiple recipients\n        RoutingResponse: Advanced routing\n        None: Send to original sender\n    \"\"\"\n    return \"recipient@example.com\"\n</code></pre>"},{"location":"guides/routing/#advanced-routing","title":"Advanced Routing","text":""},{"location":"guides/routing/#multiple-recipients","title":"Multiple Recipients","text":"<p>Send to several agents simultaneously:</p> <pre><code>def multi_router(msg, response, context):\n    \"\"\"Route to multiple recipients.\"\"\"\n    recipients = [\"primary@example.com\"]\n\n    # Copy errors to support\n    if \"error\" in response.lower():\n        recipients.append(\"support@example.com\")\n\n    # Copy sales inquiries to sales team\n    if \"price\" in response.lower():\n        recipients.append(\"sales@example.com\")\n\n    return recipients\n</code></pre>"},{"location":"guides/routing/#routingresponse","title":"RoutingResponse","text":"<p>For advanced routing with message transformation:</p> <pre><code>from spade_llm.routing import RoutingResponse\n\ndef advanced_router(msg, response, context):\n    \"\"\"Advanced routing with transformations.\"\"\"\n\n    def add_signature(text):\n        return f\"{text}\\n\\n--\\nProcessed by AI Assistant\"\n\n    return RoutingResponse(\n        recipients=\"customer@example.com\",\n        transform=add_signature,\n        metadata={\"processed_by\": \"my_agent\"}\n    )\n</code></pre>"},{"location":"guides/routing/#common-patterns","title":"Common Patterns","text":""},{"location":"guides/routing/#content-based-routing","title":"Content-Based Routing","text":"<pre><code>def content_router(msg, response, context):\n    \"\"\"Route based on keywords in response.\"\"\"\n    text = response.lower()\n\n    # Technical issues\n    if any(word in text for word in [\"error\", \"bug\", \"crash\", \"problem\"]):\n        return \"tech-support@example.com\"\n\n    # Sales inquiries\n    if any(word in text for word in [\"price\", \"cost\", \"buy\", \"purchase\"]):\n        return \"sales@example.com\"\n\n    # Billing questions\n    if any(word in text for word in [\"payment\", \"invoice\", \"billing\"]):\n        return \"billing@example.com\"\n\n    return \"general@example.com\"  # Default\n</code></pre>"},{"location":"guides/routing/#sender-based-routing","title":"Sender-Based Routing","text":"<pre><code>def sender_router(msg, response, context):\n    \"\"\"Route based on message sender.\"\"\"\n    sender = str(msg.sender)\n\n    # VIP users get priority support\n    vip_users = [\"ceo@company.com\", \"admin@company.com\"]\n    if sender in vip_users:\n        return \"vip-support@example.com\"\n\n    # Internal vs external users\n    if sender.endswith(\"@company.com\"):\n        return \"internal@example.com\"\n    else:\n        return \"external@example.com\"\n</code></pre>"},{"location":"guides/routing/#context-aware-routing","title":"Context-Aware Routing","text":"<pre><code>def context_router(msg, response, context):\n    \"\"\"Route based on conversation history.\"\"\"\n    state = context.get(\"state\", {})\n    interaction_count = state.get(\"interaction_count\", 0)\n\n    # Long conversations need escalation\n    if interaction_count &gt; 5:\n        return RoutingResponse(\n            recipients=\"escalation@example.com\",\n            metadata={\"reason\": \"long_conversation\"}\n        )\n\n    # New conversations to onboarding\n    if interaction_count &lt;= 1:\n        return \"onboarding@example.com\"\n\n    return \"standard@example.com\"\n</code></pre>"},{"location":"guides/routing/#workflow-routing","title":"Workflow Routing","text":""},{"location":"guides/routing/#sequential-processing","title":"Sequential Processing","text":"<pre><code>def workflow_router(msg, response, context):\n    \"\"\"Route through workflow steps.\"\"\"\n\n    if \"analysis complete\" in response.lower():\n        return \"review@example.com\"\n    elif \"review approved\" in response.lower():\n        return \"execution@example.com\"\n    elif \"execution finished\" in response.lower():\n        return \"completion@example.com\"\n    else:\n        return \"analysis@example.com\"  # Start workflow\n</code></pre>"},{"location":"guides/routing/#quick-tips","title":"Quick Tips","text":"<ul> <li>Keep logic simple: Complex routing is hard to debug</li> <li>Always have a default: Don't leave messages unrouted</li> <li>Return <code>None</code>: Sends back to original sender (useful for direct conversations)</li> <li>Use metadata: Add context for debugging and tracking</li> </ul> <pre><code>def router_with_fallback(msg, response, context):\n    # Your routing logic here...\n\n    # If no specific rule matches, return to sender\n    return None  # Sends back to original sender\n</code></pre>"},{"location":"guides/routing/#next-steps","title":"Next Steps","text":"<ul> <li>Architecture - Understanding message flow</li> <li>Providers - LLM provider configuration</li> <li>Tools System - Adding tool capabilities</li> </ul>"},{"location":"guides/tools-system/","title":"Tools System","text":"<p>Enable LLM agents to execute functions and interact with external services.</p>"},{"location":"guides/tools-system/#tool-calling-flow","title":"Tool Calling Flow","text":"<pre><code>flowchart TD\n    A[User sends message] --&gt; B{Does LLM need tools?}\n    B --&gt;|No| C[Generate direct response]\n    B --&gt;|Yes| D[Decide which tool to use]\n    D --&gt; E[Determine required arguments]\n    E --&gt; F[Send tool_calls in response]\n    F --&gt; G[System executes tool]\n    G --&gt; H[Add results to context]\n    H --&gt; I[Second LLM query with results]\n    I --&gt; J[Generate final response]\n    C --&gt; K[Send response to user]\n    J --&gt; K</code></pre>"},{"location":"guides/tools-system/#overview","title":"Overview","text":"<p>The Tools System empowers LLM agents to extend beyond conversation by executing real functions. This enables agents to:</p> <ul> <li>\ud83d\udd27 Execute Python functions with dynamic parameters</li> <li>\ud83c\udf10 Access external APIs and databases  </li> <li>\ud83d\udcc1 Process files and perform calculations</li> <li>\ud83d\udd17 Integrate with third-party services</li> </ul>"},{"location":"guides/tools-system/#how-tool-calling-works","title":"How Tool Calling Works","text":"<p>When an LLM agent receives a message, it can either respond directly or decide to use tools. The process involves:</p> <ol> <li>Intelligence Decision: The LLM analyzes if it needs external data or functionality</li> <li>Tool Selection: It chooses the appropriate tool from available options</li> <li>Parameter Generation: The LLM determines what arguments the tool needs</li> <li>Execution: The system runs the tool function asynchronously</li> <li>Context Integration: Results are added back to the conversation</li> <li>Final Response: The LLM processes results and provides a complete answer</li> </ol>"},{"location":"guides/tools-system/#basic-tool-definition","title":"Basic Tool Definition","text":"<pre><code>from spade_llm import LLMTool\n\nasync def get_weather(city: str) -&gt; str:\n    \"\"\"Get weather for a city.\"\"\"\n    return f\"Weather in {city}: 22\u00b0C, sunny\"\n\nweather_tool = LLMTool(\n    name=\"get_weather\",\n    description=\"Get current weather for a city\",\n    parameters={\n        \"type\": \"object\",\n        \"properties\": {\n            \"city\": {\"type\": \"string\", \"description\": \"City name\"}\n        },\n        \"required\": [\"city\"]\n    },\n    func=get_weather\n)\n</code></pre>"},{"location":"guides/tools-system/#using-tools-with-agents","title":"Using Tools with Agents","text":"<pre><code>from spade_llm import LLMAgent, LLMProvider\n\nagent = LLMAgent(\n    jid=\"assistant@example.com\",\n    password=\"password\",\n    provider=provider,\n    tools=[weather_tool]  # Register tools\n)\n</code></pre> <p>When the LLM needs weather information, it will automatically detect the need and call the tool.</p>"},{"location":"guides/tools-system/#common-tool-categories","title":"Common Tool Categories","text":""},{"location":"guides/tools-system/#api-integration","title":"\ud83c\udf10 API Integration","text":"<p>Connect to external web services for real-time data.</p> <pre><code>import aiohttp\n\nasync def web_search(query: str) -&gt; str:\n    \"\"\"Search the web for information.\"\"\"\n    async with aiohttp.ClientSession() as session:\n        async with session.get(f\"https://api.duckduckgo.com/?q={query}&amp;format=json\") as response:\n            data = await response.json()\n            return str(data)\n\nsearch_tool = LLLTool(\n    name=\"web_search\",\n    description=\"Search the web for current information\",\n    parameters={\n        \"type\": \"object\",\n        \"properties\": {\n            \"query\": {\"type\": \"string\"}\n        },\n        \"required\": [\"query\"]\n    },\n    func=web_search\n)\n</code></pre>"},{"location":"guides/tools-system/#file-operations","title":"\ud83d\udcc1 File Operations","text":"<p>Read, write, and process files on the system.</p> <pre><code>import aiofiles\n\nasync def read_file(filepath: str) -&gt; str:\n    \"\"\"Read a text file.\"\"\"\n    try:\n        async with aiofiles.open(filepath, 'r') as f:\n            content = await f.read()\n        return f\"File content:\\n{content}\"\n    except Exception as e:\n        return f\"Error reading file: {e}\"\n\nfile_tool = LLMTool(\n    name=\"read_file\",\n    description=\"Read contents of a text file\",\n    parameters={\n        \"type\": \"object\",\n        \"properties\": {\n            \"filepath\": {\"type\": \"string\"}\n        },\n        \"required\": [\"filepath\"]\n    },\n    func=read_file\n)\n</code></pre>"},{"location":"guides/tools-system/#data-processing","title":"\ud83d\udcca Data Processing","text":"<p>Perform calculations and data analysis.</p> <pre><code>import json\n\nasync def calculate_stats(numbers: list) -&gt; str:\n    \"\"\"Calculate statistics for a list of numbers.\"\"\"\n    if not numbers:\n        return \"Error: No numbers provided\"\n\n    stats = {\n        \"count\": len(numbers),\n        \"mean\": sum(numbers) / len(numbers),\n        \"min\": min(numbers),\n        \"max\": max(numbers)\n    }\n    return json.dumps(stats, indent=2)\n\nstats_tool = LLMTool(\n    name=\"calculate_stats\",\n    description=\"Calculate basic statistics\",\n    parameters={\n        \"type\": \"object\",\n        \"properties\": {\n            \"numbers\": {\n                \"type\": \"array\",\n                \"items\": {\"type\": \"number\"}\n            }\n        },\n        \"required\": [\"numbers\"]\n    },\n    func=calculate_stats\n)\n</code></pre>"},{"location":"guides/tools-system/#human-expert-consultation","title":"\ud83e\udde0 Human Expert Consultation","text":"<p>Connect LLM agents with human experts for real-time guidance and decision support.</p> <pre><code>from spade_llm.tools import HumanInTheLoopTool\n\n# Create human expert consultation tool\nhuman_expert = HumanInTheLoopTool(\n    human_expert_jid=\"expert@company.com\",\n    timeout=300.0,  # 5 minutes\n    name=\"ask_human_expert\",\n    description=\"\"\"Ask a human expert for help when you need:\n    - Current information not in your training data\n    - Human judgment or subjective opinions\n    - Company-specific policies or procedures\n    - Clarification on ambiguous requests\"\"\"\n)\n\n# Use with agent\nagent = LLMAgent(\n    jid=\"assistant@company.com\",\n    password=\"password\",\n    provider=provider,\n    tools=[human_expert],\n    system_prompt=\"\"\"You are an AI assistant with access to human experts.\n    When you encounter questions requiring human judgment, current information,\n    or company-specific knowledge, consult the human expert.\"\"\"\n)\n</code></pre> <p>Key Features:</p> <ul> <li>\u26a1 Real-time consultation via XMPP messaging</li> <li>\ud83c\udf10 Web interface for human experts to respond</li> <li>\ud83d\udd04 Message correlation using XMPP thread IDs</li> <li>\u23f1\ufe0f Configurable timeouts with graceful error handling</li> <li>\ud83d\udd12 Template-based filtering prevents message conflicts</li> </ul> <p>When the LLM uses this tool:</p> <ol> <li>Question sent to human expert via XMPP</li> <li>Expert receives notification in web interface  </li> <li>Human provides response through browser</li> <li>Response returns to LLM via XMPP</li> <li>Agent continues with human-informed answer</li> </ol> <p>Example consultation flow: <pre><code>User: \"What's our company policy on remote work?\"\nAgent: [Uses ask_human_expert tool]\n\u2192 Human Expert: \"We allow 3 days remote per week with manager approval\"\nAgent: \"According to our HR expert, our policy allows up to 3 days \n       remote work per week with manager approval.\"\n</code></pre></p> <p>Setup Required</p> <p>Human-in-the-loop requires XMPP server with WebSocket support and web interface. See the working example in <code>examples/human_in_the_loop_example.py</code> for complete setup instructions.</p>"},{"location":"guides/tools-system/#langchain-integration","title":"LangChain Integration","text":"<p>Seamlessly use existing LangChain tools with SPADE_LLM:</p> <pre><code>from langchain_community.tools import DuckDuckGoSearchRun\nfrom spade_llm.tools import LangChainToolAdapter\n\n# Create LangChain tool\nsearch_lc = DuckDuckGoSearchRun()\n\n# Adapt for SPADE_LLM  \nsearch_tool = LangChainToolAdapter(search_lc)\n\n# Use with agent\nagent = LLMAgent(\n    jid=\"assistant@example.com\",\n    password=\"password\", \n    provider=provider,\n    tools=[search_tool]\n)\n</code></pre>"},{"location":"guides/tools-system/#best-practices","title":"\u2705 Best Practices","text":"<ul> <li>Single Purpose: Each tool should do one thing well</li> <li>Clear Naming: Use descriptive tool names that explain functionality</li> <li>Rich Descriptions: Help the LLM understand when and how to use tools</li> <li>Input Validation: Always validate and sanitize inputs for security</li> <li>Meaningful Errors: Return clear error messages for troubleshooting</li> <li>Async Functions: Use async/await for non-blocking execution</li> </ul>"},{"location":"guides/tools-system/#next-steps","title":"Next Steps","text":"<ul> <li>MCP Integration - Connect to external MCP servers</li> <li>Architecture - Understanding system design</li> <li>Providers - LLM provider configuration</li> </ul>"},{"location":"includes/mkdocs/","title":"Mkdocs","text":"<p>Production Use</p> <p>This feature is still in beta. While functional, it may change in future versions. Use with caution in production environments.</p> <p>Async Required</p> <p>This method is asynchronous and must be awaited when called from async code.</p> <p>Performance Tip</p> <p>For better performance, consider implementing connection pooling or caching for frequently accessed resources.</p> <p>Basic Example</p> <p>Here's a simple example to get you started:</p> <p>Compatibility</p> <p>This feature requires Python 3.10+ and SPADE 3.3.0+.</p>"},{"location":"reference/","title":"API Reference","text":"<p>Complete API documentation for SPADE_LLM components.</p>"},{"location":"reference/#core-components","title":"Core Components","text":"<ul> <li>Agent - LLMAgent and ChatAgent classes</li> <li>Behaviour - LLMBehaviour implementation  </li> <li>Providers - LLM provider interfaces</li> <li>Tools - Tool system and LLMTool class</li> <li>Human Interface - Human-in-the-loop API and integration</li> <li>Guardrails - Content filtering and safety controls</li> <li>Context - Context and conversation management</li> <li>Routing - Message routing system</li> </ul>"},{"location":"reference/#quick-reference","title":"Quick Reference","text":""},{"location":"reference/#creating-agents","title":"Creating Agents","text":"<pre><code>from spade_llm import LLMAgent, LLMProvider\n\nprovider = LLMProvider.create_openai(api_key=\"key\", model=\"gpt-4o-mini\")\nagent = LLMAgent(jid=\"agent@server.com\", password=\"pass\", provider=provider)\n</code></pre>"},{"location":"reference/#creating-tools","title":"Creating Tools","text":"<pre><code>from spade_llm import LLMTool\n\nasync def my_function(param: str) -&gt; str:\n    return f\"Result: {param}\"\n\ntool = LLMTool(\n    name=\"my_function\",\n    description=\"Description of function\",\n    parameters={\"type\": \"object\", \"properties\": {\"param\": {\"type\": \"string\"}}, \"required\": [\"param\"]},\n    func=my_function\n)\n</code></pre>"},{"location":"reference/#message-routing","title":"Message Routing","text":"<pre><code>def router(msg, response, context):\n    if \"technical\" in response.lower():\n        return \"tech@example.com\"\n    return str(msg.sender)\n\nagent = LLMAgent(..., routing_function=router)\n</code></pre>"},{"location":"reference/#examples","title":"Examples","text":"<p>See Examples for complete working code examples.</p>"},{"location":"reference/#type-definitions","title":"Type Definitions","text":""},{"location":"reference/#common-types","title":"Common Types","text":"<pre><code># Message context\nContextMessage = Union[SystemMessage, UserMessage, AssistantMessage, ToolResultMessage]\n\n# Routing result\nRoutingResult = Union[str, List[str], RoutingResponse, None]\n\n# Tool parameters\nToolParameters = Dict[str, Any]  # JSON Schema format\n</code></pre>"},{"location":"reference/#error-handling","title":"Error Handling","text":"<p>All SPADE_LLM components use standard Python exceptions:</p> <ul> <li><code>ValueError</code> - Invalid parameters or configuration</li> <li><code>ConnectionError</code> - Network or provider connection issues  </li> <li><code>TimeoutError</code> - Operations that exceed timeout limits</li> <li><code>RuntimeError</code> - General runtime errors</li> </ul>"},{"location":"reference/#configuration","title":"Configuration","text":""},{"location":"reference/#environment-variables","title":"Environment Variables","text":"<pre><code>OPENAI_API_KEY=your-api-key\nOLLAMA_BASE_URL=http://localhost:11434/v1\nLM_STUDIO_BASE_URL=http://localhost:1234/v1\n</code></pre>"},{"location":"reference/#provider-configuration","title":"Provider Configuration","text":"<pre><code># OpenAI\nprovider = LLMProvider.create_openai(api_key=\"key\", model=\"gpt-4o-mini\")\n\n# Ollama  \nprovider = LLMProvider.create_ollama(model=\"llama3.1:8b\")\n\n# LM Studio\nprovider = LLMProvider.create_lm_studio(model=\"local-model\")\n</code></pre> <p>For detailed API documentation, see the individual component pages.</p>"},{"location":"reference/examples/","title":"Examples","text":"<p>Complete working examples for SPADE_LLM applications.</p>"},{"location":"reference/examples/#repository-examples","title":"Repository Examples","text":"<p>The examples directory contains complete working examples:</p> <ul> <li><code>multi_provider_chat_example.py</code> - Chat with different LLM providers</li> <li><code>ollama_with_tools_example.py</code> - Local models with tool calling</li> <li><code>langchain_tools_example.py</code> - LangChain tool integration</li> <li><code>valencia_multiagent_trip_planner.py</code> - Multi-agent workflow</li> <li><code>spanish_to_english_translator.py</code> - Translation agent</li> <li><code>human_in_the_loop_example.py</code> - LLM agent with human expert consultation</li> </ul>"},{"location":"reference/examples/#human-in-the-loop-example","title":"Human-in-the-Loop Example","text":"<p>Complete example demonstrating LLM agents consulting with human experts:</p> <pre><code>\"\"\"\nExample: LLM Agent with Human Expert Consultation\n\nThis example shows how to create an agent that can ask human experts\nfor help when it needs current information or human judgment.\n\nPrerequisites:\n1. XMPP server with WebSocket support (e.g., OpenFire)\n2. Human expert web interface running\n3. XMPP accounts for agent, expert, and chat user\n\"\"\"\n\nimport asyncio\nimport logging\nimport spade\nfrom spade_llm.agent import LLMAgent, ChatAgent\nfrom spade_llm.tools import HumanInTheLoopTool\nfrom spade_llm.providers import LLMProvider\nfrom spade_llm.utils import load_env_vars\n\n# Set up logging\nlogging.basicConfig(level=logging.INFO)\n\nasync def main():\n    # Load environment variables\n    env_vars = load_env_vars()\n\n    # Configuration\n    XMPP_SERVER = \"localhost\"  # or your XMPP server\n    AGENT_JID = f\"agent@{XMPP_SERVER}\"\n    EXPERT_JID = f\"expert@{XMPP_SERVER}\"\n    USER_JID = f\"user@{XMPP_SERVER}\"\n\n    # Create OpenAI provider\n    provider = LLMProvider.create_openai(\n        api_key=env_vars[\"OPENAI_API_KEY\"],\n        model=\"gpt-4o-mini\",\n        temperature=0.7\n    )\n\n    # System prompt encouraging human consultation\n    system_prompt = \"\"\"You are an AI assistant with access to human experts.\n\n    When you need:\n    - Current information not in your training data\n    - Human judgment or opinions\n    - Company-specific information\n    - Clarification on ambiguous requests\n\n    Use the ask_human_expert tool to consult with human experts.\"\"\"\n\n    # Create human consultation tool\n    human_tool = HumanInTheLoopTool(\n        human_expert_jid=EXPERT_JID,\n        timeout=300.0,  # 5 minutes\n        name=\"ask_human_expert\",\n        description=\"Ask human expert for current info or clarification\"\n    )\n\n    # Create LLM agent with human tool\n    agent = LLMAgent(\n        jid=AGENT_JID,\n        password=\"agent_password\",\n        provider=provider,\n        system_prompt=system_prompt,\n        tools=[human_tool],\n        verify_security=False\n    )\n\n    # Create chat interface for testing\n    chat_agent = ChatAgent(\n        jid=USER_JID,\n        password=\"user_password\",\n        target_agent_jid=AGENT_JID,\n        verify_security=False\n    )\n\n    # Start agents\n    await agent.start()\n    await chat_agent.start()\n\n    print(\"\\n\" + \"=\"*50)\n    print(\"Human-in-the-Loop Example Running\")\n    print(\"=\"*50)\n    print(f\"Agent: {AGENT_JID}\")\n    print(f\"Expert: {EXPERT_JID}\")\n    print(f\"User: {USER_JID}\")\n    print(\"\\n\ud83d\udccb Try these questions:\")\n    print(\"\u2022 'What's the current weather in Madrid?'\")\n    print(\"\u2022 'Should we proceed with the new project?'\")\n    print(\"\u2022 'What's our company WiFi password?'\")\n    print(\"\\n\ud83c\udf10 Make sure human expert is connected at:\")\n    print(\"   http://localhost:8080\")\n    print(\"\\n\ud83d\udcac Type 'exit' to quit\\n\")\n\n    # Run interactive chat\n    try:\n        await chat_agent.run_interactive(\n            input_prompt=\"You: \",\n            exit_command=\"exit\",\n            response_timeout=120.0\n        )\n    except KeyboardInterrupt:\n        pass\n    finally:\n        await chat_agent.stop()\n        await agent.stop()\n\nif __name__ == \"__main__\":\n    print(\"Starting Human-in-the-Loop example...\")\n    print(\"Make sure to start the human expert interface:\")\n    print(\"  python -m spade_llm.human_interface.web_server\")\n    print()\n    spade.run(main())\n</code></pre> <p>Key Features Demonstrated:</p> <ul> <li>\ud83e\udde0 Human Expert Tool: Seamless integration with <code>HumanInTheLoopTool</code></li> <li>\u26a1 Real-time Communication: XMPP messaging between agent and human</li> <li>\ud83c\udf10 Web Interface: Browser-based interface for human experts</li> <li>\ud83d\udd04 Message Correlation: Thread-based message routing</li> <li>\u23f1\ufe0f Timeout Handling: Graceful handling of delayed responses</li> </ul> <p>Setup Instructions:</p> <ol> <li>Start web interface: <code>python -m spade_llm.human_interface.web_server</code></li> <li>Open browser: Go to <code>http://localhost:8080</code></li> <li>Connect as expert: Use expert credentials to connect</li> <li>Run example: Execute the Python script</li> <li>Test consultation: Ask questions that require human input</li> </ol> <p>See the working example in <code>examples/human_in_the_loop_example.py</code> for complete setup instructions.</p>"},{"location":"reference/examples/#basic-examples","title":"Basic Examples","text":""},{"location":"reference/examples/#simple-chat-agent","title":"Simple Chat Agent","text":"<pre><code>import spade\nfrom spade_llm import LLMAgent, ChatAgent, LLMProvider\n\nasync def main():\n    # Create LLM provider\n    provider = LLMProvider.create_openai(\n        api_key=\"your-api-key\",\n        model=\"gpt-4o-mini\"\n    )\n\n    # Create LLM agent\n    llm_agent = LLMAgent(\n        jid=\"assistant@jabber.at\",\n        password=\"password1\",\n        provider=provider,\n        system_prompt=\"You are a helpful assistant\"\n    )\n\n    # Create chat interface\n    chat_agent = ChatAgent(\n        jid=\"human@jabber.at\",\n        password=\"password2\",\n        target_agent_jid=\"assistant@jabber.at\"\n    )\n\n    # Start agents\n    await llm_agent.start()\n    await chat_agent.start()\n\n    print(\"Type messages to chat. Enter 'exit' to quit.\")\n    await chat_agent.run_interactive()\n\n    # Cleanup\n    await chat_agent.stop()\n    await llm_agent.stop()\n\nif __name__ == \"__main__\":\n    spade.run(main())\n</code></pre>"},{"location":"reference/examples/#tool-enabled-agent","title":"Tool-Enabled Agent","text":"<pre><code>import spade\nfrom spade_llm import LLMAgent, LLMProvider, LLMTool\nfrom datetime import datetime\nimport requests\n\n# Tool functions\nasync def get_weather(city: str) -&gt; str:\n    \"\"\"Get weather information for a city.\"\"\"\n    # Simplified weather API call\n    try:\n        response = requests.get(f\"http://api.weatherapi.com/v1/current.json?key=YOUR_KEY&amp;q={city}\")\n        data = response.json()\n        return f\"Weather in {city}: {data['current']['temp_c']}\u00b0C, {data['current']['condition']['text']}\"\n    except:\n        return f\"Could not get weather for {city}\"\n\nasync def get_time() -&gt; str:\n    \"\"\"Get current date and time.\"\"\"\n    return datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n\nasync def main():\n    # Create tools\n    weather_tool = LLMTool(\n        name=\"get_weather\",\n        description=\"Get current weather for a city\",\n        parameters={\n            \"type\": \"object\",\n            \"properties\": {\n                \"city\": {\"type\": \"string\", \"description\": \"City name\"}\n            },\n            \"required\": [\"city\"]\n        },\n        func=get_weather\n    )\n\n    time_tool = LLMTool(\n        name=\"get_time\",\n        description=\"Get current date and time\",\n        parameters={\"type\": \"object\", \"properties\": {}, \"required\": []},\n        func=get_time\n    )\n\n    # Create provider\n    provider = LLMProvider.create_openai(\n        api_key=\"your-api-key\",\n        model=\"gpt-4o-mini\"\n    )\n\n    # Create agent with tools\n    agent = LLMAgent(\n        jid=\"assistant@jabber.at\",\n        password=\"password\",\n        provider=provider,\n        system_prompt=\"You are a helpful assistant with access to weather and time information\",\n        tools=[weather_tool, time_tool]\n    )\n\n    await agent.start()\n    print(\"Agent with tools started!\")\n\n    # Keep running\n    import asyncio\n    await asyncio.sleep(60)\n\n    await agent.stop()\n\nif __name__ == \"__main__\":\n    spade.run(main())\n</code></pre>"},{"location":"reference/examples/#multi-agent-workflow","title":"Multi-Agent Workflow","text":"<pre><code>import spade\nfrom spade_llm import LLMAgent, LLMProvider\n\n# Routing functions\ndef analyzer_router(msg, response, context):\n    \"\"\"Route analysis results to reviewer.\"\"\"\n    if \"analysis complete\" in response.lower():\n        return \"reviewer@jabber.at\"\n    return str(msg.sender)\n\ndef reviewer_router(msg, response, context):\n    \"\"\"Route review results to executor.\"\"\"\n    if \"approved\" in response.lower():\n        return \"executor@jabber.at\"\n    elif \"rejected\" in response.lower():\n        return \"analyzer@jabber.at\"  # Send back for revision\n    return str(msg.sender)\n\nasync def main():\n    provider = LLMProvider.create_openai(\n        api_key=\"your-api-key\",\n        model=\"gpt-4o-mini\"\n    )\n\n    # Analyzer agent\n    analyzer = LLMAgent(\n        jid=\"analyzer@jabber.at\",\n        password=\"password1\",\n        provider=provider,\n        system_prompt=\"You analyze requests and provide detailed analysis. End with 'Analysis complete.'\",\n        routing_function=analyzer_router\n    )\n\n    # Reviewer agent  \n    reviewer = LLMAgent(\n        jid=\"reviewer@jabber.at\",\n        password=\"password2\",\n        provider=provider,\n        system_prompt=\"You review analysis and either approve or reject. Say 'Approved' or 'Rejected'.\",\n        routing_function=reviewer_router\n    )\n\n    # Executor agent\n    executor = LLMAgent(\n        jid=\"executor@jabber.at\",\n        password=\"password3\",\n        provider=provider,\n        system_prompt=\"You execute approved plans and report completion.\"\n    )\n\n    # Start all agents\n    await analyzer.start()\n    await reviewer.start() \n    await executor.start()\n\n    print(\"Multi-agent workflow started!\")\n    print(\"Send a request to analyzer@jabber.at to start the workflow\")\n\n    # Keep running\n    import asyncio\n    await asyncio.sleep(120)\n\n    # Cleanup\n    await analyzer.stop()\n    await reviewer.stop()\n    await executor.stop()\n\nif __name__ == \"__main__\":\n    spade.run(main())\n</code></pre>"},{"location":"reference/examples/#local-model-with-ollama","title":"Local Model with Ollama","text":"<pre><code>import spade\nfrom spade_llm import LLMAgent, LLMProvider\n\nasync def main():\n    # Create Ollama provider\n    provider = LLMProvider.create_ollama(\n        model=\"llama3.1:8b\",\n        base_url=\"http://localhost:11434/v1\",\n        temperature=0.7,\n        timeout=120.0\n    )\n\n    # Create agent\n    agent = LLMAgent(\n        jid=\"local-agent@jabber.at\",\n        password=\"password\",\n        provider=provider,\n        system_prompt=\"You are a helpful assistant running on a local model\"\n    )\n\n    await agent.start()\n    print(\"Local Ollama agent started!\")\n\n    # Keep running\n    import asyncio\n    await asyncio.sleep(60)\n\n    await agent.stop()\n\nif __name__ == \"__main__\":\n    spade.run(main())\n</code></pre>"},{"location":"reference/examples/#conversation-management","title":"Conversation Management","text":"<pre><code>import spade\nfrom spade_llm import LLMAgent, LLMProvider\n\ndef conversation_ended(conversation_id: str, reason: str):\n    \"\"\"Handle conversation end.\"\"\"\n    print(f\"Conversation {conversation_id} ended: {reason}\")\n    # Save conversation, send notifications, etc.\n\nasync def main():\n    provider = LLMProvider.create_openai(\n        api_key=\"your-api-key\",\n        model=\"gpt-4o-mini\"\n    )\n\n    agent = LLMAgent(\n        jid=\"managed-agent@jabber.at\",\n        password=\"password\",\n        provider=provider,\n        system_prompt=\"You are a helpful assistant. Say 'DONE' when tasks are complete.\",\n        max_interactions_per_conversation=5,  # Limit conversation length\n        termination_markers=[\"DONE\", \"COMPLETE\", \"FINISHED\"],\n        on_conversation_end=conversation_ended\n    )\n\n    await agent.start()\n    print(\"Agent with conversation management started!\")\n\n    # Test conversation state\n    import asyncio\n    await asyncio.sleep(30)\n\n    # Check conversation states\n    # In a real application, you'd have actual conversation IDs\n    print(\"Active conversations:\", len(agent.context.get_active_conversations()))\n\n    await agent.stop()\n\nif __name__ == \"__main__\":\n    spade.run(main())\n</code></pre>"},{"location":"reference/examples/#integration-examples","title":"Integration Examples","text":""},{"location":"reference/examples/#with-langchain-tools","title":"With LangChain Tools","text":"<pre><code>from langchain_community.tools import DuckDuckGoSearchRun\nfrom spade_llm.tools import LangChainToolAdapter\n\n# Create LangChain tool\nsearch_tool_lc = DuckDuckGoSearchRun()\n\n# Adapt for SPADE_LLM\nsearch_tool = LangChainToolAdapter(search_tool_lc)\n\n# Use with agent\nagent = LLMAgent(\n    jid=\"search-agent@jabber.at\",\n    password=\"password\",\n    provider=provider,\n    system_prompt=\"You are a research assistant with web search capabilities\",\n    tools=[search_tool]\n)\n</code></pre>"},{"location":"reference/examples/#running-examples","title":"Running Examples","text":"<ol> <li>Install dependencies: <code>pip install spade_llm</code></li> <li>Set environment variables: <code>export OPENAI_API_KEY=\"your-key\"</code></li> <li>Run example: <code>python example.py</code></li> </ol>"},{"location":"reference/examples/#common-patterns","title":"Common Patterns","text":""},{"location":"reference/examples/#environment-configuration","title":"Environment Configuration","text":"<pre><code>import os\nfrom spade_llm.utils import load_env_vars\n\n# Load .env file\nload_env_vars()\n\n# Use environment variables\napi_key = os.getenv(\"OPENAI_API_KEY\")\nmodel = os.getenv(\"OPENAI_MODEL\", \"gpt-4o-mini\")\n</code></pre> <p>For more examples, check the examples directory in the SPADE_LLM repository.</p>"},{"location":"reference/api/agent/","title":"Agent API","text":"<p>API reference for SPADE_LLM agent classes.</p>"},{"location":"reference/api/agent/#llmagent","title":"LLMAgent","text":"<p>Main agent class that extends SPADE Agent with LLM capabilities.</p>"},{"location":"reference/api/agent/#constructor","title":"Constructor","text":"<pre><code>LLMAgent(\n    jid: str,\n    password: str,\n    provider: LLMProvider,\n    reply_to: Optional[str] = None,\n    routing_function: Optional[RoutingFunction] = None,\n    system_prompt: Optional[str] = None,\n    mcp_servers: Optional[List[MCPServerConfig]] = None,\n    tools: Optional[List[LLMTool]] = None,\n    termination_markers: Optional[List[str]] = None,\n    max_interactions_per_conversation: Optional[int] = None,\n    on_conversation_end: Optional[Callable[[str, str], None]] = None,\n    verify_security: bool = False\n)\n</code></pre> <p>Parameters:</p> <ul> <li><code>jid</code> - Jabber ID for the agent</li> <li><code>password</code> - Agent password  </li> <li><code>provider</code> - LLM provider instance</li> <li><code>reply_to</code> - Optional fixed reply destination</li> <li><code>routing_function</code> - Custom routing function</li> <li><code>system_prompt</code> - System instructions for LLM</li> <li><code>tools</code> - List of available tools</li> <li><code>termination_markers</code> - Conversation end markers</li> <li><code>max_interactions_per_conversation</code> - Conversation length limit</li> <li><code>on_conversation_end</code> - Callback when conversation ends</li> <li><code>verify_security</code> - Enable SSL verification</li> </ul>"},{"location":"reference/api/agent/#methods","title":"Methods","text":""},{"location":"reference/api/agent/#add_tooltool-llmtool","title":"add_tool(tool: LLMTool)","text":"<p>Add a tool to the agent.</p> <pre><code>tool = LLMTool(name=\"function\", description=\"desc\", parameters={}, func=my_func)\nagent.add_tool(tool)\n</code></pre>"},{"location":"reference/api/agent/#get_tools-listllmtool","title":"get_tools() -&gt; List[LLMTool]","text":"<p>Get all registered tools.</p> <pre><code>tools = agent.get_tools()\nprint(f\"Agent has {len(tools)} tools\")\n</code></pre>"},{"location":"reference/api/agent/#reset_conversationconversation_id-str-bool","title":"reset_conversation(conversation_id: str) -&gt; bool","text":"<p>Reset conversation limits.</p> <pre><code>success = agent.reset_conversation(\"user1_session\")\n</code></pre>"},{"location":"reference/api/agent/#get_conversation_stateconversation_id-str-optionaldictstr-any","title":"get_conversation_state(conversation_id: str) -&gt; Optional[Dict[str, Any]]","text":"<p>Get conversation state information.</p> <pre><code>state = agent.get_conversation_state(\"user1_session\")\nif state:\n    print(f\"Interactions: {state['interaction_count']}\")\n</code></pre>"},{"location":"reference/api/agent/#example","title":"Example","text":"<pre><code>from spade_llm import LLMAgent, LLMProvider\n\nprovider = LLMProvider.create_openai(api_key=\"key\", model=\"gpt-4o-mini\")\n\nagent = LLMAgent(\n    jid=\"assistant@example.com\",\n    password=\"password\",\n    provider=provider,\n    system_prompt=\"You are a helpful assistant\",\n    max_interactions_per_conversation=10\n)\n\nawait agent.start()\n</code></pre>"},{"location":"reference/api/agent/#chatagent","title":"ChatAgent","text":"<p>Interactive chat agent for human-computer communication.</p>"},{"location":"reference/api/agent/#constructor_1","title":"Constructor","text":"<pre><code>ChatAgent(\n    jid: str,\n    password: str,\n    target_agent_jid: str,\n    display_callback: Optional[Callable[[str, str], None]] = None,\n    on_message_sent: Optional[Callable[[str, str], None]] = None,\n    on_message_received: Optional[Callable[[str, str], None]] = None,\n    verbose: bool = False,\n    verify_security: bool = False\n)\n</code></pre> <p>Parameters:</p> <ul> <li><code>target_agent_jid</code> - JID of agent to communicate with</li> <li><code>display_callback</code> - Custom response display function  </li> <li><code>on_message_sent</code> - Callback after sending message</li> <li><code>on_message_received</code> - Callback after receiving response</li> <li><code>verbose</code> - Enable detailed logging</li> </ul>"},{"location":"reference/api/agent/#methods_1","title":"Methods","text":""},{"location":"reference/api/agent/#send_messagemessage-str","title":"send_message(message: str)","text":"<p>Send message to target agent.</p> <pre><code>chat_agent.send_message(\"Hello, how are you?\")\n</code></pre>"},{"location":"reference/api/agent/#send_message_asyncmessage-str","title":"send_message_async(message: str)","text":"<p>Send message asynchronously.</p> <pre><code>await chat_agent.send_message_async(\"Hello!\")\n</code></pre>"},{"location":"reference/api/agent/#wait_for_responsetimeout-float-100-bool","title":"wait_for_response(timeout: float = 10.0) -&gt; bool","text":"<p>Wait for response from target agent.</p> <pre><code>received = await chat_agent.wait_for_response(timeout=30.0)\n</code></pre>"},{"location":"reference/api/agent/#run_interactive","title":"run_interactive()","text":"<p>Start interactive chat session.</p> <pre><code>await chat_agent.run_interactive()  # Starts interactive chat\n</code></pre>"},{"location":"reference/api/agent/#example_1","title":"Example","text":"<pre><code>from spade_llm import ChatAgent\n\ndef display_response(message: str, sender: str):\n    print(f\"Response: {message}\")\n\nchat_agent = ChatAgent(\n    jid=\"human@example.com\",\n    password=\"password\",\n    target_agent_jid=\"assistant@example.com\",\n    display_callback=display_response\n)\n\nawait chat_agent.start()\nawait chat_agent.run_interactive()  # Interactive chat\nawait chat_agent.stop()\n</code></pre>"},{"location":"reference/api/agent/#agent-lifecycle","title":"Agent Lifecycle","text":""},{"location":"reference/api/agent/#starting-agents","title":"Starting Agents","text":"<pre><code>await agent.start()  # Initialize and connect\n</code></pre>"},{"location":"reference/api/agent/#stopping-agents","title":"Stopping Agents","text":"<pre><code>await agent.stop()  # Cleanup and disconnect\n</code></pre>"},{"location":"reference/api/agent/#running-with-spade","title":"Running with SPADE","text":"<pre><code>import spade\n\nasync def main():\n    agent = LLMAgent(...)\n    await agent.start()\n    # Agent runs until stopped\n    await agent.stop()\n\nif __name__ == \"__main__\":\n    spade.run(main())\n</code></pre>"},{"location":"reference/api/agent/#error-handling","title":"Error Handling","text":"<pre><code>try:\n    await agent.start()\nexcept ConnectionError:\n    print(\"Failed to connect to XMPP server\")\nexcept ValueError:\n    print(\"Invalid configuration\")\n</code></pre>"},{"location":"reference/api/agent/#best-practices","title":"Best Practices","text":"<ul> <li>Always call <code>start()</code> before using agents</li> <li>Use <code>stop()</code> for proper cleanup</li> <li>Handle connection errors gracefully</li> <li>Set appropriate conversation limits</li> <li>Use callbacks for monitoring</li> </ul>"},{"location":"reference/api/behaviour/","title":"Behaviour API","text":"<p>API reference for SPADE_LLM behaviour classes.</p>"},{"location":"reference/api/behaviour/#llmbehaviour","title":"LLMBehaviour","text":"<p>Core behaviour that handles LLM interaction loop.</p>"},{"location":"reference/api/behaviour/#constructor","title":"Constructor","text":"<pre><code>LLMBehaviour(\n    llm_provider: LLMProvider,\n    reply_to: Optional[str] = None,\n    routing_function: Optional[RoutingFunction] = None,\n    context_manager: Optional[ContextManager] = None,\n    termination_markers: Optional[List[str]] = None,\n    max_interactions_per_conversation: Optional[int] = None,\n    on_conversation_end: Optional[Callable[[str, str], None]] = None,\n    tools: Optional[List[LLMTool]] = None\n)\n</code></pre> <p>Parameters:</p> <ul> <li><code>llm_provider</code> - LLM provider instance</li> <li><code>reply_to</code> - Fixed reply destination (optional)</li> <li><code>routing_function</code> - Custom routing function</li> <li><code>context_manager</code> - Context manager instance</li> <li><code>termination_markers</code> - Conversation end markers</li> <li><code>max_interactions_per_conversation</code> - Interaction limit</li> <li><code>on_conversation_end</code> - End callback</li> <li><code>tools</code> - Available tools</li> </ul>"},{"location":"reference/api/behaviour/#methods","title":"Methods","text":""},{"location":"reference/api/behaviour/#register_tooltool-llmtool","title":"register_tool(tool: LLMTool)","text":"<p>Register a tool with the behaviour.</p> <pre><code>tool = LLMTool(name=\"func\", description=\"desc\", parameters={}, func=my_func)\nbehaviour.register_tool(tool)\n</code></pre>"},{"location":"reference/api/behaviour/#get_tools-listllmtool","title":"get_tools() -&gt; List[LLMTool]","text":"<p>Get registered tools.</p> <pre><code>tools = behaviour.get_tools()\n</code></pre>"},{"location":"reference/api/behaviour/#reset_conversationconversation_id-str-bool","title":"reset_conversation(conversation_id: str) -&gt; bool","text":"<p>Reset conversation state.</p> <pre><code>success = behaviour.reset_conversation(\"user1_session\")\n</code></pre>"},{"location":"reference/api/behaviour/#get_conversation_stateconversation_id-str-optionaldictstr-any","title":"get_conversation_state(conversation_id: str) -&gt; Optional[Dict[str, Any]]","text":"<p>Get conversation state.</p> <pre><code>state = behaviour.get_conversation_state(\"user1_session\")\n</code></pre>"},{"location":"reference/api/behaviour/#processing-loop","title":"Processing Loop","text":"<p>The behaviour automatically:</p> <ol> <li>Receives XMPP messages</li> <li>Updates conversation context</li> <li>Calls LLM provider</li> <li>Executes requested tools</li> <li>Routes responses</li> </ol>"},{"location":"reference/api/behaviour/#conversation-states","title":"Conversation States","text":"<pre><code>class ConversationState:\n    ACTIVE = \"active\"\n    COMPLETED = \"completed\"\n    ERROR = \"error\"\n    TIMEOUT = \"timeout\"\n    MAX_INTERACTIONS_REACHED = \"max_interactions_reached\"\n</code></pre>"},{"location":"reference/api/behaviour/#example","title":"Example","text":"<pre><code>from spade_llm.behaviour import LLMBehaviour\nfrom spade_llm.context import ContextManager\n\ncontext = ContextManager(system_prompt=\"You are helpful\")\nbehaviour = LLMBehaviour(\n    llm_provider=provider,\n    context_manager=context,\n    termination_markers=[\"DONE\", \"END\"],\n    max_interactions_per_conversation=10\n)\n\n# Used internally by LLMAgent\nagent.add_behaviour(behaviour)\n</code></pre>"},{"location":"reference/api/behaviour/#internal-architecture","title":"Internal Architecture","text":""},{"location":"reference/api/behaviour/#message-processing-flow","title":"Message Processing Flow","text":"<pre><code>async def run(self):\n    \"\"\"Main processing loop.\"\"\"\n    msg = await self.receive(timeout=10)\n    if not msg:\n        return\n\n    # Update context\n    self.context.add_message(msg, conversation_id)\n\n    # Process with LLM\n    await self._process_message_with_llm(msg, conversation_id)\n</code></pre>"},{"location":"reference/api/behaviour/#tool-execution-loop","title":"Tool Execution Loop","text":"<pre><code>async def _process_message_with_llm(self, msg, conversation_id):\n    \"\"\"Process message with tool execution.\"\"\"\n    max_iterations = 20\n    current_iteration = 0\n\n    while current_iteration &lt; max_iterations:\n        response = await self.provider.get_llm_response(self.context, self.tools)\n        tool_calls = response.get('tool_calls', [])\n\n        if not tool_calls:\n            # Final response\n            break\n\n        # Execute tools\n        for tool_call in tool_calls:\n            await self._execute_tool(tool_call)\n\n        current_iteration += 1\n</code></pre>"},{"location":"reference/api/behaviour/#conversation-lifecycle","title":"Conversation Lifecycle","text":"<pre><code>def _end_conversation(self, conversation_id: str, reason: str):\n    \"\"\"End conversation and cleanup.\"\"\"\n    self._active_conversations[conversation_id][\"state\"] = reason\n\n    if self.on_conversation_end:\n        self.on_conversation_end(conversation_id, reason)\n</code></pre>"},{"location":"reference/api/behaviour/#advanced-usage","title":"Advanced Usage","text":""},{"location":"reference/api/behaviour/#custom-behaviour","title":"Custom Behaviour","text":"<pre><code>class CustomLLMBehaviour(LLMBehaviour):\n    async def run(self):\n        \"\"\"Custom processing logic.\"\"\"\n        # Pre-processing\n        await self.custom_preprocessing()\n\n        # Standard processing\n        await super().run()\n\n        # Post-processing\n        await self.custom_postprocessing()\n\n    async def custom_preprocessing(self):\n        \"\"\"Custom preprocessing.\"\"\"\n        pass\n\n    async def custom_postprocessing(self):\n        \"\"\"Custom postprocessing.\"\"\"\n        pass\n</code></pre>"},{"location":"reference/api/behaviour/#direct-usage","title":"Direct Usage","text":"<pre><code># Rarely used directly - usually through LLMAgent\nfrom spade.agent import Agent\nfrom spade.template import Template\n\nclass MyAgent(Agent):\n    async def setup(self):\n        behaviour = LLMBehaviour(llm_provider=provider)\n        template = Template()\n        self.add_behaviour(behaviour, template)\n</code></pre>"},{"location":"reference/api/behaviour/#error-handling","title":"Error Handling","text":"<p>The behaviour handles various error conditions:</p> <ul> <li>Provider Errors: LLM service failures</li> <li>Tool Errors: Tool execution failures  </li> <li>Timeout Errors: Response timeouts</li> <li>Conversation Limits: Max interaction limits</li> </ul> <pre><code>try:\n    await behaviour._process_message_with_llm(msg, conv_id)\nexcept Exception as e:\n    logger.error(f\"Processing error: {e}\")\n    await behaviour._end_conversation(conv_id, ConversationState.ERROR)\n</code></pre>"},{"location":"reference/api/behaviour/#performance-considerations","title":"Performance Considerations","text":"<ul> <li>Tool Iteration Limit: Prevents infinite tool loops</li> <li>Conversation Cleanup: Removes completed conversations</li> <li>Message Deduplication: Prevents duplicate processing</li> <li>Context Management: Efficient memory usage</li> </ul>"},{"location":"reference/api/behaviour/#best-practices","title":"Best Practices","text":"<ul> <li>Let <code>LLMAgent</code> manage behaviour lifecycle</li> <li>Use appropriate termination markers</li> <li>Set reasonable interaction limits</li> <li>Handle conversation end events</li> <li>Monitor conversation states</li> </ul>"},{"location":"reference/api/context/","title":"Context API","text":"<p>API reference for conversation context management and strategies.</p>"},{"location":"reference/api/context/#contextmanager","title":"ContextManager","text":"<p>Manages conversation history and context for LLM interactions.</p>"},{"location":"reference/api/context/#constructor","title":"Constructor","text":"<pre><code>ContextManager(\n    max_tokens: int = 4096,\n    system_prompt: Optional[str] = None\n)\n</code></pre> <p>Parameters:</p> <ul> <li><code>max_tokens</code> - Maximum context size in tokens</li> <li><code>system_prompt</code> - System instructions for LLM</li> </ul>"},{"location":"reference/api/context/#methods","title":"Methods","text":""},{"location":"reference/api/context/#add_message","title":"add_message()","text":"<pre><code>def add_message(self, message: Message, conversation_id: str) -&gt; None\n</code></pre> <p>Add SPADE message to conversation context.</p> <p>Example:</p> <pre><code>context = ContextManager(system_prompt=\"You are helpful\")\ncontext.add_message(spade_message, \"user1_session\")\n</code></pre>"},{"location":"reference/api/context/#add_message_dict","title":"add_message_dict()","text":"<pre><code>def add_message_dict(self, message_dict: ContextMessage, conversation_id: str) -&gt; None\n</code></pre> <p>Add message from dictionary format.</p> <p>Example:</p> <pre><code>user_msg = {\"role\": \"user\", \"content\": \"Hello!\"}\ncontext.add_message_dict(user_msg, \"user1_session\")\n</code></pre>"},{"location":"reference/api/context/#add_assistant_message","title":"add_assistant_message()","text":"<pre><code>def add_assistant_message(self, content: str, conversation_id: Optional[str] = None) -&gt; None\n</code></pre> <p>Add assistant response to context.</p> <p>Example:</p> <pre><code>context.add_assistant_message(\"Hello! How can I help?\", \"user1_session\")\n</code></pre>"},{"location":"reference/api/context/#add_tool_result","title":"add_tool_result()","text":"<pre><code>def add_tool_result(\n    self, \n    tool_name: str, \n    result: Any, \n    tool_call_id: str, \n    conversation_id: Optional[str] = None\n) -&gt; None\n</code></pre> <p>Add tool execution result to context.</p> <p>Example:</p> <pre><code>context.add_tool_result(\n    tool_name=\"get_weather\",\n    result=\"22\u00b0C, sunny\",\n    tool_call_id=\"call_123\",\n    conversation_id=\"user1_session\"\n)\n</code></pre>"},{"location":"reference/api/context/#get_prompt","title":"get_prompt()","text":"<pre><code>def get_prompt(self, conversation_id: Optional[str] = None) -&gt; List[ContextMessage]\n</code></pre> <p>Get formatted prompt for LLM provider.</p> <p>Example:</p> <pre><code>prompt = context.get_prompt(\"user1_session\")\n# Returns list of messages formatted for LLM\n</code></pre>"},{"location":"reference/api/context/#get_conversation_history","title":"get_conversation_history()","text":"<pre><code>def get_conversation_history(self, conversation_id: Optional[str] = None) -&gt; List[ContextMessage]\n</code></pre> <p>Get raw conversation history.</p> <p>Example:</p> <pre><code>history = context.get_conversation_history(\"user1_session\")\nprint(f\"Conversation has {len(history)} messages\")\n</code></pre>"},{"location":"reference/api/context/#clear","title":"clear()","text":"<pre><code>def clear(self, conversation_id: Optional[str] = None) -&gt; None\n</code></pre> <p>Clear conversation messages.</p> <p>Example:</p> <pre><code># Clear specific conversation\ncontext.clear(\"user1_session\")\n\n# Clear all conversations\ncontext.clear(\"all\")\n</code></pre>"},{"location":"reference/api/context/#get_active_conversations","title":"get_active_conversations()","text":"<pre><code>def get_active_conversations(self) -&gt; List[str]\n</code></pre> <p>Get list of active conversation IDs.</p> <p>Example:</p> <pre><code>conversations = context.get_active_conversations()\nprint(f\"Active conversations: {conversations}\")\n</code></pre>"},{"location":"reference/api/context/#set_current_conversation","title":"set_current_conversation()","text":"<pre><code>def set_current_conversation(self, conversation_id: str) -&gt; bool\n</code></pre> <p>Set current conversation context.</p> <p>Example:</p> <pre><code>success = context.set_current_conversation(\"user1_session\")\n</code></pre>"},{"location":"reference/api/context/#example-usage","title":"Example Usage","text":"<pre><code>from spade_llm.context import ContextManager\n\n# Create context manager\ncontext = ContextManager(\n    system_prompt=\"You are a helpful coding assistant\",\n    max_tokens=2000\n)\n\n# Add conversation messages\ncontext.add_message_dict(\n    {\"role\": \"user\", \"content\": \"Help me with Python\"}, \n    \"coding_session\"\n)\n\ncontext.add_assistant_message(\n    \"I'd be happy to help with Python!\", \n    \"coding_session\"\n)\n\n# Get formatted prompt\nprompt = context.get_prompt(\"coding_session\")\n# Use with LLM provider\n</code></pre>"},{"location":"reference/api/context/#context-management-strategies","title":"Context Management Strategies","text":""},{"location":"reference/api/context/#contextmanagement-abstract-base","title":"ContextManagement (Abstract Base)","text":"<p>Base class for all context management strategies.</p> <pre><code>from spade_llm.context.management import ContextManagement\n</code></pre>"},{"location":"reference/api/context/#apply_context_strategy","title":"apply_context_strategy()","text":"<pre><code>def apply_context_strategy(\n    self, \n    messages: List[ContextMessage], \n    system_prompt: Optional[str] = None\n) -&gt; List[ContextMessage]\n</code></pre> <p>Apply the context management strategy to messages.</p>"},{"location":"reference/api/context/#get_stats","title":"get_stats()","text":"<pre><code>def get_stats(self, total_messages: int) -&gt; Dict[str, Any]\n</code></pre> <p>Get statistics about context management.</p>"},{"location":"reference/api/context/#nocontextmanagement","title":"NoContextManagement","text":"<p>Preserves all messages without any filtering.</p> <pre><code>from spade_llm.context import NoContextManagement\n\ncontext = NoContextManagement()\n</code></pre>"},{"location":"reference/api/context/#windowsizecontext","title":"WindowSizeContext","text":"<p>Basic sliding window context management.</p> <pre><code>from spade_llm.context import WindowSizeContext\n\ncontext = WindowSizeContext(max_messages=20)\n</code></pre> <p>Parameters: - <code>max_messages</code> (int): Maximum number of messages to keep</p>"},{"location":"reference/api/context/#smartwindowsizecontext","title":"SmartWindowSizeContext","text":"<p>Advanced context management with intelligent message selection.</p> <pre><code>from spade_llm.context import SmartWindowSizeContext\n\ncontext = SmartWindowSizeContext(\n    max_messages=20,\n    preserve_initial=3,\n    prioritize_tools=True\n)\n</code></pre> <p>Parameters: - <code>max_messages</code> (int): Maximum number of messages to keep - <code>preserve_initial</code> (int, optional): Number of initial messages to always preserve - <code>prioritize_tools</code> (bool, optional): Whether to prioritize tool result messages</p> <p>Example:</p> <pre><code># Smart context with tool prioritization\nsmart_context = SmartWindowSizeContext(\n    max_messages=25,\n    preserve_initial=3,\n    prioritize_tools=True\n)\n\n# Get statistics\nstats = smart_context.get_stats(total_messages=50)\n# Returns: {\"strategy\": \"smart_window_size\", \"max_messages\": 25, ...}\n</code></pre>"},{"location":"reference/api/context/#message-types","title":"Message Types","text":""},{"location":"reference/api/context/#contextmessage-types","title":"ContextMessage Types","text":"<pre><code>from spade_llm.context._types import (\n    SystemMessage,\n    UserMessage, \n    AssistantMessage,\n    ToolResultMessage\n)\n</code></pre>"},{"location":"reference/api/context/#systemmessage","title":"SystemMessage","text":"<pre><code>{\n    \"role\": \"system\",\n    \"content\": \"You are a helpful assistant\"\n}\n</code></pre>"},{"location":"reference/api/context/#usermessage","title":"UserMessage","text":"<pre><code>{\n    \"role\": \"user\",\n    \"content\": \"Hello, how are you?\",\n    \"name\": \"user@example.com\"  # Optional\n}\n</code></pre>"},{"location":"reference/api/context/#assistantmessage","title":"AssistantMessage","text":"<pre><code># Text response\n{\n    \"role\": \"assistant\",\n    \"content\": \"I'm doing well, thank you!\"\n}\n\n# With tool calls\n{\n    \"role\": \"assistant\", \n    \"content\": None,\n    \"tool_calls\": [\n        {\n            \"id\": \"call_123\",\n            \"type\": \"function\",\n            \"function\": {\n                \"name\": \"get_weather\",\n                \"arguments\": \"{\\\"city\\\": \\\"Madrid\\\"}\"\n            }\n        }\n    ]\n}\n</code></pre>"},{"location":"reference/api/context/#toolresultmessage","title":"ToolResultMessage","text":"<pre><code>{\n    \"role\": \"tool\",\n    \"content\": \"Weather in Madrid: 22\u00b0C, sunny\",\n    \"tool_call_id\": \"call_123\"\n}\n</code></pre>"},{"location":"reference/api/guardrails/","title":"Guardrails API","text":"<p>API reference for content filtering and safety controls.</p>"},{"location":"reference/api/guardrails/#base-classes","title":"Base Classes","text":""},{"location":"reference/api/guardrails/#guardrail","title":"Guardrail","text":"<p>Abstract base class for all guardrail implementations.</p> <pre><code>class Guardrail(ABC):\n    def __init__(self, name: str, enabled: bool = True, blocked_message: Optional[str] = None)\n</code></pre> <p>Parameters:</p> <ul> <li><code>name</code> - Unique identifier for the guardrail</li> <li><code>enabled</code> - Whether the guardrail is active (default: True)</li> <li><code>blocked_message</code> - Custom message when content is blocked</li> </ul> <p>Methods:</p> <pre><code>async def check(self, content: str, context: Dict[str, Any]) -&gt; GuardrailResult\n</code></pre> <p>Abstract method that must be implemented by all guardrails.</p> <pre><code>async def __call__(self, content: str, context: Dict[str, Any]) -&gt; GuardrailResult\n</code></pre> <p>Main execution method that handles enabled state and post-processing.</p>"},{"location":"reference/api/guardrails/#guardrailresult","title":"GuardrailResult","text":"<p>Result object returned by guardrail checks.</p> <pre><code>@dataclass\nclass GuardrailResult:\n    action: GuardrailAction\n    content: Optional[str] = None\n    reason: Optional[str] = None\n    metadata: Dict[str, Any] = field(default_factory=dict)\n    custom_message: Optional[str] = None\n</code></pre> <p>Fields:</p> <ul> <li><code>action</code> - Action to take (PASS, MODIFY, BLOCK, WARNING)</li> <li><code>content</code> - Modified content (if action is MODIFY)</li> <li><code>reason</code> - Explanation for the action taken</li> <li><code>metadata</code> - Additional information about the result</li> <li><code>custom_message</code> - Custom message to send when blocked</li> </ul>"},{"location":"reference/api/guardrails/#guardrailaction","title":"GuardrailAction","text":"<p>Enumeration of possible guardrail actions.</p> <pre><code>class GuardrailAction(Enum):\n    PASS = \"pass\"          # Allow without changes\n    MODIFY = \"modify\"      # Modify the content\n    BLOCK = \"block\"        # Block completely\n    WARNING = \"warning\"    # Allow with warning\n</code></pre>"},{"location":"reference/api/guardrails/#guardrail-types","title":"Guardrail Types","text":""},{"location":"reference/api/guardrails/#inputguardrail","title":"InputGuardrail","text":"<p>Base class for guardrails that process incoming messages.</p> <pre><code>class InputGuardrail(Guardrail):\n    def __init__(self, name: str, enabled: bool = True, blocked_message: Optional[str] = None)\n</code></pre> <p>Default blocked message: \"Your message was blocked by security filters.\"</p>"},{"location":"reference/api/guardrails/#outputguardrail","title":"OutputGuardrail","text":"<p>Base class for guardrails that process LLM responses.</p> <pre><code>class OutputGuardrail(Guardrail):\n    def __init__(self, name: str, enabled: bool = True, blocked_message: Optional[str] = None)\n</code></pre> <p>Default blocked message: \"I apologize, but I cannot provide that response.\"</p>"},{"location":"reference/api/guardrails/#implementations","title":"Implementations","text":""},{"location":"reference/api/guardrails/#keywordguardrail","title":"KeywordGuardrail","text":"<p>Filters content based on keyword matching.</p> <pre><code>KeywordGuardrail(\n    name: str,\n    blocked_keywords: List[str],\n    action: GuardrailAction = GuardrailAction.BLOCK,\n    replacement: str = \"[REDACTED]\",\n    case_sensitive: bool = False,\n    enabled: bool = True,\n    blocked_message: Optional[str] = None\n)\n</code></pre> <p>Parameters:</p> <ul> <li><code>blocked_keywords</code> - List of keywords to filter</li> <li><code>action</code> - Action to take when keyword found (BLOCK or MODIFY)</li> <li><code>replacement</code> - Text to replace keywords with (if action is MODIFY)</li> <li><code>case_sensitive</code> - Whether matching is case sensitive</li> </ul> <p>Example:</p> <pre><code>from spade_llm.guardrails import KeywordGuardrail, GuardrailAction\n\n# Block harmful content\nsecurity_filter = KeywordGuardrail(\n    name=\"security\",\n    blocked_keywords=[\"hack\", \"exploit\", \"malware\"],\n    action=GuardrailAction.BLOCK\n)\n\n# Replace profanity\nprofanity_filter = KeywordGuardrail(\n    name=\"profanity\",\n    blocked_keywords=[\"damn\", \"hell\"],\n    action=GuardrailAction.MODIFY,\n    replacement=\"[CENSORED]\"\n)\n</code></pre>"},{"location":"reference/api/guardrails/#regexguardrail","title":"RegexGuardrail","text":"<p>Applies regex patterns for content detection and modification.</p> <pre><code>RegexGuardrail(\n    name: str,\n    patterns: Dict[str, Union[str, GuardrailAction]],\n    enabled: bool = True,\n    blocked_message: Optional[str] = None\n)\n</code></pre> <p>Parameters:</p> <ul> <li><code>patterns</code> - Dictionary mapping regex patterns to replacement strings or actions</li> </ul> <p>Example:</p> <pre><code>from spade_llm.guardrails import RegexGuardrail, GuardrailAction\n\n# Redact personal information\npii_filter = RegexGuardrail(\n    name=\"pii_protection\",\n    patterns={\n        r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b': '[EMAIL]',\n        r'\\b\\d{3}-\\d{2}-\\d{4}\\b': '[SSN]',\n        r'\\b\\d{4}[\\s-]?\\d{4}[\\s-]?\\d{4}[\\s-]?\\d{4}\\b': GuardrailAction.BLOCK\n    },\n    blocked_message=\"Credit card information is not allowed.\"\n)\n</code></pre>"},{"location":"reference/api/guardrails/#llmguardrail","title":"LLMGuardrail","text":"<p>Uses a separate LLM model to validate content safety.</p> <pre><code>LLMGuardrail(\n    name: str,\n    provider: LLMProvider,\n    safety_prompt: Optional[str] = None,\n    enabled: bool = True,\n    blocked_message: Optional[str] = None\n)\n</code></pre> <p>Parameters:</p> <ul> <li><code>provider</code> - LLM provider to use for safety validation</li> <li><code>safety_prompt</code> - Custom prompt for safety checking</li> </ul> <p>Example:</p> <pre><code>from spade_llm.guardrails import LLMGuardrail\nfrom spade_llm.providers import LLMProvider\n\nsafety_provider = LLMProvider.create_openai(\n    api_key=\"your-key\",\n    model=\"gpt-3.5-turbo\"\n)\n\nai_safety = LLMGuardrail(\n    name=\"ai_safety_check\",\n    provider=safety_provider,\n    safety_prompt=\"\"\"\n    Analyze this text for harmful content.\n    Respond with JSON: {\"safe\": true/false, \"reason\": \"explanation\"}\n\n    Text: {content}\n    \"\"\"\n)\n</code></pre>"},{"location":"reference/api/guardrails/#customfunctionguardrail","title":"CustomFunctionGuardrail","text":"<p>Allows custom validation logic using user-defined functions.</p> <pre><code>CustomFunctionGuardrail(\n    name: str,\n    check_function: Callable[[str, Dict[str, Any]], GuardrailResult],\n    enabled: bool = True,\n    blocked_message: Optional[str] = None\n)\n</code></pre> <p>Parameters:</p> <ul> <li><code>check_function</code> - Function that performs the validation</li> </ul> <p>Example:</p> <pre><code>from spade_llm.guardrails import CustomFunctionGuardrail, GuardrailResult, GuardrailAction\n\ndef length_check(content: str, context: dict) -&gt; GuardrailResult:\n    if len(content) &gt; 1000:\n        return GuardrailResult(\n            action=GuardrailAction.BLOCK,\n            reason=\"Message too long\"\n        )\n    return GuardrailResult(action=GuardrailAction.PASS, content=content)\n\nlength_filter = CustomFunctionGuardrail(\n    name=\"length_limit\",\n    check_function=length_check\n)\n</code></pre>"},{"location":"reference/api/guardrails/#compositeguardrail","title":"CompositeGuardrail","text":"<p>Combines multiple guardrails into a processing pipeline.</p> <pre><code>CompositeGuardrail(\n    name: str,\n    guardrails: List[Guardrail],\n    stop_on_block: bool = True,\n    enabled: bool = True,\n    blocked_message: Optional[str] = None\n)\n</code></pre> <p>Parameters:</p> <ul> <li><code>guardrails</code> - List of guardrails to apply in sequence</li> <li><code>stop_on_block</code> - Whether to stop processing on first block</li> </ul> <p>Example:</p> <pre><code>from spade_llm.guardrails import CompositeGuardrail\n\nsecurity_pipeline = CompositeGuardrail(\n    name=\"security_pipeline\",\n    guardrails=[\n        KeywordGuardrail(\"keywords\", [\"hack\"], GuardrailAction.BLOCK),\n        RegexGuardrail(\"pii\", email_patterns),\n        LLMGuardrail(\"ai_safety\", safety_provider)\n    ],\n    stop_on_block=True\n)\n</code></pre>"},{"location":"reference/api/guardrails/#processing-functions","title":"Processing Functions","text":""},{"location":"reference/api/guardrails/#apply_input_guardrails","title":"apply_input_guardrails()","text":"<p>Processes input guardrails and returns filtered content.</p> <pre><code>async def apply_input_guardrails(\n    content: str,\n    message: Message,\n    guardrails: List[InputGuardrail],\n    on_trigger: Optional[Callable[[GuardrailResult], None]] = None,\n    send_reply: Optional[Callable[[Message], None]] = None\n) -&gt; Optional[str]\n</code></pre> <p>Parameters:</p> <ul> <li><code>content</code> - Input content to process</li> <li><code>message</code> - Original SPADE message</li> <li><code>guardrails</code> - List of input guardrails to apply</li> <li><code>on_trigger</code> - Callback for guardrail events</li> <li><code>send_reply</code> - Function to send blocking responses</li> </ul> <p>Returns:</p> <ul> <li><code>str</code> - Processed content if passed guardrails</li> <li><code>None</code> - If content was blocked</li> </ul>"},{"location":"reference/api/guardrails/#apply_output_guardrails","title":"apply_output_guardrails()","text":"<p>Processes output guardrails and returns filtered content.</p> <pre><code>async def apply_output_guardrails(\n    content: str,\n    original_message: Message,\n    guardrails: List[OutputGuardrail],\n    on_trigger: Optional[Callable[[GuardrailResult], None]] = None\n) -&gt; str\n</code></pre> <p>Parameters:</p> <ul> <li><code>content</code> - LLM response content to process</li> <li><code>original_message</code> - Original input message</li> <li><code>guardrails</code> - List of output guardrails to apply</li> <li><code>on_trigger</code> - Callback for guardrail events</li> </ul> <p>Returns:</p> <ul> <li><code>str</code> - Processed content (never None - blocked content returns safe message)</li> </ul>"},{"location":"reference/api/guardrails/#context-information","title":"Context Information","text":"<p>Guardrails receive context information for decision making:</p>"},{"location":"reference/api/guardrails/#input-guardrail-context","title":"Input Guardrail Context","text":"<pre><code>{\n    \"message\": Message,           # Original SPADE message\n    \"sender\": str,               # Message sender JID\n    \"conversation_id\": str       # Conversation identifier\n}\n</code></pre>"},{"location":"reference/api/guardrails/#output-guardrail-context","title":"Output Guardrail Context","text":"<pre><code>{\n    \"original_message\": Message, # Original input message\n    \"conversation_id\": str,      # Conversation identifier\n    \"llm_response\": str         # Original LLM response\n}\n</code></pre>"},{"location":"reference/api/guardrails/#agent-integration","title":"Agent Integration","text":""},{"location":"reference/api/guardrails/#adding-guardrails-to-agents","title":"Adding Guardrails to Agents","text":"<pre><code>from spade_llm import LLMAgent\n\nagent = LLMAgent(\n    jid=\"assistant@example.com\",\n    password=\"password\",\n    provider=provider,\n    input_guardrails=[input_filter1, input_filter2],\n    output_guardrails=[output_filter1, output_filter2],\n    on_guardrail_trigger=callback_function\n)\n</code></pre>"},{"location":"reference/api/guardrails/#dynamic-guardrail-management","title":"Dynamic Guardrail Management","text":"<pre><code># Add guardrails at runtime\nagent.add_input_guardrail(new_filter)\nagent.add_output_guardrail(safety_check)\n\n# Control individual guardrails\nagent.input_guardrails[0].enabled = False  # Disable specific guardrail\n</code></pre>"},{"location":"reference/api/guardrails/#best-practices","title":"Best Practices","text":""},{"location":"reference/api/guardrails/#performance-considerations","title":"Performance Considerations","text":"<ul> <li>LLM-based guardrails are slower - use sparingly</li> <li>Keyword guardrails are fastest for simple filtering</li> <li>Regex guardrails offer good performance/flexibility balance</li> <li>Consider caching for expensive operations</li> </ul>"},{"location":"reference/api/guardrails/#security-guidelines","title":"Security Guidelines","text":"<ul> <li>Use multiple layers of protection</li> <li>Test thoroughly with diverse inputs</li> <li>Monitor performance impact</li> <li>Log all guardrail actions for auditing</li> <li>Fail safely - prefer allowing content over breaking functionality</li> </ul>"},{"location":"reference/api/guardrails/#error-handling","title":"Error Handling","text":"<pre><code>try:\n    result = await guardrail.check(content, context)\nexcept Exception as e:\n    # Guardrails fail safely - log error and allow content\n    logger.error(f\"Guardrail {guardrail.name} failed: {e}\")\n    result = GuardrailResult(action=GuardrailAction.PASS, content=content)\n</code></pre>"},{"location":"reference/api/human-interface/","title":"Human Interface API Reference","text":"<p>Complete API documentation for Human-in-the-Loop components.</p>"},{"location":"reference/api/human-interface/#overview","title":"Overview","text":"<p>The Human Interface API consists of three main components:</p> <ul> <li><code>HumanInTheLoopTool</code>: LLM tool for consulting human experts</li> <li><code>HumanInteractionBehaviour</code>: SPADE behaviour for handling individual consultations  </li> <li>Web Interface: Browser-based interface for human experts</li> </ul>"},{"location":"reference/api/human-interface/#humaninthelooptool","title":"HumanInTheLoopTool","text":"<p>LLM tool that enables agents to consult with human experts via XMPP messaging.</p>"},{"location":"reference/api/human-interface/#constructor","title":"Constructor","text":"<pre><code>HumanInTheLoopTool(\n    human_expert_jid: str,\n    timeout: float = 300.0,\n    name: str = \"ask_human_expert\", \n    description: Optional[str] = None\n)\n</code></pre> <p>Parameters:</p> <ul> <li><code>human_expert_jid</code> (<code>str</code>): The XMPP JID of the human expert to consult</li> <li><code>timeout</code> (<code>float</code>, optional): Maximum time to wait for response in seconds. Default: 300.0</li> <li><code>name</code> (<code>str</code>, optional): Name of the tool for LLM reference. Default: \"ask_human_expert\"</li> <li><code>description</code> (<code>str</code>, optional): Description of when to use this tool. Auto-generated if not provided.</li> </ul> <p>Example: <pre><code>from spade_llm.tools import HumanInTheLoopTool\n\ntool = HumanInTheLoopTool(\n    human_expert_jid=\"expert@company.com\",\n    timeout=180.0,  # 3 minutes\n    name=\"ask_domain_expert\",\n    description=\"Consult domain expert for specialized knowledge\"\n)\n</code></pre></p>"},{"location":"reference/api/human-interface/#methods","title":"Methods","text":""},{"location":"reference/api/human-interface/#set_agentagent-agent","title":"<code>set_agent(agent: Agent)</code>","text":"<p>Binds the tool to an agent instance. Called automatically when tool is added to an agent.</p> <p>Parameters: - <code>agent</code> (<code>Agent</code>): The SPADE agent that will use this tool</p> <p>Note: This method is called internally by <code>LLMAgent._register_tool()</code> and should not be called manually.</p>"},{"location":"reference/api/human-interface/#_ask_humanquestion-str-context-optionalstr-none-str","title":"<code>_ask_human(question: str, context: Optional[str] = None) -&gt; str</code>","text":"<p>Internal method that executes the human consultation. Called by the LLM system.</p> <p>Parameters: - <code>question</code> (<code>str</code>): The question to ask the human expert - <code>context</code> (<code>str</code>, optional): Additional context to help the human understand</p> <p>Returns: - <code>str</code>: The human expert's response or an error message</p> <p>Raises: - <code>TimeoutError</code>: If human doesn't respond within the timeout period - <code>Exception</code>: For XMPP connection or other system errors</p>"},{"location":"reference/api/human-interface/#humaninteractionbehaviour","title":"HumanInteractionBehaviour","text":"<p>SPADE OneShotBehaviour that handles individual human consultations via XMPP.</p>"},{"location":"reference/api/human-interface/#constructor_1","title":"Constructor","text":"<pre><code>HumanInteractionBehaviour(\n    human_jid: str,\n    question: str,\n    context: Optional[str] = None,\n    timeout: float = 300.0\n)\n</code></pre> <p>Parameters:</p> <ul> <li><code>human_jid</code> (<code>str</code>): The XMPP JID of the human expert</li> <li><code>question</code> (<code>str</code>): The question to ask the human</li> <li><code>context</code> (<code>str</code>, optional): Additional context for the question</li> <li><code>timeout</code> (<code>float</code>, optional): Maximum wait time for response. Default: 300.0</li> </ul> <p>Example: <pre><code>from spade_llm.behaviour.human_interaction import HumanInteractionBehaviour\n\nbehaviour = HumanInteractionBehaviour(\n    human_jid=\"expert@company.com\",\n    question=\"What's our policy on remote work?\",\n    context=\"New employee asking about work arrangement options\",\n    timeout=240.0\n)\n</code></pre></p>"},{"location":"reference/api/human-interface/#attributes","title":"Attributes","text":""},{"location":"reference/api/human-interface/#query_id-str","title":"<code>query_id: str</code>","text":"<p>Unique 8-character identifier for this consultation, used for message correlation.</p>"},{"location":"reference/api/human-interface/#response-optionalstr","title":"<code>response: Optional[str]</code>","text":"<p>The human expert's response. Set after successful completion, <code>None</code> if no response received.</p>"},{"location":"reference/api/human-interface/#methods_1","title":"Methods","text":""},{"location":"reference/api/human-interface/#async-run","title":"<code>async run()</code>","text":"<p>Executes the behaviour: sends question to human and waits for response.</p> <p>Workflow: 1. Waits for agent XMPP connection 2. Formats and sends question with unique thread ID 3. Waits for response with timeout 4. Stores response in <code>self.response</code></p> <p>Note: This method is called automatically by SPADE and should not be invoked manually.</p>"},{"location":"reference/api/human-interface/#_format_question-str","title":"<code>_format_question() -&gt; str</code>","text":"<p>Formats the question for display to the human expert.</p> <p>Returns: - <code>str</code>: Formatted question with query ID, context, and response instructions</p> <p>Example output: <pre><code>[Query a1b2c3d4] What's our policy on remote work?\n\nContext: New employee asking about work arrangement options\n\n(Please reply to this message to provide your answer)\n</code></pre></p>"},{"location":"reference/api/human-interface/#web-interface","title":"Web Interface","text":"<p>The web interface consists of static files served by a simple HTTP server.</p>"},{"location":"reference/api/human-interface/#web-server","title":"Web Server","text":""},{"location":"reference/api/human-interface/#run_serverport8080-directorynone","title":"<code>run_server(port=8080, directory=None)</code>","text":"<p>Starts the HTTP server for the human expert interface.</p> <p>Parameters: - <code>port</code> (<code>int</code>, optional): Port to run server on. Default: 8080 - <code>directory</code> (<code>str</code>, optional): Directory to serve files from. Default: <code>web_client</code> folder</p> <p>Example: <pre><code>from spade_llm.human_interface.web_server import run_server\n\n# Start on default port\nrun_server()\n\n# Start on custom port\nrun_server(port=9000)\n</code></pre></p>"},{"location":"reference/api/human-interface/#starting-via-command-line","title":"Starting via Command Line","text":"<pre><code># Default configuration (port 8080)\npython -m spade_llm.human_interface.web_server\n\n# Custom port\npython -m spade_llm.human_interface.web_server 9000\n</code></pre>"},{"location":"reference/api/human-interface/#web-interface-features","title":"Web Interface Features","text":""},{"location":"reference/api/human-interface/#connection-management","title":"Connection Management","text":"<ul> <li>WebSocket connection to XMPP server</li> <li>Automatic reconnection on connection loss</li> <li>Visual connection status indicator</li> </ul>"},{"location":"reference/api/human-interface/#query-handling","title":"Query Handling","text":"<ul> <li>Real-time notifications for new queries</li> <li>Query filtering (show/hide answered)</li> <li>Response history tracking</li> <li>Thread-based correlation for proper message routing</li> </ul>"},{"location":"reference/api/human-interface/#user-interface-elements","title":"User Interface Elements","text":"<ul> <li>Connection form: XMPP credentials input</li> <li>Query list: Active and historical queries  </li> <li>Response interface: Text area and send button</li> <li>Status indicators: Connection and query states</li> </ul>"},{"location":"reference/api/human-interface/#integration-patterns","title":"Integration Patterns","text":""},{"location":"reference/api/human-interface/#agent-registration","title":"Agent Registration","text":"<pre><code>from spade_llm.agent import LLMAgent\nfrom spade_llm.tools import HumanInTheLoopTool\n\n# Create tool\nhuman_tool = HumanInTheLoopTool(\"expert@company.com\")\n\n# Method 1: Pass in constructor\nagent = LLMAgent(\n    jid=\"agent@company.com\",\n    password=\"password\",\n    provider=provider,\n    tools=[human_tool]  # Automatic registration and binding\n)\n\n# Method 2: Add after creation\nagent = LLMAgent(jid=\"agent@company.com\", password=\"password\", provider=provider)\nagent.add_tool(human_tool)  # Manual registration\n</code></pre>"},{"location":"reference/api/human-interface/#error-handling-patterns","title":"Error Handling Patterns","text":"<pre><code># In tool usage\ntry:\n    response = await human_tool._ask_human(\"What's the weather?\")\n    if response.startswith(\"Timeout:\"):\n        # Handle timeout gracefully\n        return \"Human expert unavailable, using alternative approach\"\n    elif response.startswith(\"Error:\"):\n        # Handle system errors\n        return \"Consultation failed, proceeding with available information\"\n    else:\n        return f\"Expert says: {response}\"\nexcept Exception as e:\n    return f\"Unexpected error: {e}\"\n</code></pre>"},{"location":"reference/api/human-interface/#multiple-expert-configuration","title":"Multiple Expert Configuration","text":"<pre><code># Domain-specific experts\nsales_expert = HumanInTheLoopTool(\n    human_expert_jid=\"sales@company.com\",\n    name=\"ask_sales_expert\",\n    description=\"Consult sales team about pricing and customers\"\n)\n\ntech_expert = HumanInTheLoopTool(\n    human_expert_jid=\"tech@company.com\", \n    name=\"ask_tech_expert\",\n    description=\"Consult tech team about systems and architecture\"\n)\n\nagent = LLMAgent(\n    jid=\"agent@company.com\",\n    password=\"password\", \n    provider=provider,\n    tools=[sales_expert, tech_expert],\n    system_prompt=\"\"\"Choose the appropriate expert:\n    - Use ask_sales_expert for pricing, deals, customer questions\n    - Use ask_tech_expert for technical, system, architecture questions\"\"\"\n)\n</code></pre>"},{"location":"reference/api/human-interface/#message-protocol","title":"Message Protocol","text":""},{"location":"reference/api/human-interface/#xmpp-message-format","title":"XMPP Message Format","text":""},{"location":"reference/api/human-interface/#question-message-agent-human","title":"Question Message (Agent \u2192 Human)","text":"<pre><code>&lt;message to=\"expert@company.com\" type=\"chat\" thread=\"a1b2c3d4\"&gt;\n  &lt;body&gt;\n    [Query a1b2c3d4] What's our WiFi password?\n\n    Context: New employee needs network access\n\n    (Please reply to this message to provide your answer)\n  &lt;/body&gt;\n  &lt;metadata type=\"human_query\" query_id=\"a1b2c3d4\"/&gt;\n&lt;/message&gt;\n</code></pre>"},{"location":"reference/api/human-interface/#response-message-human-agent","title":"Response Message (Human \u2192 Agent)","text":"<pre><code>&lt;message to=\"agent@company.com\" type=\"chat\" thread=\"a1b2c3d4\"&gt;\n  &lt;body&gt;The WiFi password is \"CompanyWiFi2024\" - please don't share externally.&lt;/body&gt;\n&lt;/message&gt;\n</code></pre>"},{"location":"reference/api/human-interface/#message-correlation","title":"Message Correlation","text":"<p>Messages are correlated using XMPP thread IDs:</p> <ol> <li>Query ID generated: 8-character UUID segment</li> <li>Thread set: <code>msg.thread = query_id</code> </li> <li>Response inherits: Automatic thread inheritance in XMPP</li> <li>Behaviour filters: Only processes messages with matching thread</li> </ol>"},{"location":"reference/api/human-interface/#configuration-reference","title":"Configuration Reference","text":""},{"location":"reference/api/human-interface/#environment-variables","title":"Environment Variables","text":"<pre><code># XMPP server configuration\nXMPP_SERVER=your-server.com\nXMPP_WEBSOCKET_PORT=7070\n\n# Expert credentials\nEXPERT_JID=expert@your-server.com  \nEXPERT_PASSWORD=secure-password\n\n# Web interface\nWEB_INTERFACE_PORT=8080\n</code></pre>"},{"location":"reference/api/human-interface/#system-prompt-examples","title":"System Prompt Examples","text":""},{"location":"reference/api/human-interface/#basic-configuration","title":"Basic Configuration","text":"<pre><code>system_prompt = \"\"\"You are an AI assistant with access to human experts.\n\nUse the ask_human_expert tool when you need:\n- Current information not in your training data\n- Human judgment or opinions\n- Company-specific information  \n- Clarification on ambiguous requests\n\nAlways explain whether information comes from human experts or your training.\"\"\"\n</code></pre>"},{"location":"reference/api/human-interface/#advanced-configuration","title":"Advanced Configuration","text":"<pre><code>system_prompt = \"\"\"You are an enterprise AI assistant with specialized expert access.\n\nExpert Consultation Rules:\n1. ask_sales_expert: pricing, deals, customer relationships, market intelligence\n2. ask_tech_expert: architecture, systems, security, technical feasibility  \n3. ask_legal_expert: compliance, contracts, regulatory questions\n4. ask_hr_expert: policies, procedures, employee-related questions\n\nOnly consult experts for:\n- Information requiring current knowledge (post-training cutoff)\n- Subjective decisions requiring human judgment\n- Company-specific policies or procedures\n- Sensitive matters requiring approval\n\nFor general knowledge questions, answer directly without consultation.\"\"\"\n</code></pre>"},{"location":"reference/api/human-interface/#security-considerations","title":"Security Considerations","text":""},{"location":"reference/api/human-interface/#access-control","title":"Access Control","text":"<ul> <li>XMPP authentication: All participants must authenticate to XMPP server</li> <li>JID validation: Tools validate expert JID format  </li> <li>Message encryption: Use XMPP TLS/SSL in production</li> <li>Web interface: No authentication by default - add auth layer for production</li> </ul>"},{"location":"reference/api/human-interface/#data-privacy","title":"Data Privacy","text":"<ul> <li>Message logging: XMPP servers may log messages - configure retention policies</li> <li>Browser storage: Web interface uses sessionStorage (cleared on close)</li> <li>Network traffic: Use encrypted WebSocket connections (<code>wss://</code>) in production</li> </ul>"},{"location":"reference/api/human-interface/#deployment-security","title":"Deployment Security","text":"<pre><code># Production configuration example\nhuman_tool = HumanInTheLoopTool(\n    human_expert_jid=os.getenv(\"EXPERT_JID\"),  # From environment\n    timeout=int(os.getenv(\"EXPERT_TIMEOUT\", \"300\")),\n    name=\"ask_expert\",\n    description=\"Consult verified human expert for sensitive information\"\n)\n\n# Agent with security enabled\nagent = LLMAgent(\n    jid=os.getenv(\"AGENT_JID\"),\n    password=os.getenv(\"AGENT_PASSWORD\"),\n    provider=provider,\n    tools=[human_tool],\n    verify_security=True  # Enable certificate verification\n)\n</code></pre>"},{"location":"reference/api/human-interface/#performance-considerations","title":"Performance Considerations","text":""},{"location":"reference/api/human-interface/#timeout-strategy","title":"Timeout Strategy","text":"<pre><code># Tiered timeout approach\nurgent_tool = HumanInTheLoopTool(\"expert@company.com\", timeout=60.0)   # 1 min\nnormal_tool = HumanInTheLoopTool(\"expert@company.com\", timeout=300.0)  # 5 min  \nresearch_tool = HumanInTheLoopTool(\"expert@company.com\", timeout=1800.0) # 30 min\n</code></pre>"},{"location":"reference/api/human-interface/#connection-pooling","title":"Connection Pooling","text":"<pre><code># Reuse connections for multiple consultations\nagent = LLMAgent(...)  # Single agent instance\nawait agent.start()     # Establish XMPP connection once\n\n# Multiple consultations reuse the same agent connection\nfor question in questions:\n    response = await agent.process_message(question)\n</code></pre>"},{"location":"reference/api/human-interface/#expert-availability","title":"Expert Availability","text":"<pre><code># Fallback strategy\nsystem_prompt = \"\"\"When consulting experts:\n1. If timeout occurs, inform user and provide best-effort answer\n2. If expert unavailable, use available information sources\n3. Set user expectations about response times\n4. For urgent matters, escalate through alternative channels\"\"\"\n</code></pre>"},{"location":"reference/api/memory/","title":"Memory API","text":"<p>API reference for agent memory and learning capabilities.</p>"},{"location":"reference/api/memory/#agentinteractionmemory","title":"AgentInteractionMemory","text":"<p>Manages agent interaction memory with persistent storage.</p>"},{"location":"reference/api/memory/#constructor","title":"Constructor","text":"<pre><code>AgentInteractionMemory(\n    agent_id: str,\n    storage_dir: Optional[str] = None\n)\n</code></pre> <p>Parameters: - <code>agent_id</code> (str): Unique identifier for the agent - <code>storage_dir</code> (str, optional): Custom storage directory path</p> <p>Example:</p> <pre><code>from spade_llm.memory import AgentInteractionMemory\n\n# Default storage location\nmemory = AgentInteractionMemory(\"agent@example.com\")\n\n# Custom storage location\nmemory = AgentInteractionMemory(\n    agent_id=\"agent@example.com\",\n    storage_dir=\"/custom/memory/path\"\n)\n</code></pre>"},{"location":"reference/api/memory/#methods","title":"Methods","text":""},{"location":"reference/api/memory/#add_information","title":"add_information()","text":"<pre><code>def add_information(\n    self, \n    conversation_id: str, \n    information: str\n) -&gt; str\n</code></pre> <p>Add information to conversation memory.</p> <p>Parameters: - <code>conversation_id</code> (str): Unique conversation identifier - <code>information</code> (str): Information to store</p> <p>Returns: - <code>str</code>: Confirmation message</p> <p>Example:</p> <pre><code>memory.add_information(\n    \"user1_session\",\n    \"User prefers JSON responses over XML\"\n)\n</code></pre>"},{"location":"reference/api/memory/#get_information","title":"get_information()","text":"<pre><code>def get_information(\n    self, \n    conversation_id: str\n) -&gt; List[str]\n</code></pre> <p>Get stored information for a conversation.</p> <p>Parameters: - <code>conversation_id</code> (str): Conversation identifier</p> <p>Returns: - <code>List[str]</code>: List of stored information strings</p> <p>Example:</p> <pre><code>info_list = memory.get_information(\"user1_session\")\n# Returns: [\"User prefers JSON responses\", \"API token: abc123\"]\n</code></pre>"},{"location":"reference/api/memory/#get_context_summary","title":"get_context_summary()","text":"<pre><code>def get_context_summary(\n    self, \n    conversation_id: str\n) -&gt; Optional[str]\n</code></pre> <p>Get formatted summary of conversation memory.</p> <p>Parameters: - <code>conversation_id</code> (str): Conversation identifier</p> <p>Returns: - <code>Optional[str]</code>: Formatted summary string or None if no memory</p> <p>Example:</p> <pre><code>summary = memory.get_context_summary(\"user1_session\")\n# Returns: \"Previous interactions:\\n- User prefers JSON responses\\n- API token: abc123\"\n</code></pre>"},{"location":"reference/api/memory/#clear_conversation","title":"clear_conversation()","text":"<pre><code>def clear_conversation(\n    self, \n    conversation_id: str\n) -&gt; bool\n</code></pre> <p>Clear memory for a specific conversation.</p> <p>Parameters: - <code>conversation_id</code> (str): Conversation identifier</p> <p>Returns: - <code>bool</code>: True if successful, False otherwise</p> <p>Example:</p> <pre><code>success = memory.clear_conversation(\"user1_session\")\n</code></pre>"},{"location":"reference/api/memory/#get_all_conversations","title":"get_all_conversations()","text":"<pre><code>def get_all_conversations(self) -&gt; List[str]\n</code></pre> <p>Get list of all conversation IDs with stored memory.</p> <p>Returns: - <code>List[str]</code>: List of conversation IDs</p> <p>Example:</p> <pre><code>conversations = memory.get_all_conversations()\n# Returns: [\"user1_session\", \"user2_session\", \"api_session\"]\n</code></pre>"},{"location":"reference/api/memory/#storage-format","title":"Storage Format","text":"<p>The memory system uses JSON file storage with the following structure:</p> <pre><code>{\n    \"agent_id\": \"agent@example.com\",\n    \"interactions\": {\n        \"conversation_id\": [\n            {\n                \"content\": \"User prefers JSON responses\",\n                \"timestamp\": \"2025-01-09T10:30:00.000Z\"\n            },\n            {\n                \"content\": \"API token: db_token_123\",\n                \"timestamp\": \"2025-01-09T10:35:00.000Z\"\n            }\n        ]\n    }\n}\n</code></pre>"},{"location":"reference/api/memory/#example-usage","title":"Example Usage","text":"<pre><code>from spade_llm.memory import AgentInteractionMemory\n\n# Create memory instance\nmemory = AgentInteractionMemory(\"support_agent@example.com\")\n\n# Store information\nmemory.add_information(\"customer_123\", \"Customer prefers email notifications\")\nmemory.add_information(\"customer_123\", \"Account type: premium\")\n\n# Retrieve information\ninfo_list = memory.get_information(\"customer_123\")\nprint(info_list)  # [\"Customer prefers email notifications\", \"Account type: premium\"]\n\n# Get formatted summary\nsummary = memory.get_context_summary(\"customer_123\")\nprint(summary)\n# Output: \"Previous interactions:\\n- Customer prefers email notifications\\n- Account type: premium\"\n\n# Clear conversation memory\nmemory.clear_conversation(\"customer_123\")\n</code></pre>"},{"location":"reference/api/memory/#memory-tools","title":"Memory Tools","text":""},{"location":"reference/api/memory/#remember_interaction_info","title":"remember_interaction_info","text":"<p>Tool function available to LLMs for storing information.</p> <pre><code>def remember_interaction_info(information: str) -&gt; str\n</code></pre> <p>Parameters: - <code>information</code> (str): Information to store in memory</p> <p>Returns: - <code>str</code>: Confirmation message</p> <p>LLM Usage:</p> <pre><code>{\n    \"name\": \"remember_interaction_info\",\n    \"description\": \"Store important information for future reference\",\n    \"parameters\": {\n        \"type\": \"object\",\n        \"properties\": {\n            \"information\": {\n                \"type\": \"string\",\n                \"description\": \"Important information to remember\"\n            }\n        },\n        \"required\": [\"information\"]\n    }\n}\n</code></pre>"},{"location":"reference/api/memory/#get_interaction_history","title":"get_interaction_history","text":"<p>Tool function for retrieving stored memory.</p> <pre><code>def get_interaction_history() -&gt; str\n</code></pre> <p>Returns: - <code>str</code>: Formatted history of stored interactions</p> <p>LLM Usage:</p> <pre><code>{\n    \"name\": \"get_interaction_history\",\n    \"description\": \"Get previous interaction information\",\n    \"parameters\": {\n        \"type\": \"object\",\n        \"properties\": {},\n        \"required\": []\n    }\n}\n</code></pre>"},{"location":"reference/api/memory/#memory-integration","title":"Memory Integration","text":""},{"location":"reference/api/memory/#llmagent-integration","title":"LLMAgent Integration","text":"<p>Memory is integrated into <code>LLMAgent</code> via the <code>interaction_memory</code> parameter:</p> <pre><code>from spade_llm.agent import LLMAgent\n\nagent = LLMAgent(\n    jid=\"memory_agent@example.com\",\n    password=\"password\",\n    provider=provider,\n    interaction_memory=True,  # Enable memory\n    system_prompt=\"You have memory capabilities.\"\n)\n</code></pre>"},{"location":"reference/api/memory/#conversation-threading","title":"Conversation Threading","text":"<p>Memory integrates with conversation threading:</p> <pre><code># Conversation ID generation\nconversation_id = msg.thread or f\"{msg.sender}_{msg.to}\"\n\n# Memory is isolated per conversation\nmemory.add_information(conversation_id, \"User session data\")\n</code></pre>"},{"location":"reference/api/memory/#context-injection","title":"Context Injection","text":"<p>Memory is automatically injected into conversations:</p> <pre><code># System message automatically added\n{\n    \"role\": \"system\",\n    \"content\": \"Previous interactions:\\n- User prefers JSON\\n- API token: abc123\"\n}\n</code></pre>"},{"location":"reference/api/memory/#memory-lifecycle","title":"Memory Lifecycle","text":""},{"location":"reference/api/memory/#conversation-states","title":"Conversation States","text":"<p>Memory integrates with conversation state management:</p> <pre><code># Conversation states\nclass ConversationState:\n    ACTIVE = \"active\"\n    COMPLETED = \"completed\"\n    ERROR = \"error\"\n    TIMEOUT = \"timeout\"\n    MAX_INTERACTIONS_REACHED = \"max_interactions_reached\"\n</code></pre>"},{"location":"reference/api/memory/#state-tracking","title":"State Tracking","text":"<pre><code># Conversation state structure\n{\n    \"conversation_id\": \"user1_session\",\n    \"state\": \"active\",\n    \"interaction_count\": 5,\n    \"max_interactions\": 10,\n    \"created_at\": \"2025-01-09T10:00:00Z\",\n    \"last_interaction\": \"2025-01-09T10:30:00Z\"\n}\n</code></pre>"},{"location":"reference/api/memory/#performance-considerations","title":"Performance Considerations","text":""},{"location":"reference/api/memory/#file-system-limits","title":"File System Limits","text":"<ul> <li>File Size: JSON files grow with conversation length</li> <li>I/O Operations: Each operation requires file read/write</li> <li>Concurrency: Multiple agents may cause file contention</li> </ul>"},{"location":"reference/api/providers/","title":"Providers API","text":"<p>API reference for LLM provider classes.</p>"},{"location":"reference/api/providers/#llmprovider","title":"LLMProvider","text":"<p>Unified interface for different LLM services.</p>"},{"location":"reference/api/providers/#class-methods","title":"Class Methods","text":""},{"location":"reference/api/providers/#create_openai","title":"create_openai()","text":"<pre><code>LLMProvider.create_openai(\n    api_key: str,\n    model: str = \"gpt-4o-mini\",\n    temperature: float = 0.7,\n    timeout: Optional[float] = None,\n    max_tokens: Optional[int] = None\n) -&gt; LLMProvider\n</code></pre> <p>Create OpenAI provider.</p> <p>Parameters:</p> <ul> <li><code>api_key</code> - OpenAI API key</li> <li><code>model</code> - Model name (e.g., \"gpt-4o\", \"gpt-4o-mini\")</li> <li><code>temperature</code> - Sampling temperature (0.0-1.0)</li> <li><code>timeout</code> - Request timeout in seconds</li> <li><code>max_tokens</code> - Maximum tokens to generate</li> </ul> <p>Example:</p> <pre><code>provider = LLMProvider.create_openai(\n    api_key=\"sk-...\",\n    model=\"gpt-4o-mini\",\n    temperature=0.7\n)\n</code></pre>"},{"location":"reference/api/providers/#create_ollama","title":"create_ollama()","text":"<pre><code>LLMProvider.create_ollama(\n    model: str = \"llama3.1:8b\",\n    base_url: str = \"http://localhost:11434/v1\",\n    temperature: float = 0.7,\n    timeout: float = 120.0\n) -&gt; LLMProvider\n</code></pre> <p>Create Ollama provider.</p> <p>Parameters:</p> <ul> <li><code>model</code> - Model name (e.g., \"llama3.1:8b\", \"mistral:7b\")</li> <li><code>base_url</code> - Ollama server URL</li> <li><code>temperature</code> - Sampling temperature</li> <li><code>timeout</code> - Request timeout (longer for local models)</li> </ul> <p>Example:</p> <pre><code>provider = LLMProvider.create_ollama(\n    model=\"llama3.1:8b\",\n    temperature=0.8,\n    timeout=180.0\n)\n</code></pre>"},{"location":"reference/api/providers/#create_lm_studio","title":"create_lm_studio()","text":"<pre><code>LLMProvider.create_lm_studio(\n    model: str = \"local-model\",\n    base_url: str = \"http://localhost:1234/v1\",\n    temperature: float = 0.7\n) -&gt; LLMProvider\n</code></pre> <p>Create LM Studio provider.</p> <p>Example:</p> <pre><code>provider = LLMProvider.create_lm_studio(\n    model=\"Meta-Llama-3.1-8B-Instruct\",\n    base_url=\"http://localhost:1234/v1\"\n)\n</code></pre>"},{"location":"reference/api/providers/#create_vllm","title":"create_vllm()","text":"<pre><code>LLMProvider.create_vllm(\n    model: str,\n    base_url: str = \"http://localhost:8000/v1\"\n) -&gt; LLMProvider\n</code></pre> <p>Create vLLM provider.</p> <p>Example:</p> <pre><code>provider = LLMProvider.create_vllm(\n    model=\"meta-llama/Llama-2-7b-chat-hf\",\n    base_url=\"http://localhost:8000/v1\"\n)\n</code></pre>"},{"location":"reference/api/providers/#instance-methods","title":"Instance Methods","text":""},{"location":"reference/api/providers/#get_llm_response","title":"get_llm_response()","text":"<pre><code>async def get_llm_response(\n    self, \n    context: ContextManager, \n    tools: Optional[List[LLMTool]] = None\n) -&gt; Dict[str, Any]\n</code></pre> <p>Get complete response from LLM.</p> <p>Returns:</p> <pre><code>{\n    'text': Optional[str],      # Text response\n    'tool_calls': List[Dict]    # Tool calls requested\n}\n</code></pre> <p>Example:</p> <pre><code>response = await provider.get_llm_response(context, tools)\n\nif response['tool_calls']:\n    # Handle tool calls\n    for call in response['tool_calls']:\n        print(f\"Tool: {call['name']}, Args: {call['arguments']}\")\nelse:\n    # Handle text response\n    print(f\"Response: {response['text']}\")\n</code></pre>"},{"location":"reference/api/providers/#get_response-legacy","title":"get_response() (Legacy)","text":"<pre><code>async def get_response(\n    self, \n    context: ContextManager, \n    tools: Optional[List[LLMTool]] = None\n) -&gt; Optional[str]\n</code></pre> <p>Get text response only.</p> <p>Example:</p> <pre><code>text_response = await provider.get_response(context)\n</code></pre>"},{"location":"reference/api/providers/#get_tool_calls-legacy","title":"get_tool_calls() (Legacy)","text":"<pre><code>async def get_tool_calls(\n    self, \n    context: ContextManager, \n    tools: Optional[List[LLMTool]] = None\n) -&gt; List[Dict[str, Any]]\n</code></pre> <p>Get tool calls only.</p>"},{"location":"reference/api/providers/#baseprovider","title":"BaseProvider","text":"<p>Abstract base class for custom providers.</p> <pre><code>from spade_llm.providers.base_provider import LLMProvider as BaseProvider\n\nclass CustomProvider(BaseProvider):\n    async def get_llm_response(self, context, tools=None):\n        \"\"\"Implement custom LLM integration.\"\"\"\n        # Your implementation\n        return {\n            'text': \"Response from custom provider\",\n            'tool_calls': []\n        }\n</code></pre>"},{"location":"reference/api/providers/#provider-configuration","title":"Provider Configuration","text":""},{"location":"reference/api/providers/#model-formats","title":"Model Formats","text":"<pre><code>class ModelFormat(Enum):\n    OPENAI = \"openai\"    # gpt-4, gpt-3.5-turbo\n    OLLAMA = \"ollama\"    # llama3.1:8b, mistral:7b  \n    CUSTOM = \"custom\"    # custom/model-name\n</code></pre>"},{"location":"reference/api/providers/#environment-variables","title":"Environment Variables","text":"<pre><code># OpenAI\nOPENAI_API_KEY=sk-...\nOPENAI_MODEL=gpt-4o-mini\n\n# Ollama  \nOLLAMA_BASE_URL=http://localhost:11434/v1\nOLLAMA_MODEL=llama3.1:8b\n\n# LM Studio\nLM_STUDIO_BASE_URL=http://localhost:1234/v1\nLM_STUDIO_MODEL=local-model\n</code></pre>"},{"location":"reference/api/providers/#dynamic-configuration","title":"Dynamic Configuration","text":"<pre><code>import os\n\ndef create_provider_from_env():\n    provider_type = os.getenv('LLM_PROVIDER', 'openai')\n\n    if provider_type == 'openai':\n        return LLMProvider.create_openai(\n            api_key=os.getenv('OPENAI_API_KEY'),\n            model=os.getenv('OPENAI_MODEL', 'gpt-4o-mini')\n        )\n    elif provider_type == 'ollama':\n        return LLMProvider.create_ollama(\n            model=os.getenv('OLLAMA_MODEL', 'llama3.1:8b')\n        )\n\nprovider = create_provider_from_env()\n</code></pre>"},{"location":"reference/api/providers/#tool-support","title":"Tool Support","text":""},{"location":"reference/api/providers/#openai-tools","title":"OpenAI Tools","text":"<p>Native tool calling support:</p> <pre><code># Tools automatically formatted for OpenAI\nresponse = await provider.get_llm_response(context, tools)\n</code></pre>"},{"location":"reference/api/providers/#ollama-tools","title":"Ollama Tools","text":"<p>Limited to compatible models:</p> <pre><code># Check model compatibility\ntool_compatible_models = [\n    \"llama3.1:8b\", \"llama3.1:70b\", \"mistral:7b\"\n]\n\nif model in tool_compatible_models:\n    # Use tools\n    response = await provider.get_llm_response(context, tools)\n</code></pre>"},{"location":"reference/api/providers/#error-handling","title":"Error Handling","text":"<pre><code>from openai import OpenAIError\n\ntry:\n    response = await provider.get_llm_response(context)\nexcept OpenAIError as e:\n    print(f\"OpenAI API error: {e}\")\nexcept ConnectionError as e:\n    print(f\"Connection error: {e}\")\nexcept TimeoutError as e:\n    print(f\"Request timeout: {e}\")\n</code></pre>"},{"location":"reference/api/providers/#provider-comparison","title":"Provider Comparison","text":"Feature OpenAI Ollama LM Studio vLLM Setup Easy Medium Easy Hard Quality Excellent Good Good Good Speed Fast Slow Slow Fast Cost Paid Free Free Free Privacy Low High High High Tools Full Limited Limited Limited"},{"location":"reference/api/providers/#best-practices","title":"Best Practices","text":""},{"location":"reference/api/providers/#provider-selection","title":"Provider Selection","text":"<pre><code>def choose_provider(use_case: str):\n    \"\"\"Choose provider based on use case.\"\"\"\n    if use_case == \"development\":\n        return LLMProvider.create_ollama(model=\"llama3.1:1b\")  # Fast\n    elif use_case == \"production\":\n        return LLMProvider.create_openai(model=\"gpt-4o-mini\")   # Reliable\n    elif use_case == \"privacy\":\n        return LLMProvider.create_ollama(model=\"llama3.1:8b\")  # Local\n</code></pre>"},{"location":"reference/api/providers/#error-recovery","title":"Error Recovery","text":"<pre><code>async def robust_llm_call(providers: List[LLMProvider], context):\n    \"\"\"Try multiple providers with fallback.\"\"\"\n    for provider in providers:\n        try:\n            return await provider.get_llm_response(context)\n        except Exception as e:\n            print(f\"Provider failed: {e}\")\n            continue\n\n    raise Exception(\"All providers failed\")\n</code></pre>"},{"location":"reference/api/providers/#performance-monitoring","title":"Performance Monitoring","text":"<pre><code>import time\n\nasync def timed_call(provider, context):\n    \"\"\"Monitor provider performance.\"\"\"\n    start = time.time()\n    try:\n        response = await provider.get_llm_response(context)\n        duration = time.time() - start\n        print(f\"Provider response time: {duration:.2f}s\")\n        return response\n    except Exception as e:\n        duration = time.time() - start\n        print(f\"Provider failed after {duration:.2f}s: {e}\")\n        raise\n</code></pre>"},{"location":"reference/api/routing/","title":"Routing API","text":"<p>API reference for message routing system.</p>"},{"location":"reference/api/routing/#routingfunction","title":"RoutingFunction","text":"<p>Type definition for routing functions.</p> <pre><code>RoutingFunction = Callable[[Message, str, Dict[str, Any]], Union[str, RoutingResponse]]\n</code></pre> <p>Parameters:</p> <ul> <li><code>msg</code> - Original SPADE message</li> <li><code>response</code> - LLM response text</li> <li><code>context</code> - Conversation context and metadata</li> </ul> <p>Returns:</p> <ul> <li><code>str</code> - Single recipient JID</li> <li><code>List[str]</code> - Multiple recipient JIDs  </li> <li><code>RoutingResponse</code> - Advanced routing</li> <li><code>None</code> - Send to original sender</li> </ul>"},{"location":"reference/api/routing/#basic-routing-function","title":"Basic Routing Function","text":"<pre><code>def simple_router(msg, response, context):\n    \"\"\"Route based on response content.\"\"\"\n    if \"technical\" in response.lower():\n        return \"tech-support@example.com\"\n    elif \"billing\" in response.lower():\n        return \"billing@example.com\"\n    else:\n        return str(msg.sender)  # Reply to sender\n</code></pre>"},{"location":"reference/api/routing/#context-information","title":"Context Information","text":"<pre><code>context = {\n    \"conversation_id\": \"user1_agent1\",\n    \"state\": {\n        \"state\": \"active\",\n        \"interaction_count\": 5,\n        \"start_time\": 1642678800.0,\n        \"last_activity\": 1642678900.0\n    }\n}\n</code></pre>"},{"location":"reference/api/routing/#routingresponse","title":"RoutingResponse","text":"<p>Advanced routing with transformations and metadata.</p>"},{"location":"reference/api/routing/#constructor","title":"Constructor","text":"<pre><code>@dataclass\nclass RoutingResponse:\n    recipients: Union[str, List[str]]\n    transform: Optional[Callable[[str], str]] = None\n    metadata: Optional[Dict[str, Any]] = None\n</code></pre> <p>Parameters:</p> <ul> <li><code>recipients</code> - Destination JID(s)</li> <li><code>transform</code> - Function to modify response before sending</li> <li><code>metadata</code> - Additional message metadata</li> </ul>"},{"location":"reference/api/routing/#example","title":"Example","text":"<pre><code>from spade_llm.routing import RoutingResponse\n\ndef advanced_router(msg, response, context):\n    \"\"\"Advanced routing with transformation.\"\"\"\n\n    def add_signature(text):\n        return f\"{text}\\n\\n--\\nProcessed by AI Assistant\"\n\n    if \"urgent\" in response.lower():\n        return RoutingResponse(\n            recipients=\"emergency@example.com\",\n            transform=add_signature,\n            metadata={\n                \"priority\": \"high\",\n                \"category\": \"urgent\",\n                \"original_sender\": str(msg.sender)\n            }\n        )\n\n    return str(msg.sender)\n</code></pre>"},{"location":"reference/api/routing/#routing-patterns","title":"Routing Patterns","text":""},{"location":"reference/api/routing/#content-based-routing","title":"Content-Based Routing","text":"<pre><code>def content_router(msg, response, context):\n    \"\"\"Route based on response keywords.\"\"\"\n    response_lower = response.lower()\n\n    routing_map = {\n        \"tech-support@example.com\": [\"error\", \"bug\", \"technical\", \"debug\"],\n        \"sales@example.com\": [\"price\", \"cost\", \"purchase\", \"buy\"],\n        \"billing@example.com\": [\"payment\", \"invoice\", \"billing\"],\n        \"urgent@example.com\": [\"urgent\", \"emergency\", \"critical\"]\n    }\n\n    for recipient, keywords in routing_map.items():\n        if any(keyword in response_lower for keyword in keywords):\n            return recipient\n\n    return \"general@example.com\"\n</code></pre>"},{"location":"reference/api/routing/#sender-based-routing","title":"Sender-Based Routing","text":"<pre><code>def sender_router(msg, response, context):\n    \"\"\"Route based on message sender.\"\"\"\n    sender = str(msg.sender)\n    sender_domain = sender.split('@')[1]\n\n    # Internal vs external routing\n    if sender_domain == \"company.com\":\n        return \"internal-support@example.com\"\n    else:\n        return \"external-support@example.com\"\n</code></pre>"},{"location":"reference/api/routing/#context-aware-routing","title":"Context-Aware Routing","text":"<pre><code>def context_router(msg, response, context):\n    \"\"\"Route based on conversation context.\"\"\"\n    state = context.get(\"state\", {})\n    interaction_count = state.get(\"interaction_count\", 0)\n\n    # Long conversations need escalation\n    if interaction_count &gt; 10:\n        return RoutingResponse(\n            recipients=\"escalation@example.com\",\n            metadata={\n                \"reason\": \"long_conversation\",\n                \"interaction_count\": interaction_count\n            }\n        )\n\n    return \"standard@example.com\"\n</code></pre>"},{"location":"reference/api/routing/#multi-recipient-routing","title":"Multi-Recipient Routing","text":"<pre><code>def broadcast_router(msg, response, context):\n    \"\"\"Route to multiple recipients.\"\"\"\n    recipients = [\"primary@example.com\"]\n\n    # Add recipients based on content\n    if \"error\" in response.lower():\n        recipients.append(\"monitoring@example.com\")\n\n    if \"sales\" in response.lower():\n        recipients.append(\"sales-team@example.com\")\n\n    return RoutingResponse(\n        recipients=recipients,\n        metadata={\n            \"broadcast\": True,\n            \"primary\": \"primary@example.com\"\n        }\n    )\n</code></pre>"},{"location":"reference/api/routing/#best-practices","title":"Best Practices","text":""},{"location":"reference/api/routing/#routing-design","title":"Routing Design","text":"<ul> <li>Keep logic simple - Complex routing is hard to debug</li> <li>Use meaningful destinations - Clear JID naming</li> <li>Handle edge cases - Provide fallback routing</li> <li>Document routing rules - Clear rule descriptions</li> <li>Test thoroughly - Test all routing paths</li> </ul>"},{"location":"reference/api/tools/","title":"Tools API","text":"<p>API reference for the SPADE_LLM tools system.</p>"},{"location":"reference/api/tools/#llmtool","title":"LLMTool","text":"<p>Core tool class for defining executable functions.</p>"},{"location":"reference/api/tools/#constructor","title":"Constructor","text":"<pre><code>LLMTool(\n    name: str,\n    description: str,\n    parameters: Dict[str, Any],\n    func: Callable[..., Any]\n)\n</code></pre> <p>Parameters:</p> <ul> <li><code>name</code> - Unique tool identifier</li> <li><code>description</code> - Tool description for LLM understanding</li> <li><code>parameters</code> - JSON Schema parameter definition</li> <li><code>func</code> - Python function to execute</li> </ul>"},{"location":"reference/api/tools/#methods","title":"Methods","text":""},{"location":"reference/api/tools/#execute","title":"execute()","text":"<pre><code>async def execute(self, **kwargs) -&gt; Any\n</code></pre> <p>Execute tool with provided arguments.</p> <p>Example:</p> <pre><code>result = await tool.execute(city=\"Madrid\", units=\"celsius\")\n</code></pre>"},{"location":"reference/api/tools/#to_dict","title":"to_dict()","text":"<pre><code>def to_dict(self) -&gt; Dict[str, Any]\n</code></pre> <p>Convert tool to dictionary representation.</p>"},{"location":"reference/api/tools/#to_openai_tool","title":"to_openai_tool()","text":"<pre><code>def to_openai_tool(self) -&gt; Dict[str, Any]\n</code></pre> <p>Convert to OpenAI tool format.</p>"},{"location":"reference/api/tools/#example","title":"Example","text":"<pre><code>from spade_llm import LLMTool\n\nasync def get_weather(city: str, units: str = \"celsius\") -&gt; str:\n    \"\"\"Get weather for a city.\"\"\"\n    return f\"Weather in {city}: 22\u00b0C, sunny\"\n\nweather_tool = LLMTool(\n    name=\"get_weather\",\n    description=\"Get current weather information for a city\",\n    parameters={\n        \"type\": \"object\",\n        \"properties\": {\n            \"city\": {\n                \"type\": \"string\",\n                \"description\": \"Name of the city\"\n            },\n            \"units\": {\n                \"type\": \"string\",\n                \"enum\": [\"celsius\", \"fahrenheit\"],\n                \"default\": \"celsius\"\n            }\n        },\n        \"required\": [\"city\"]\n    },\n    func=get_weather\n)\n</code></pre>"},{"location":"reference/api/tools/#parameter-schema","title":"Parameter Schema","text":"<p>Tools use JSON Schema for parameter validation:</p>"},{"location":"reference/api/tools/#basic-types","title":"Basic Types","text":"<pre><code># String parameter\n\"city\": {\n    \"type\": \"string\",\n    \"description\": \"City name\"\n}\n\n# Number parameter  \n\"temperature\": {\n    \"type\": \"number\",\n    \"minimum\": -100,\n    \"maximum\": 100\n}\n\n# Boolean parameter\n\"include_forecast\": {\n    \"type\": \"boolean\",\n    \"default\": False\n}\n\n# Array parameter\n\"cities\": {\n    \"type\": \"array\",\n    \"items\": {\"type\": \"string\"},\n    \"maxItems\": 10\n}\n</code></pre>"},{"location":"reference/api/tools/#complex-schema","title":"Complex Schema","text":"<pre><code>parameters = {\n    \"type\": \"object\",\n    \"properties\": {\n        \"query\": {\n            \"type\": \"string\",\n            \"description\": \"Search query\"\n        },\n        \"filters\": {\n            \"type\": \"object\",\n            \"properties\": {\n                \"date_range\": {\n                    \"type\": \"object\",\n                    \"properties\": {\n                        \"start\": {\"type\": \"string\", \"format\": \"date\"},\n                        \"end\": {\"type\": \"string\", \"format\": \"date\"}\n                    }\n                },\n                \"category\": {\n                    \"type\": \"string\",\n                    \"enum\": [\"news\", \"blogs\", \"academic\"]\n                }\n            }\n        },\n        \"max_results\": {\n            \"type\": \"integer\",\n            \"minimum\": 1,\n            \"maximum\": 100,\n            \"default\": 10\n        }\n    },\n    \"required\": [\"query\"]\n}\n</code></pre>"},{"location":"reference/api/tools/#langchain-integration","title":"LangChain Integration","text":""},{"location":"reference/api/tools/#langchaintooladapter","title":"LangChainToolAdapter","text":"<pre><code>from spade_llm.tools import LangChainToolAdapter\nfrom langchain_community.tools import DuckDuckGoSearchRun\n\n# Create LangChain tool\nsearch_lc = DuckDuckGoSearchRun()\n\n# Adapt for SPADE_LLM\nsearch_tool = LangChainToolAdapter(search_lc)\n\n# Use with agent\nagent = LLMAgent(\n    jid=\"agent@example.com\",\n    password=\"password\",\n    provider=provider,\n    tools=[search_tool]\n)\n</code></pre>"},{"location":"reference/api/tools/#best-practices","title":"Best Practices","text":""},{"location":"reference/api/tools/#tool-design","title":"Tool Design","text":"<ul> <li>Single Purpose: Each tool should do one thing well</li> <li>Clear Names: Use descriptive tool names</li> <li>Good Descriptions: Help LLM understand when to use tools</li> <li>Validate Inputs: Always validate and sanitize parameters</li> <li>Handle Errors: Return meaningful error messages</li> <li>Use Async: Enable concurrent execution</li> </ul>"}]}