{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"SPADE_LLM : SPADE with Large Language Models","text":"<p>Extension for SPADE that integrates Large Language Models into multi-agent systems.</p>"},{"location":"#features","title":"Features","text":"<ul> <li>Multi-Provider Support: OpenAI, Ollama, LM Studio, vLLM</li> <li>Tool System: Function calling with async execution</li> <li>Context Management: Multi-conversation support with automatic cleanup</li> <li>Message Routing: Conditional routing based on LLM responses</li> <li>MCP Integration: Model Context Protocol server support</li> <li>Production Ready: Comprehensive error handling and logging</li> </ul>"},{"location":"#quick-start","title":"Quick Start","text":"<pre><code>import spade\nfrom spade_llm import LLMAgent, LLMProvider\n\nasync def main():\n    provider = LLMProvider.create_openai(\n        api_key=\"your-api-key\",\n        model=\"gpt-4o-mini\"\n    )\n\n    agent = LLMAgent(\n        jid=\"assistant@example.com\",\n        password=\"password\",\n        provider=provider,\n        system_prompt=\"You are a helpful assistant\"\n    )\n\n    await agent.start()\n\nif __name__ == \"__main__\":\n    spade.run(main())\n</code></pre>"},{"location":"#architecture","title":"Architecture","text":"<pre><code>graph LR\n    A[LLMAgent] --&gt; B[LLMBehaviour]\n    B --&gt; C[ContextManager]\n    B --&gt; D[LLMProvider]\n    B --&gt; E[LLMTool]\n    D --&gt; F[OpenAI/Ollama/etc]</code></pre>"},{"location":"#documentation","title":"Documentation","text":"<ul> <li>Installation - Setup and requirements</li> <li>Quick Start - Basic usage examples</li> <li>Providers - LLM provider configuration</li> <li>Tools - Tool system usage</li> <li>API Reference - Complete API documentation</li> </ul>"},{"location":"#examples","title":"Examples","text":"<p>See the examples directory for complete working examples including chat agents, tool usage, and multi-agent systems.</p>"},{"location":"contributing/","title":"Contributing","text":"<p>We welcome contributions to SPADE_LLM! This guide will help you get started.</p>"},{"location":"contributing/#quick-start","title":"Quick Start","text":"<ol> <li>Fork the repository on GitHub</li> <li>Clone your fork locally</li> <li>Create a branch for your changes</li> <li>Make your changes and test them</li> <li>Submit a pull request</li> </ol>"},{"location":"contributing/#development-setup","title":"Development Setup","text":""},{"location":"contributing/#prerequisites","title":"Prerequisites","text":"<ul> <li>Python 3.10+</li> <li>Git</li> <li>Virtual environment tool (venv, conda, etc.)</li> <li>A XMPP server connection </li> </ul>"},{"location":"contributing/#installation","title":"Installation","text":"<pre><code># Clone your fork\ngit clone https://github.com/your-username/spade_llm.git\ncd spade_llm\n\n# Create virtual environment\npython -m venv .venv\nsource .venv/bin/activate  # On Windows: .venv\\Scripts\\activate\n\n# Install in development mode\npip install -e \".[dev]\"\n\n# Install pre-commit hooks\npre-commit install\n</code></pre>"},{"location":"contributing/#running-tests","title":"Running Tests","text":"<pre><code># Run all tests\npytest\n\n# Run with coverage\npytest --cov=spade_llm\n\n# Run specific test file\npytest tests/test_agent/test_llm_agent.py\n\n# Run with verbose output\npytest -v\n</code></pre>"},{"location":"contributing/#types-of-contributions","title":"Types of Contributions","text":""},{"location":"contributing/#bug-reports","title":"Bug Reports","text":"<p>When reporting bugs, please include:</p> <ul> <li>Clear description of the problem</li> <li>Steps to reproduce the issue</li> <li>Expected vs actual behavior</li> <li>Environment details (Python version, OS, etc.)</li> <li>Minimal code example that reproduces the bug</li> </ul>"},{"location":"contributing/#feature-requests","title":"Feature Requests","text":"<p>For new features, please:</p> <ul> <li>Check existing issues to avoid duplicates</li> <li>Describe the use case and motivation</li> <li>Propose implementation approach if possible</li> <li>Consider backward compatibility</li> </ul>"},{"location":"contributing/#submitting-changes","title":"Submitting Changes","text":""},{"location":"contributing/#pull-request-process","title":"Pull Request Process","text":"<ol> <li> <p>Create a feature branch <pre><code>git checkout -b feature/your-feature-name\n</code></pre></p> </li> <li> <p>Make your changes</p> </li> <li>Write code</li> <li>Add tests</li> <li> <p>Update documentation</p> </li> <li> <p>Test your changes <pre><code>pytest\nflake8 spade_llm tests\nmypy spade_llm\n</code></pre></p> </li> <li> <p>Commit with clear messages <pre><code>git add .\ngit commit -m \"Add feature: clear description of changes\"\n</code></pre></p> </li> <li> <p>Push to your fork <pre><code>git push origin feature/your-feature-name\n</code></pre></p> </li> <li> <p>Create pull request on GitHub</p> </li> </ol>"},{"location":"contributing/#pull-request-guidelines","title":"Pull Request Guidelines","text":"<ul> <li>Clear title describing the change</li> <li>Detailed description of what and why</li> <li>Link related issues using keywords (fixes #123)</li> <li>Include screenshots for UI changes</li> <li>Check that CI passes</li> </ul>"},{"location":"contributing/#development-guidelines","title":"Development Guidelines","text":""},{"location":"contributing/#architecture-principles","title":"Architecture Principles","text":"<ul> <li>Extend, don't replace SPADE functionality</li> <li>Maintain compatibility with existing SPADE agents</li> <li>Use async/await throughout</li> <li>Keep interfaces simple and consistent</li> <li>Favor composition over inheritance</li> </ul>"},{"location":"contributing/#code-organization","title":"Code Organization","text":"<pre><code>spade_llm/\n\u251c\u2500\u2500 agent/          # Agent classes\n\u251c\u2500\u2500 behaviour/      # Behaviour implementations\n\u251c\u2500\u2500 context/        # Context management\n\u251c\u2500\u2500 providers/      # LLM provider interfaces\n\u251c\u2500\u2500 tools/          # Tool system\n\u251c\u2500\u2500 routing/        # Message routing\n\u251c\u2500\u2500 mcp/           # MCP integration\n\u2514\u2500\u2500 utils/         # Utility functions\n</code></pre>"},{"location":"contributing/#release-process","title":"Release Process","text":""},{"location":"contributing/#version-numbering","title":"Version Numbering","text":"<p>We use semantic versioning (semver):</p> <ul> <li>Major (X.0.0) - Breaking changes</li> <li>Minor (0.X.0) - New features, backward compatible</li> <li>Patch (0.0.X) - Bug fixes, backward compatible</li> </ul>"},{"location":"contributing/#release-checklist","title":"Release Checklist","text":"<ul> <li>[ ] All tests pass</li> <li>[ ] Documentation updated</li> <li>[ ] Changelog updated</li> <li>[ ] Version bumped</li> <li>[ ] Release notes prepared</li> <li>[ ] Tagged release created</li> </ul>"},{"location":"contributing/#community","title":"Community","text":""},{"location":"contributing/#getting-help","title":"Getting Help","text":"<ul> <li>GitHub Issues - Bug reports and feature requests</li> <li>Discussions - General questions and ideas</li> <li>Documentation - Comprehensive guides and API reference</li> </ul>"},{"location":"contributing/#code-of-conduct","title":"Code of Conduct","text":"<ul> <li>Be respectful and inclusive</li> <li>Help others learn and contribute</li> <li>Focus on constructive feedback</li> <li>Follow project guidelines and standards</li> </ul>"},{"location":"contributing/#recognition","title":"Recognition","text":"<p>Contributors are recognized in:</p> <ul> <li>CONTRIBUTORS.md file</li> <li>Release notes for significant contributions</li> <li>Documentation acknowledgments</li> </ul> <p>Thank you for contributing to SPADE_LLM! \ud83d\ude80</p>"},{"location":"contributing/development/","title":"Development Guide","text":"<p>Detailed guide for SPADE_LLM development and testing.</p>"},{"location":"contributing/development/#development-environment","title":"Development Environment","text":""},{"location":"contributing/development/#setup","title":"Setup","text":"<pre><code># Clone repository\ngit clone https://github.com/sosanzma/spade_llm.git\ncd spade_llm\n\n# Create virtual environment\npython -m venv .venv\nsource .venv/bin/activate\n\n# Install development dependencies\npip install -e \".[dev,docs]\"\n\n# Setup pre-commit hooks\npre-commit install\n</code></pre>"},{"location":"contributing/development/#project-structure","title":"Project Structure","text":"<pre><code>spade_llm/\n\u251c\u2500\u2500 spade_llm/          # Main package\n\u2502   \u251c\u2500\u2500 agent/          # Agent implementations\n\u2502   \u251c\u2500\u2500 behaviour/      # Behaviour classes\n\u2502   \u251c\u2500\u2500 context/        # Context management\n\u2502   \u251c\u2500\u2500 providers/      # LLM providers\n\u2502   \u251c\u2500\u2500 tools/          # Tool system\n\u2502   \u251c\u2500\u2500 routing/        # Message routing\n\u2502   \u251c\u2500\u2500 mcp/           # MCP integration\n\u2502   \u2514\u2500\u2500 utils/         # Utilities\n\u251c\u2500\u2500 tests/             # Test suite\n\u251c\u2500\u2500 examples/          # Usage examples\n\u251c\u2500\u2500 docs/             # Documentation\n\u2514\u2500\u2500 requirements*.txt  # Dependencies\n</code></pre>"},{"location":"contributing/development/#testing","title":"Testing","text":""},{"location":"contributing/development/#test-organization","title":"Test Organization","text":"<pre><code>tests/\n\u251c\u2500\u2500 test_agent/        # Agent tests\n\u251c\u2500\u2500 test_behaviour/    # Behaviour tests\n\u251c\u2500\u2500 test_context/      # Context tests\n\u251c\u2500\u2500 test_providers/    # Provider tests\n\u251c\u2500\u2500 test_tools/        # Tool tests\n\u251c\u2500\u2500 test_routing/      # Routing tests\n\u251c\u2500\u2500 conftest.py        # Test configuration\n\u2514\u2500\u2500 factories.py       # Test factories\n</code></pre>"},{"location":"contributing/development/#running-tests","title":"Running Tests","text":"<pre><code># Run all tests\npytest\n\n# Run with coverage\npytest --cov=spade_llm --cov-report=html\n\n# Run specific test module\npytest tests/test_agent/\n\n# Run specific test\npytest tests/test_agent/test_llm_agent.py::test_agent_creation\n\n# Run with verbose output\npytest -v -s\n</code></pre>"},{"location":"contributing/development/#documentation-standards","title":"Documentation Standards","text":""},{"location":"contributing/development/#docstring-format","title":"Docstring Format","text":"<pre><code>def example_function(param1: str, param2: int = 0) -&gt; str:\n    \"\"\"Brief description of the function.\n\n    Longer description if needed. Explain the purpose,\n    behavior, and any important details.\n\n    Args:\n        param1: Description of first parameter\n        param2: Description of second parameter with default value\n\n    Returns:\n        Description of return value\n\n    Raises:\n        ValueError: When invalid input is provided\n        ConnectionError: When service is unavailable\n\n    Example:\n        ```python\n        result = example_function(\"hello\", 42)\n        print(result)  # Output: processed result\n        ```\n    \"\"\"\n    # Implementation here\n    pass\n</code></pre>"},{"location":"contributing/development/#class-documentation","title":"Class Documentation","text":"<pre><code>class ExampleClass:\n    \"\"\"Brief description of the class.\n\n    Longer description explaining the class purpose,\n    usage patterns, and important behavior.\n\n    Attributes:\n        attribute1: Description of attribute\n        attribute2: Description of another attribute\n\n    Example:\n        ```python\n        instance = ExampleClass(param=\"value\")\n        result = instance.method()\n        ```\n    \"\"\"\n\n    def __init__(self, param: str):\n        \"\"\"Initialize the class.\n\n        Args:\n            param: Configuration parameter\n        \"\"\"\n        self.attribute1 = param\n</code></pre> <p>This development guide should help you contribute effectively to SPADE_LLM. For specific questions, check the existing issues or create a new one.</p>"},{"location":"getting-started/","title":"Getting Started","text":"<p>Learn how to install and use SPADE_LLM to create LLM-powered agents.</p>"},{"location":"getting-started/#prerequisites","title":"Prerequisites","text":"<ul> <li>Python 3.10+</li> <li>Basic knowledge of Python async/await</li> <li>XMPP server</li> <li>Access to an LLM provider (OpenAI API key or local model)</li> </ul>"},{"location":"getting-started/#learning-path","title":"Learning Path","text":"<ol> <li>Installation - Install SPADE_LLM and dependencies</li> <li>Quick Start - Create your first agent</li> <li>First Agent - Complete tutorial</li> </ol>"},{"location":"getting-started/#need-help","title":"Need Help?","text":"<ul> <li>Check the API Reference for detailed documentation</li> <li>Browse Examples for working code</li> <li>Open an issue on GitHub</li> </ul>"},{"location":"getting-started/first-agent/","title":"Your First Agent","text":"<p>Step-by-step tutorial for creating a complete LLM-powered agent.</p>"},{"location":"getting-started/first-agent/#prerequisites","title":"Prerequisites","text":"<ul> <li>SPADE_LLM installed</li> <li>OpenAI API key or local model running</li> <li>XMPP server access</li> </ul>"},{"location":"getting-started/first-agent/#step-1-basic-agent","title":"Step 1: Basic Agent","text":"<p>Create <code>my_agent.py</code>:</p> <pre><code>import spade\nfrom spade_llm import LLMAgent, LLMProvider\n\nasync def main():\n    # Configure LLM provider\n    provider = LLMProvider.create_openai(\n        api_key=\"your-api-key\",\n        model=\"gpt-4o-mini\",\n        temperature=0.7\n    )\n\n    # Create agent\n    agent = LLMAgent(\n        jid=\"myagent@jabber.at\", \n        password=\"mypassword\",\n        provider=provider,\n        system_prompt=\"You are a helpful coding assistant\"\n    )\n\n    await agent.start()\n    print(\"Agent started successfully!\")\n\n    # Keep running\n    import asyncio\n    await asyncio.sleep(60)\n\n    await agent.stop()\n    print(\"Agent stopped\")\n\nif __name__ == \"__main__\":\n    spade.run(main())\n</code></pre> <p>Run with: <code>python my_agent.py</code></p>"},{"location":"getting-started/first-agent/#step-2-add-tools","title":"Step 2: Add Tools","text":"<p>Add function calling capabilities:</p> <pre><code>from spade_llm import LLMTool\nfrom datetime import datetime\n\n# Define tool function\nasync def get_current_time() -&gt; str:\n    \"\"\"Get the current date and time.\"\"\"\n    return datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n\n# Create tool\ntime_tool = LLMTool(\n    name=\"get_current_time\",\n    description=\"Get current date and time\",\n    parameters={\"type\": \"object\", \"properties\": {}, \"required\": []},\n    func=get_current_time\n)\n\n# Add to agent\nagent = LLMAgent(\n    jid=\"myagent@jabber.at\",\n    password=\"mypassword\", \n    provider=provider,\n    system_prompt=\"You are a helpful assistant with access to time information\",\n    tools=[time_tool]  # Add tools here\n)\n</code></pre>"},{"location":"getting-started/first-agent/#step-3-interactive-chat","title":"Step 3: Interactive Chat","text":"<p>Create interactive chat interface:</p> <pre><code>from spade_llm import ChatAgent\n\nasync def main():\n    # LLM Agent\n    llm_agent = LLMAgent(\n        jid=\"assistant@jabber.at\",\n        password=\"password1\",\n        provider=provider,\n        system_prompt=\"You are a helpful assistant\"\n    )\n\n    # Chat Agent for human interaction\n    chat_agent = ChatAgent(\n        jid=\"human@jabber.at\",\n        password=\"password2\", \n        target_agent_jid=\"assistant@jabber.at\"\n    )\n\n    await llm_agent.start()\n    await chat_agent.start()\n\n    print(\"Chat system ready! Type messages below.\")\n    print(\"Type 'exit' to quit.\")\n\n    # Start interactive chat\n    await chat_agent.run_interactive()\n\n    await chat_agent.stop()\n    await llm_agent.stop()\n\nif __name__ == \"__main__\":\n    spade.run(main())\n</code></pre>"},{"location":"getting-started/first-agent/#step-4-conversation-management","title":"Step 4: Conversation Management","text":"<p>Add conversation limits and callbacks:</p> <pre><code>def on_conversation_end(conversation_id: str, reason: str):\n    print(f\"Conversation {conversation_id} ended: {reason}\")\n\nagent = LLMAgent(\n    jid=\"assistant@jabber.at\",\n    password=\"password\",\n    provider=provider,\n    system_prompt=\"You are a helpful assistant\",\n    max_interactions_per_conversation=10,\n    termination_markers=[\"&lt;DONE&gt;\", \"goodbye\"],\n    on_conversation_end=on_conversation_end\n)\n</code></pre>"},{"location":"getting-started/first-agent/#step-5-message-routing","title":"Step 5: Message Routing","text":"<p>Route responses to different recipients:</p> <pre><code>def my_router(msg, response, context):\n    \"\"\"Route based on response content.\"\"\"\n    if \"technical\" in response.lower():\n        return \"tech-support@jabber.at\"\n    elif \"sales\" in response.lower():\n        return \"sales@jabber.at\"\n    else:\n        return str(msg.sender)  # Reply to sender\n\nagent = LLMAgent(\n    jid=\"router@jabber.at\",\n    password=\"password\",\n    provider=provider,\n    routing_function=my_router\n)\n</code></pre>"},{"location":"getting-started/first-agent/#complete-example","title":"Complete Example","text":"<p>Here's a full-featured agent combining all concepts:</p> <pre><code>import spade\nfrom spade_llm import LLMAgent, ChatAgent, LLMProvider, LLMTool\nfrom datetime import datetime\n\n# Tool function\nasync def get_time() -&gt; str:\n    \"\"\"Get current time.\"\"\"\n    return datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n\n# Routing function\ndef smart_router(msg, response, context):\n    \"\"\"Route based on content.\"\"\"\n    if \"time\" in response.lower():\n        return \"time-service@jabber.at\"\n    return str(msg.sender)\n\n# Conversation callback\ndef on_end(conv_id: str, reason: str):\n    print(f\"Conversation ended: {conv_id} ({reason})\")\n\nasync def main():\n    # Create provider\n    provider = LLMProvider.create_openai(\n        api_key=\"your-api-key\",\n        model=\"gpt-4o-mini\"\n    )\n\n    # Create tool\n    time_tool = LLMTool(\n        name=\"get_time\",\n        description=\"Get current time\",\n        parameters={\"type\": \"object\", \"properties\": {}, \"required\": []},\n        func=get_time\n    )\n\n    # Create LLM agent\n    llm_agent = LLMAgent(\n        jid=\"smart-assistant@jabber.at\",\n        password=\"password1\",\n        provider=provider,\n        system_prompt=\"You are a smart assistant with time access\",\n        tools=[time_tool],\n        routing_function=smart_router,\n        max_interactions_per_conversation=20,\n        on_conversation_end=on_end\n    )\n\n    # Create chat agent\n    chat_agent = ChatAgent(\n        jid=\"human@jabber.at\",\n        password=\"password2\",\n        target_agent_jid=\"smart-assistant@jabber.at\"\n    )\n\n    # Start agents\n    await llm_agent.start()\n    await chat_agent.start()\n\n    print(\"Smart assistant ready!\")\n    print(\"Try asking: 'What time is it?' or 'Help me with Python'\")\n\n    # Interactive chat\n    await chat_agent.run_interactive()\n\n    # Cleanup\n    await chat_agent.stop()\n    await llm_agent.stop()\n\nif __name__ == \"__main__\":\n    spade.run(main())\n</code></pre>"},{"location":"getting-started/first-agent/#testing-your-agent","title":"Testing Your Agent","text":"<ol> <li>Run the agent: <code>python my_agent.py</code></li> <li>Send test messages using an XMPP client</li> <li>Check agent responses and tool execution</li> <li>Monitor conversation limits and termination</li> </ol>"},{"location":"getting-started/first-agent/#troubleshooting","title":"Troubleshooting","text":"<p>Agent won't start: - Check XMPP credentials - Verify server connectivity - Try <code>verify_security=False</code> for development</p> <p>No LLM responses: - Verify API key - Check provider configuration - Test with simple queries first</p> <p>Tools not working: - Ensure tool functions are async - Check parameter schema - Verify tool registration</p>"},{"location":"getting-started/first-agent/#next-steps","title":"Next Steps","text":"<ul> <li>Tools System - Advanced tool development</li> <li>Message Routing - Complex routing patterns</li> <li>Examples - More complete examples</li> <li>API Reference - Detailed documentation</li> </ul>"},{"location":"getting-started/installation/","title":"Installation","text":""},{"location":"getting-started/installation/#requirements","title":"Requirements","text":"<ul> <li>Python 3.10+</li> <li>4GB+ RAM (8GB+ for local models)</li> </ul>"},{"location":"getting-started/installation/#install","title":"Install","text":"<pre><code>pip install spade_llm\n</code></pre>"},{"location":"getting-started/installation/#verify-installation","title":"Verify Installation","text":"<pre><code>import spade_llm\nfrom spade_llm import LLMAgent, LLMProvider\n\nprint(f\"SPADE_LLM version: {spade_llm.__version__}\")\n</code></pre>"},{"location":"getting-started/installation/#llm-provider-setup","title":"LLM Provider Setup","text":"<p>Choose one provider:</p>"},{"location":"getting-started/installation/#openai","title":"OpenAI","text":"<pre><code>export OPENAI_API_KEY=\"your-api-key\"\n</code></pre>"},{"location":"getting-started/installation/#ollama-local","title":"Ollama (Local)","text":"<pre><code># Install Ollama\ncurl -fsSL https://ollama.ai/install.sh | sh\n\n# Download a model\nollama pull llama3.1:8b\nollama serve\n</code></pre>"},{"location":"getting-started/installation/#lm-studio-local","title":"LM Studio (Local)","text":"<ol> <li>Download LM Studio</li> <li>Download a model through the GUI</li> <li>Start the local server</li> </ol>"},{"location":"getting-started/installation/#xmpp-server","title":"XMPP Server","text":"<p>For development, use a public XMPP server like <code>jabber.at</code> or set up Prosody locally.</p>"},{"location":"getting-started/installation/#development-install","title":"Development Install","text":"<pre><code>git clone https://github.com/sosanzma/spade_llm.git\ncd spade_llm\npip install -e \".[dev]\"\n</code></pre>"},{"location":"getting-started/installation/#troubleshooting","title":"Troubleshooting","text":"<p>Import errors: Ensure you're in the correct Python environment <pre><code>python -m pip install spade_llm\n</code></pre></p> <p>SSL errors: For development only, disable SSL verification: <pre><code>agent = LLMAgent(..., verify_security=False)\n</code></pre></p> <p>Ollama connection: Check if Ollama is running: <pre><code>curl http://localhost:11434/v1/models\n</code></pre></p>"},{"location":"getting-started/quickstart/","title":"Quick Start","text":"<p>Get your first SPADE_LLM agent running in minutes.</p>"},{"location":"getting-started/quickstart/#setup","title":"Setup","text":""},{"location":"getting-started/quickstart/#1-install","title":"1. Install","text":"<pre><code>pip install spade_llm\n</code></pre>"},{"location":"getting-started/quickstart/#2-get-llm-access","title":"2. Get LLM Access","text":"<p>OpenAI (easiest): <pre><code>export OPENAI_API_KEY=\"your-api-key\"\n</code></pre></p> <p>Ollama (free): <pre><code>ollama pull llama3.1:8b\nollama serve\n</code></pre></p>"},{"location":"getting-started/quickstart/#3-create-agent","title":"3. Create Agent","text":"<pre><code>import spade\nfrom spade_llm import LLMAgent, LLMProvider\n\nasync def main():\n    # Configure provider\n    provider = LLMProvider.create_openai(\n        api_key=\"your-api-key\",\n        model=\"gpt-4o-mini\"\n    )\n\n    # Create agent\n    agent = LLMAgent(\n        jid=\"assistant@jabber.at\",\n        password=\"your-password\",\n        provider=provider,\n        system_prompt=\"You are a helpful assistant\"\n    )\n\n    await agent.start()\n    print(\"Agent started!\")\n\n    # Keep running\n    import asyncio\n    await asyncio.sleep(60)\n    await agent.stop()\n\nif __name__ == \"__main__\":\n    spade.run(main())\n</code></pre>"},{"location":"getting-started/quickstart/#4-run","title":"4. Run","text":"<pre><code>python my_agent.py\n</code></pre>"},{"location":"getting-started/quickstart/#alternative-providers","title":"Alternative Providers","text":""},{"location":"getting-started/quickstart/#ollama","title":"Ollama","text":"<pre><code>provider = LLMProvider.create_ollama(\n    model=\"llama3.1:8b\"\n)\n</code></pre>"},{"location":"getting-started/quickstart/#lm-studio","title":"LM Studio","text":"<pre><code>provider = LLMProvider.create_lm_studio(\n    model=\"local-model\",\n    base_url=\"http://localhost:1234/v1\"\n)\n</code></pre>"},{"location":"getting-started/quickstart/#chat-example","title":"Chat Example","text":"<p>Interactive chat agent:</p> <pre><code>import spade\nfrom spade_llm import LLMAgent, ChatAgent, LLMProvider\n\nasync def main():\n    # LLM Agent\n    provider = LLMProvider.create_openai(\n        api_key=\"your-key\",\n        model=\"gpt-4o-mini\"\n    )\n\n    llm_agent = LLMAgent(\n        jid=\"assistant@jabber.at\",\n        password=\"password1\",\n        provider=provider\n    )\n\n    # Chat Agent (for human interaction)\n    chat_agent = ChatAgent(\n        jid=\"human@jabber.at\", \n        password=\"password2\",\n        target_agent_jid=\"assistant@jabber.at\"\n    )\n\n    await llm_agent.start()\n    await chat_agent.start()\n\n    # Start interactive chat\n    await chat_agent.run_interactive()\n\n    await chat_agent.stop()\n    await llm_agent.stop()\n\nif __name__ == \"__main__\":\n    spade.run(main())\n</code></pre>"},{"location":"getting-started/quickstart/#next-steps","title":"Next Steps","text":"<ul> <li>First Agent Tutorial - Detailed walkthrough</li> <li>Providers Guide - LLM provider configuration</li> <li>Tools System - Add function calling</li> <li>Examples - Complete working examples</li> </ul>"},{"location":"guides/","title":"Guides","text":"<p>Comprehensive guides for SPADE_LLM features and concepts.</p>"},{"location":"guides/#core-concepts","title":"Core Concepts","text":"<ul> <li>Architecture - System components and design</li> <li>Providers - LLM provider configuration and usage</li> <li>Tools System - Function calling and tool integration</li> <li>MCP - Model context protocol integration</li> <li>Conversations - Context and conversation management</li> <li>Routing - Message routing and multi-agent workflows</li> </ul>"},{"location":"guides/#usage-patterns","title":"Usage Patterns","text":"<p>Each guide covers: - Core concepts and configuration - Common usage patterns - Best practices and troubleshooting - Complete code examples</p>"},{"location":"guides/#next-steps","title":"Next Steps","text":"<ul> <li>API Reference - Detailed API documentation</li> <li>Examples - Working code examples</li> </ul>"},{"location":"guides/architecture/","title":"Architecture","text":"<p>SPADE_LLM extends SPADE's multi-agent framework with LLM capabilities while maintaining full compatibility.</p>"},{"location":"guides/architecture/#component-overview","title":"Component Overview","text":"<pre><code>graph TB\n    A[LLMAgent] --&gt; B[LLMBehaviour]\n    B --&gt; C[ContextManager]\n    B --&gt; D[LLMProvider]\n    B --&gt; E[LLMTool]\n\n    D --&gt; F[OpenAI/Ollama/etc]\n    E --&gt; G[Python Functions]\n    E --&gt; H[MCP Servers]</code></pre>"},{"location":"guides/architecture/#core-components","title":"\ud83c\udfd7\ufe0f Core Components","text":""},{"location":"guides/architecture/#llmagent","title":"\ud83e\udd16 LLMAgent","text":"<p>The main agent class that extends SPADE's <code>Agent</code> with LLM capabilities:</p> <ul> <li>Manages LLM provider connection and configuration</li> <li>Registers tools and handles their lifecycle</li> <li>Controls conversation limits and termination conditions</li> <li>Provides the bridge between SPADE's XMPP messaging and LLM processing</li> </ul>"},{"location":"guides/architecture/#llmbehaviour","title":"\u26a1 LLMBehaviour","text":"<p>The core processing engine that orchestrates the entire LLM workflow:</p> <ol> <li>Receives XMPP messages from other agents</li> <li>Updates conversation context with new information</li> <li>Calls LLM provider for intelligent responses</li> <li>Executes tools when requested by the LLM</li> <li>Routes responses to appropriate recipients</li> </ol> <p>This is where the magic happens - transforming simple messages into intelligent interactions.</p>"},{"location":"guides/architecture/#contextmanager","title":"\ud83e\udde0 ContextManager","text":"<p>Manages conversation state across multiple concurrent discussions:</p> <ul> <li>Tracks multiple conversations simultaneously by thread ID</li> <li>Formats messages appropriately for different LLM providers</li> <li>Handles context windowing to manage token limits efficiently</li> <li>Ensures each conversation maintains its own context and history</li> </ul>"},{"location":"guides/architecture/#llmprovider","title":"\ud83d\udd0c LLMProvider","text":"<p>Unified interface for different LLM services, providing consistency:</p> <ul> <li>Abstracts provider-specific APIs (OpenAI, Ollama, Anthropic, etc.)</li> <li>Handles tool calling formats across different providers</li> <li>Provides consistent error handling and retry mechanisms</li> <li>Makes it easy to switch between different LLM services</li> </ul>"},{"location":"guides/architecture/#llmtool","title":"\ud83d\udee0\ufe0f LLMTool","text":"<p>Framework for executable functions that extend LLM capabilities:</p> <ul> <li>Async execution support for non-blocking operations</li> <li>JSON Schema parameter validation for type safety</li> <li>Integration with LangChain and MCP for ecosystem compatibility</li> <li>Enables LLMs to perform real actions beyond conversation</li> </ul>"},{"location":"guides/architecture/#message-flow","title":"\ud83d\udce8 Message Flow","text":"<pre><code>sequenceDiagram\n    participant A as External Agent\n    participant B as LLMBehaviour\n    participant C as LLMProvider\n    participant D as LLM Service\n    participant E as LLMTool\n\n    A-&gt;&gt;B: XMPP Message\n    B-&gt;&gt;C: Get Response\n    C-&gt;&gt;D: API Call\n    D-&gt;&gt;C: Tool Calls\n    C-&gt;&gt;B: Tool Requests\n    loop For Each Tool\n        B-&gt;&gt;E: Execute\n        E-&gt;&gt;B: Result\n    end\n    B-&gt;&gt;C: Get Final Response\n    C-&gt;&gt;D: API Call\n    D-&gt;&gt;C: Final Response\n    B-&gt;&gt;A: Response Message</code></pre>"},{"location":"guides/architecture/#conversation-lifecycle","title":"\ud83d\udd04 Conversation Lifecycle","text":"<p>The conversation lifecycle follows a well-defined process:</p> <ol> <li>Initialization: New conversation created from message thread</li> <li>Processing: Messages processed through LLM with tool execution</li> <li>Termination: Ends via markers, limits, or manual control</li> <li>Cleanup: Resources freed and callbacks executed</li> </ol> <p>Each stage is designed to be robust and efficient, ensuring conversations can handle complex, multi-turn interactions while maintaining system stability.</p>"},{"location":"guides/architecture/#integration-points","title":"\ud83d\udd27 Integration Points","text":"<p>The architecture provides multiple integration points for customization:</p> <ul> <li>Custom Providers: Add new LLM services</li> <li>Tool Extensions: Create domain-specific tools</li> <li>Routing Logic: Implement custom message routing</li> <li>Context Management: Customize conversation handling</li> <li>MCP Integration: Connect to external servers</li> </ul> <p>This flexible design ensures SPADE_LLM can adapt to various use cases while maintaining its core multi-agent capabilities.</p>"},{"location":"guides/architecture/#next-steps","title":"Next Steps","text":"<ul> <li>Providers - Configure LLM providers</li> <li>Tools System - Add tool capabilities</li> <li>Routing - Implement message routing</li> <li>MCP - Connect to external services</li> </ul>"},{"location":"guides/conversations/","title":"Conversation Management","text":"<p>SPADE_LLM automatically manages conversation context across multi-turn interactions, enabling intelligent, stateful dialogues between agents.</p>"},{"location":"guides/conversations/#conversation-flow","title":"Conversation Flow","text":"<pre><code>graph TD\n    A[New Message Arrives] --&gt; B{Conversation Exists?}\n    B --&gt;|No| C[Create New Conversation]\n    B --&gt;|Yes| D[Load Existing Context]\n    C --&gt; E[Initialize Context with System Prompt]\n    D --&gt; F[Add Message to Context]\n    E --&gt; F\n    F --&gt; G[Check Interaction Count]\n    G --&gt; H{Max Interactions Reached?}\n    H --&gt;|Yes| I[Mark for Termination]\n    H --&gt;|No| J[Process with LLM]\n    J --&gt; K[Check Response for Termination Markers]\n    K --&gt; L{Termination Marker Found?}\n    L --&gt;|Yes| M[End Conversation]\n    L --&gt;|No| N[Send Response]\n    I --&gt; M\n    M --&gt; O[Execute Cleanup Callbacks]\n    N --&gt; P[Update Context]\n    P --&gt; Q[Continue Conversation]</code></pre>"},{"location":"guides/conversations/#overview","title":"Overview","text":"<p>The conversation system provides comprehensive management for multi-agent interactions:</p> <ul> <li>Multi-turn Context: Maintains complete conversation history across interactions</li> <li>Concurrent Conversations: Supports multiple simultaneous conversations per agent  </li> <li>Automatic Lifecycle: Manages memory and conversation cleanup efficiently</li> <li>Flexible Termination: Controls interaction limits and ending conditions</li> </ul>"},{"location":"guides/conversations/#basic-configuration","title":"Basic Configuration","text":"<p>Conversation management is automatic. Each message thread creates a separate conversation with configurable parameters:</p> <pre><code>from spade_llm import LLMAgent, LLMProvider\n\nagent = LLMAgent(\n    jid=\"assistant@example.com\",\n    password=\"password\",\n    provider=provider,\n    system_prompt=\"You are a helpful assistant\",\n    max_interactions_per_conversation=10,\n    termination_markers=[\"&lt;DONE&gt;\", \"&lt;END&gt;\"]\n)\n</code></pre>"},{"location":"guides/conversations/#conversation-lifecycle","title":"Conversation Lifecycle","text":"<p>The conversation lifecycle follows a structured process:</p> <ol> <li>Initialization: New conversation created from message thread ID</li> <li>Processing: Messages added to context, LLM generates responses</li> <li>Monitoring: System tracks interactions and checks termination conditions</li> <li>Termination: Conversation ends through various mechanisms</li> <li>Cleanup: Resources freed and callbacks executed</li> </ol>"},{"location":"guides/conversations/#lifecycle-states","title":"Lifecycle States","text":"<ul> <li>Active: Conversation is processing messages normally</li> <li>Approaching Limit: Near maximum interaction count</li> <li>Terminated: Conversation has ended and is being cleaned up</li> <li>Cleanup Complete: All resources have been freed</li> </ul>"},{"location":"guides/conversations/#termination-markers","title":"Termination Markers","text":"<p>Termination markers provide a way for LLMs to explicitly signal when a conversation should end. This enables intelligent conversation closure based on content rather than just interaction counts.</p>"},{"location":"guides/conversations/#how-termination-markers-work","title":"How Termination Markers Work","text":"<p>When the LLM includes a termination marker in its response:</p> <ol> <li>Detection: The system scans the LLM response for configured markers</li> <li>Immediate Termination: Conversation is marked for closure regardless of interaction count</li> <li>Response Processing: The marker is typically removed from the final response sent to users</li> <li>Cleanup Execution: Standard termination procedures are triggered</li> </ol>"},{"location":"guides/conversations/#configuration-examples","title":"Configuration Examples","text":"<pre><code># Single termination marker\nagent = LLMAgent(\n    jid=\"assistant@example.com\",\n    password=\"password\",\n    provider=provider,\n    termination_markers=[\"&lt;CONVERSATION_END&gt;\"]\n)\n\n# Multiple termination markers\nagent = LLMAgent(\n    jid=\"assistant@example.com\", \n    password=\"password\",\n    provider=provider,\n    termination_markers=[\"&lt;DONE&gt;\", \"&lt;END&gt;\", \"&lt;GOODBYE&gt;\", \"&lt;TERMINATE&gt;\"]\n)\n</code></pre>"},{"location":"guides/conversations/#llm-integration","title":"LLM Integration","text":"<p>Train your LLM to use termination markers appropriately:</p> <pre><code>system_prompt = \"\"\"\nYou are a helpful assistant. When a conversation naturally comes to an end \nor the user says goodbye, include &lt;DONE&gt; at the end of your response to \nproperly close the conversation.\n\nExample:\nUser: \"Thanks for your help, goodbye!\"\nAssistant: \"You're welcome! Have a great day! &lt;DONE&gt;\"\n\"\"\"\n\nagent = LLMAgent(\n    jid=\"assistant@example.com\",\n    password=\"password\",\n    provider=provider,\n    system_prompt=system_prompt,\n    termination_markers=[\"&lt;DONE&gt;\"]\n)\n</code></pre>"},{"location":"guides/conversations/#termination-marker-best-practices","title":"Termination Marker Best Practices","text":"<ul> <li>Choose Unique Markers: Use markers unlikely to appear in normal conversation</li> <li>Document for LLMs: Clearly instruct the LLM when and how to use markers</li> <li>Multiple Options: Provide several markers for different termination scenarios</li> <li>Consistent Format: Use consistent marker syntax across your system</li> </ul>"},{"location":"guides/conversations/#context-management","title":"Context Management","text":""},{"location":"guides/conversations/#accessing-conversation-state","title":"Accessing Conversation State","text":"<p>Programmatic access to conversation information:</p> <pre><code># Get conversation state\nconversation_id = \"user1_session\"\nstate = agent.get_conversation_state(conversation_id)\n\n# Check conversation status\nis_active = state.get(\"active\", False)\ninteraction_count = state.get(\"interaction_count\", 0)\nmax_interactions = state.get(\"max_interactions\", 10)\n\n# Reset conversation (removes limits)\nsuccess = agent.reset_conversation(conversation_id)\n</code></pre>"},{"location":"guides/conversations/#custom-context-manager","title":"Custom Context Manager","text":"<p>Advanced context control for specialized use cases:</p> <pre><code>from spade_llm.context import ContextManager\n\n# Custom context with specific settings\ncontext = ContextManager(\n    system_prompt=\"You are a specialized coding assistant\",\n    max_tokens=4000,\n    conversation_memory_limit=20  # Keep last 20 exchanges\n)\n\n# Use with agent\nagent = LLMAgent(\n    jid=\"coder@example.com\",\n    password=\"password\",\n    provider=provider,\n    context_manager=context\n)\n</code></pre>"},{"location":"guides/conversations/#conversation-control","title":"Conversation Control","text":""},{"location":"guides/conversations/#termination-callbacks","title":"Termination Callbacks","text":"<p>Handle conversation endings with custom logic:</p> <pre><code>def on_conversation_end(conversation_id: str, reason: str):\n    \"\"\"Called when conversation terminates.\"\"\"\n    print(f\"Conversation {conversation_id} ended: {reason}\")\n\n    # Possible reasons:\n    # - \"max_interactions_reached\"\n    # - \"termination_marker_found\" \n    # - \"manual_termination\"\n\n    # Custom cleanup logic\n    save_conversation_log(conversation_id)\n    send_completion_notification(conversation_id)\n\nagent = LLMAgent(\n    jid=\"assistant@example.com\",\n    password=\"password\", \n    provider=provider,\n    max_interactions_per_conversation=5,\n    on_conversation_end=on_conversation_end\n)\n</code></pre>"},{"location":"guides/conversations/#best-practices","title":"Best Practices","text":""},{"location":"guides/conversations/#context-design","title":"Context Design","text":"<ul> <li>Use clear, focused system prompts that explain the agent's role</li> <li>Set appropriate interaction limits based on expected conversation length</li> <li>Implement proper cleanup callbacks for resource management</li> <li>Handle termination gracefully with user-friendly messages</li> </ul>"},{"location":"guides/conversations/#performance-optimization","title":"Performance Optimization","text":"<ul> <li>Monitor memory usage for long-running conversations</li> <li>Implement context compression when approaching token limits</li> <li>Use conversation limits to prevent runaway interactions</li> <li>Clean up inactive conversations regularly to free resources</li> </ul>"},{"location":"guides/conversations/#termination-strategy","title":"Termination Strategy","text":"<ul> <li>Train LLMs to recognize natural conversation endings</li> <li>Use multiple termination markers for different scenarios</li> <li>Combine markers with interaction limits for robust termination</li> <li>Test termination behavior thoroughly in different conversation contexts</li> </ul>"},{"location":"guides/conversations/#next-steps","title":"Next Steps","text":"<ul> <li>Tools System - Add capabilities to conversations</li> <li>Message Routing - Control conversation flow between agents</li> <li>Architecture - Understanding conversation management internals</li> <li>Providers - Configure LLM providers for conversations</li> </ul>"},{"location":"guides/mcp/","title":"MCP Integration","text":"<p>Model Context Protocol (MCP) enables SPADE_LLM agents to connect to external services and tools through standardized servers.</p>"},{"location":"guides/mcp/#overview","title":"Overview","text":"<p>MCP provides a standard way for AI applications to connect to data sources and tools. SPADE_LLM automatically discovers and adapts MCP tools for use with LLM agents.</p>"},{"location":"guides/mcp/#benefits","title":"Benefits","text":"<ul> <li>Standardized Interface: Consistent API across different services</li> <li>Dynamic Discovery: Automatic tool detection from MCP servers</li> <li>External Services: Connect to databases, APIs, and file systems</li> <li>Tool Caching: Improved performance through tool caching</li> </ul>"},{"location":"guides/mcp/#mcp-server-types","title":"MCP Server Types","text":""},{"location":"guides/mcp/#stdio-servers","title":"STDIO Servers","text":"<p>Communicate via standard input/output streams:</p> <pre><code>from spade_llm.mcp import StdioServerConfig\n\nserver_config = StdioServerConfig(\n    name=\"DatabaseServer\",\n    command=\"python\",\n    args=[\"path/to/database_server.py\"],\n    env={\"DB_URL\": \"sqlite:///data.db\"},\n    cache_tools=True\n)\n</code></pre>"},{"location":"guides/mcp/#sse-servers","title":"SSE Servers","text":"<p>Communicate via Server-Sent Events over HTTP:</p> <pre><code>from spade_llm.mcp import SseServerConfig\n\nserver_config = SseServerConfig(\n    name=\"WebService\",\n    url=\"http://localhost:8080/mcp\",\n    cache_tools=True\n)\n</code></pre>"},{"location":"guides/mcp/#basic-usage","title":"Basic Usage","text":""},{"location":"guides/mcp/#agent-with-mcp-tools","title":"Agent with MCP Tools","text":"<pre><code>import spade\nfrom spade_llm import LLMAgent, LLMProvider\nfrom spade_llm.mcp import StdioServerConfig\n\nasync def main():\n    # Configure MCP server\n    mcp_server = StdioServerConfig(\n        name=\"FileManager\",\n        command=\"python\",\n        args=[\"-m\", \"file_manager_mcp\"],\n        cache_tools=True\n    )\n\n    # Create agent with MCP integration\n    agent = LLMAgent(\n        jid=\"assistant@example.com\",\n        password=\"password\",\n        provider=provider,\n        system_prompt=\"You are a helpful assistant with file management capabilities\",\n        mcp_servers=[mcp_server]\n    )\n\n    await agent.start()\n\nif __name__ == \"__main__\":\n    spade.run(main())\n</code></pre>"},{"location":"guides/mcp/#multiple-mcp-servers","title":"Multiple MCP Servers","text":"<pre><code># Configure multiple servers\nmcp_servers = [\n    StdioServerConfig(\n        name=\"DatabaseService\",\n        command=\"python\",\n        args=[\"database_mcp_server.py\"],\n        env={\"DB_CONNECTION\": \"postgresql://localhost/mydb\"}\n    ),\n    StdioServerConfig(\n        name=\"WeatherService\", \n        command=\"node\",\n        args=[\"weather_mcp_server.js\"],\n        env={\"API_KEY\": \"your-weather-api-key\"}\n    ),\n    SseServerConfig(\n        name=\"CloudStorage\",\n        url=\"http://localhost:9000/mcp\"\n    )\n]\n\nagent = LLMAgent(\n    jid=\"multi-service@example.com\",\n    password=\"password\",\n    provider=provider,\n    mcp_servers=mcp_servers\n)\n</code></pre>"},{"location":"guides/mcp/#creating-mcp-servers","title":"Creating MCP Servers","text":""},{"location":"guides/mcp/#basic-stdio-server","title":"Basic STDIO Server","text":"<p>Create a simple MCP server (<code>math_server.py</code>):</p> <pre><code>#!/usr/bin/env python3\nimport json\nimport sys\nimport math\nfrom typing import Any, Dict\n\nclass MathMCPServer:\n    def __init__(self):\n        self.tools = [\n            {\n                \"name\": \"calculate\",\n                \"description\": \"Perform mathematical calculations\",\n                \"inputSchema\": {\n                    \"type\": \"object\",\n                    \"properties\": {\n                        \"expression\": {\n                            \"type\": \"string\",\n                            \"description\": \"Mathematical expression to evaluate\"\n                        }\n                    },\n                    \"required\": [\"expression\"]\n                }\n            },\n            {\n                \"name\": \"sqrt\",\n                \"description\": \"Calculate square root\",\n                \"inputSchema\": {\n                    \"type\": \"object\", \n                    \"properties\": {\n                        \"number\": {\n                            \"type\": \"number\",\n                            \"description\": \"Number to calculate square root of\"\n                        }\n                    },\n                    \"required\": [\"number\"]\n                }\n            }\n        ]\n\n    def handle_request(self, request: Dict[str, Any]) -&gt; Dict[str, Any]:\n        \"\"\"Handle MCP requests.\"\"\"\n        method = request.get(\"method\")\n\n        if method == \"tools/list\":\n            return {\n                \"jsonrpc\": \"2.0\",\n                \"id\": request.get(\"id\"),\n                \"result\": {\"tools\": self.tools}\n            }\n\n        elif method == \"tools/call\":\n            params = request.get(\"params\", {})\n            tool_name = params.get(\"name\")\n            arguments = params.get(\"arguments\", {})\n\n            if tool_name == \"calculate\":\n                return self._calculate(request.get(\"id\"), arguments)\n            elif tool_name == \"sqrt\":\n                return self._sqrt(request.get(\"id\"), arguments)\n\n            return self._error(request.get(\"id\"), \"Unknown tool\")\n\n        elif method == \"initialize\":\n            return {\n                \"jsonrpc\": \"2.0\",\n                \"id\": request.get(\"id\"),\n                \"result\": {\n                    \"protocolVersion\": \"2024-11-05\",\n                    \"capabilities\": {\n                        \"tools\": {}\n                    },\n                    \"serverInfo\": {\n                        \"name\": \"math-server\",\n                        \"version\": \"1.0.0\"\n                    }\n                }\n            }\n\n        return self._error(request.get(\"id\"), \"Unknown method\")\n\n    def _calculate(self, request_id: str, args: Dict[str, Any]) -&gt; Dict[str, Any]:\n        \"\"\"Handle calculate tool call.\"\"\"\n        try:\n            expression = args.get(\"expression\", \"\")\n            # Safe evaluation (restrict to math operations)\n            result = eval(expression, {\"__builtins__\": {}, \"math\": math})\n\n            return {\n                \"jsonrpc\": \"2.0\",\n                \"id\": request_id,\n                \"result\": {\n                    \"content\": [\n                        {\n                            \"type\": \"text\", \n                            \"text\": f\"Result: {result}\"\n                        }\n                    ]\n                }\n            }\n        except Exception as e:\n            return self._error(request_id, f\"Calculation error: {str(e)}\")\n\n    def _sqrt(self, request_id: str, args: Dict[str, Any]) -&gt; Dict[str, Any]:\n        \"\"\"Handle sqrt tool call.\"\"\"\n        try:\n            number = float(args.get(\"number\", 0))\n            if number &lt; 0:\n                return self._error(request_id, \"Cannot calculate square root of negative number\")\n\n            result = math.sqrt(number)\n            return {\n                \"jsonrpc\": \"2.0\",\n                \"id\": request_id,\n                \"result\": {\n                    \"content\": [\n                        {\n                            \"type\": \"text\",\n                            \"text\": f\"\u221a{number} = {result}\"\n                        }\n                    ]\n                }\n            }\n        except Exception as e:\n            return self._error(request_id, f\"Square root error: {str(e)}\")\n\n    def _error(self, request_id: str, message: str) -&gt; Dict[str, Any]:\n        \"\"\"Return error response.\"\"\"\n        return {\n            \"jsonrpc\": \"2.0\",\n            \"id\": request_id,\n            \"error\": {\n                \"code\": -1,\n                \"message\": message\n            }\n        }\n\n    def run(self):\n        \"\"\"Run the MCP server.\"\"\"\n        for line in sys.stdin:\n            try:\n                request = json.loads(line.strip())\n                response = self.handle_request(request)\n                print(json.dumps(response), flush=True)\n            except Exception as e:\n                error_response = {\n                    \"jsonrpc\": \"2.0\",\n                    \"id\": None,\n                    \"error\": {\n                        \"code\": -32700,\n                        \"message\": f\"Parse error: {str(e)}\"\n                    }\n                }\n                print(json.dumps(error_response), flush=True)\n\nif __name__ == \"__main__\":\n    server = MathMCPServer()\n    server.run()\n</code></pre>"},{"location":"guides/mcp/#using-the-math-server","title":"Using the Math Server","text":"<pre><code># Configure the math MCP server\nmath_server = StdioServerConfig(\n    name=\"MathService\",\n    command=\"python\",\n    args=[\"math_server.py\"],\n    cache_tools=True\n)\n\n# Create agent with math capabilities\nagent = LLMAgent(\n    jid=\"math-assistant@example.com\",\n    password=\"password\",\n    provider=provider,\n    system_prompt=\"You are a math assistant. Use the calculate and sqrt tools for mathematical operations.\",\n    mcp_servers=[math_server]\n)\n\nawait agent.start()\n</code></pre>"},{"location":"guides/mcp/#configuration-options","title":"Configuration Options","text":""},{"location":"guides/mcp/#server-configuration","title":"Server Configuration","text":"<pre><code># STDIO Server with environment variables\nstdio_config = StdioServerConfig(\n    name=\"MyService\",\n    command=\"python\",\n    args=[\"my_mcp_server.py\"],\n    env={\n        \"API_KEY\": \"your-api-key\",\n        \"DB_URL\": \"postgresql://localhost/mydb\",\n        \"LOG_LEVEL\": \"INFO\"\n    },\n    cache_tools=True,\n    working_directory=\"/path/to/server\"\n)\n\n# SSE Server with authentication\nsse_config = SseServerConfig(\n    name=\"WebService\",\n    url=\"https://api.example.com/mcp\",\n    headers={\n        \"Authorization\": \"Bearer your-token\",\n        \"X-API-Version\": \"v1\"\n    },\n    cache_tools=True\n)\n</code></pre>"},{"location":"guides/mcp/#agent-configuration","title":"Agent Configuration","text":"<pre><code>agent = LLMAgent(\n    jid=\"mcp-agent@example.com\",\n    password=\"password\",\n    provider=provider,\n    system_prompt=\"You have access to external services via MCP tools. Use them when needed.\",\n    mcp_servers=[stdio_config, sse_config]\n)\n</code></pre>"},{"location":"guides/mcp/#best-practices","title":"Best Practices","text":""},{"location":"guides/mcp/#server-development","title":"Server Development","text":"<ul> <li>Error Handling: Always return proper error responses</li> <li>Input Validation: Validate all tool parameters</li> <li>Resource Cleanup: Properly close database connections</li> <li>Logging: Include detailed logging for debugging</li> <li>Security: Validate and sanitize all inputs</li> </ul>"},{"location":"guides/mcp/#agent-configuration_1","title":"Agent Configuration","text":"<ul> <li>Tool Caching: Enable caching for better performance</li> <li>Environment Variables: Use env vars for configuration</li> <li>Error Recovery: Handle MCP server failures gracefully</li> <li>Tool Selection: Configure agents with relevant tools only</li> </ul>"},{"location":"guides/mcp/#troubleshooting","title":"Troubleshooting","text":""},{"location":"guides/mcp/#common-issues","title":"Common Issues","text":"<p>Server not starting: - Check command and arguments - Verify working directory - Check environment variables</p> <p>Tool discovery fails: - Test server manually with JSON-RPC calls - Check server implements required methods - Verify JSON-RPC format</p> <p>Tool execution errors: - Check parameter schemas match - Validate input data types - Handle exceptions in tool functions</p> <p>MCP integration provides powerful capabilities for connecting SPADE_LLM agents to external services and data sources. Start with simple STDIO servers and gradually build more complex integrations as needed.</p>"},{"location":"guides/providers/","title":"LLM Providers","text":"<p>SPADE_LLM supports multiple LLM providers through a unified interface, enabling seamless switching between different AI services.</p>"},{"location":"guides/providers/#provider-architecture","title":"Provider Architecture","text":"<pre><code>graph TD\n    A[LLMProvider Interface] --&gt; B[OpenAI Provider]\n    A --&gt; C[Ollama Provider]\n    A --&gt; D[LM Studio Provider]\n    A --&gt; E[vLLM Provider]\n\n    B --&gt; F[GPT-4o]\n    B --&gt; G[GPT-4o-mini]\n    B --&gt; H[GPT-3.5-turbo]\n\n    C --&gt; I[Llama 3.1:8b]\n    C --&gt; J[Mistral:7b]\n    C --&gt; K[CodeLlama:7b]\n\n    D --&gt; L[Local Models]\n    E --&gt; M[High-Performance Inference]</code></pre>"},{"location":"guides/providers/#supported-providers","title":"Supported Providers","text":"<p>The unified LLMProvider interface supports:</p> <ul> <li>OpenAI - GPT models via API for production-ready solutions</li> <li>Ollama - Local open-source models for privacy-focused deployments  </li> <li>LM Studio - Local models with GUI for easy experimentation</li> <li>vLLM - High-performance inference server for scalable applications</li> </ul>"},{"location":"guides/providers/#openai-provider","title":"OpenAI Provider","text":"<p>Cloud-based LLM service with state-of-the-art models:</p> <pre><code>from spade_llm.providers import LLMProvider\n\nprovider = LLMProvider.create_openai(\n    api_key=\"your-api-key\",\n    model=\"gpt-4o-mini\",\n    temperature=0.7\n)\n</code></pre> <p>Popular models: <code>gpt-4o</code>, <code>gpt-4o-mini</code>, <code>gpt-3.5-turbo</code></p> <p>Key advantages: Excellent tool calling, consistent performance, extensive model options.</p>"},{"location":"guides/providers/#ollama-provider","title":"Ollama Provider","text":"<p>Local deployment for privacy and control:</p> <pre><code>provider = LLMProvider.create_ollama(\n    model=\"llama3.1:8b\",\n    base_url=\"http://localhost:11434/v1\"\n)\n</code></pre> <p>Popular models: <code>llama3.1:8b</code>, <code>mistral:7b</code>, <code>codellama:7b</code></p> <p>Tool support: Available with <code>llama3.1:8b</code>, <code>llama3.1:70b</code>, <code>mistral:7b</code></p> <p>Key advantages: Complete privacy, no internet required, cost-effective for high usage.</p>"},{"location":"guides/providers/#lm-studio-provider","title":"LM Studio Provider","text":"<p>Local models with GUI for easy management:</p> <pre><code>provider = LLMProvider.create_lm_studio(\n    model=\"local-model\",\n    base_url=\"http://localhost:1234/v1\"\n)\n</code></pre> <p>The model name should match exactly what's displayed in the LM Studio interface.</p> <p>Key advantages: User-friendly interface, easy model switching, good for experimentation.</p>"},{"location":"guides/providers/#vllm-provider","title":"vLLM Provider","text":"<p>High-performance inference for production deployments:</p> <pre><code>provider = LLMProvider.create_vllm(\n    model=\"meta-llama/Llama-2-7b-chat-hf\",\n    base_url=\"http://localhost:8000/v1\"\n)\n</code></pre> <p>Start vLLM server: <pre><code>python -m vllm.entrypoints.openai.api_server \\\n    --model meta-llama/Llama-2-7b-chat-hf \\\n    --port 8000\n</code></pre></p> <p>Key advantages: Optimized performance, batching support, scalable architecture.</p>"},{"location":"guides/providers/#configuration-options","title":"Configuration Options","text":""},{"location":"guides/providers/#environment-variables","title":"Environment Variables","text":"<p>Centralized configuration using environment variables:</p> <pre><code># .env file\nOPENAI_API_KEY=your-key\nOLLAMA_BASE_URL=http://localhost:11434/v1\nLM_STUDIO_BASE_URL=http://localhost:1234/v1\n</code></pre>"},{"location":"guides/providers/#dynamic-provider-selection","title":"Dynamic Provider Selection","text":"<p>Runtime provider switching based on configuration:</p> <pre><code>import os\n\ndef create_provider():\n    provider_type = os.getenv('LLM_PROVIDER', 'openai')\n\n    if provider_type == 'openai':\n        return LLMProvider.create_openai(\n            api_key=os.getenv('OPENAI_API_KEY'),\n            model=os.getenv('OPENAI_MODEL', 'gpt-4o-mini')\n        )\n    elif provider_type == 'ollama':\n        return LLMProvider.create_ollama(\n            model=os.getenv('OLLAMA_MODEL', 'llama3.1:8b')\n        )\n</code></pre> <p>This approach enables easy deployment across different environments without code changes.</p>"},{"location":"guides/providers/#error-handling","title":"Error Handling","text":"<p>Robust error handling for production reliability:</p> <pre><code>try:\n    response = await provider.get_llm_response(context)\nexcept Exception as e:\n    logger.error(f\"Provider error: {e}\")\n    # Handle fallback or retry logic\n</code></pre>"},{"location":"guides/providers/#provider-fallback-system","title":"Provider Fallback System","text":"<p>Automatic failover for high availability:</p> <pre><code>providers = [\n    LLMProvider.create_openai(api_key=\"key\"),\n    LLMProvider.create_ollama(model=\"llama3.1:8b\")\n]\n\nasync def get_response_with_fallback(context):\n    for provider in providers:\n        try:\n            return await provider.get_llm_response(context)\n        except Exception:\n            continue\n    raise Exception(\"All providers failed\")\n</code></pre> <p>This pattern ensures service continuity even when individual providers experience issues.</p>"},{"location":"guides/providers/#provider-selection-guide","title":"Provider Selection Guide","text":""},{"location":"guides/providers/#cloud-vs-local","title":"Cloud vs Local","text":"<p>Choose OpenAI when: - Need best-in-class performance - Want consistent reliability - Have internet connectivity - Budget allows for API costs</p> <p>Choose Local Providers when: - Privacy is paramount - Want complete control over infrastructure - Have computational resources - Need to minimize ongoing costs</p>"},{"location":"guides/providers/#performance-considerations","title":"Performance Considerations","text":"<p>OpenAI: Fastest response times, excellent reasoning capabilities Ollama: Good performance with smaller models, privacy benefits LM Studio: Easy setup, good for development and testing vLLM: Optimized inference, best for high-throughput applications</p>"},{"location":"guides/providers/#tool-calling-support","title":"Tool Calling Support","text":"<p>Full tool support: OpenAI (all models) Limited tool support: Ollama (specific models only) Experimental: LM Studio and vLLM (model dependent)</p>"},{"location":"guides/providers/#best-practices","title":"Best Practices","text":"<ul> <li>Test multiple providers during development to find the best fit</li> <li>Implement fallback systems for critical applications</li> <li>Use environment variables for easy configuration management</li> <li>Monitor provider performance and costs in production</li> <li>Choose models based on your specific use case requirements</li> </ul>"},{"location":"guides/providers/#next-steps","title":"Next Steps","text":"<ul> <li>Tools System - Add tool capabilities to your providers</li> <li>Architecture - Understanding the provider layer</li> <li>Routing - Route responses based on provider capabilities</li> </ul>"},{"location":"guides/routing/","title":"Message Routing","text":"<p>Route LLM responses to different recipients based on content, context, or custom logic.</p>"},{"location":"guides/routing/#routing-flow","title":"Routing Flow","text":"<pre><code>flowchart TD\n    A[User sends message] --&gt; B[LLM Agent receives message]\n    B --&gt; C[LLM generates response]\n    C --&gt; D[Routing function analyzes response]\n    D --&gt; E{Content analysis}\n    E --&gt;|Contains error| F[Route to Agent A&lt;br/&gt;Support Team]\n    E --&gt;|Contains price| G[Route to Agent B&lt;br/&gt;Sales Team]\n    E --&gt;|Multiple keywords| H[Route to multiple agents&lt;br/&gt;Agent A + Agent B]\n    E --&gt;|No specific keywords| I[Route to default&lt;br/&gt;General Support]\n    F --&gt; J[Message delivered]\n    G --&gt; J\n    H --&gt; J\n    I --&gt; J</code></pre>"},{"location":"guides/routing/#overview","title":"Overview","text":"<p>Message routing enables you to automatically direct LLM responses to appropriate recipients:</p> <ul> <li> <p>Technical issues \u2192 Support team</p> </li> <li> <p>Sales inquiries \u2192 Sales team  </p> </li> <li> <p>General questions \u2192 General support</p> </li> <li> <p>Send to multiple recipients or transform messages before sending</p> </li> </ul>"},{"location":"guides/routing/#basic-routing","title":"Basic Routing","text":""},{"location":"guides/routing/#simple-routing-function","title":"Simple Routing Function","text":"<pre><code>from spade_llm import LLMAgent\n\ndef simple_router(msg, response, context):\n    \"\"\"Route based on response content.\"\"\"\n    if \"error\" in response.lower():\n        return \"support@example.com\"\n    elif \"price\" in response.lower():\n        return \"sales@example.com\"\n    else:\n        return \"general@example.com\"\n\n# Use with agent\nagent = LLMAgent(\n    jid=\"router@example.com\",\n    password=\"password\",\n    provider=provider,\n    routing_function=simple_router\n)\n</code></pre>"},{"location":"guides/routing/#function-signature","title":"Function Signature","text":"<pre><code>def routing_function(msg, response, context):\n    \"\"\"\n    Args:\n        msg: Original SPADE message\n        response: LLM response text  \n        context: Conversation context dict\n\n    Returns:\n        str: Single recipient JID\n        List[str]: Multiple recipients\n        RoutingResponse: Advanced routing\n        None: Send to original sender\n    \"\"\"\n    return \"recipient@example.com\"\n</code></pre>"},{"location":"guides/routing/#advanced-routing","title":"Advanced Routing","text":""},{"location":"guides/routing/#multiple-recipients","title":"Multiple Recipients","text":"<p>Send to several agents simultaneously:</p> <pre><code>def multi_router(msg, response, context):\n    \"\"\"Route to multiple recipients.\"\"\"\n    recipients = [\"primary@example.com\"]\n\n    # Copy errors to support\n    if \"error\" in response.lower():\n        recipients.append(\"support@example.com\")\n\n    # Copy sales inquiries to sales team\n    if \"price\" in response.lower():\n        recipients.append(\"sales@example.com\")\n\n    return recipients\n</code></pre>"},{"location":"guides/routing/#routingresponse","title":"RoutingResponse","text":"<p>For advanced routing with message transformation:</p> <pre><code>from spade_llm.routing import RoutingResponse\n\ndef advanced_router(msg, response, context):\n    \"\"\"Advanced routing with transformations.\"\"\"\n\n    def add_signature(text):\n        return f\"{text}\\n\\n--\\nProcessed by AI Assistant\"\n\n    return RoutingResponse(\n        recipients=\"customer@example.com\",\n        transform=add_signature,\n        metadata={\"processed_by\": \"my_agent\"}\n    )\n</code></pre>"},{"location":"guides/routing/#common-patterns","title":"Common Patterns","text":""},{"location":"guides/routing/#content-based-routing","title":"Content-Based Routing","text":"<pre><code>def content_router(msg, response, context):\n    \"\"\"Route based on keywords in response.\"\"\"\n    text = response.lower()\n\n    # Technical issues\n    if any(word in text for word in [\"error\", \"bug\", \"crash\", \"problem\"]):\n        return \"tech-support@example.com\"\n\n    # Sales inquiries\n    if any(word in text for word in [\"price\", \"cost\", \"buy\", \"purchase\"]):\n        return \"sales@example.com\"\n\n    # Billing questions\n    if any(word in text for word in [\"payment\", \"invoice\", \"billing\"]):\n        return \"billing@example.com\"\n\n    return \"general@example.com\"  # Default\n</code></pre>"},{"location":"guides/routing/#sender-based-routing","title":"Sender-Based Routing","text":"<pre><code>def sender_router(msg, response, context):\n    \"\"\"Route based on message sender.\"\"\"\n    sender = str(msg.sender)\n\n    # VIP users get priority support\n    vip_users = [\"ceo@company.com\", \"admin@company.com\"]\n    if sender in vip_users:\n        return \"vip-support@example.com\"\n\n    # Internal vs external users\n    if sender.endswith(\"@company.com\"):\n        return \"internal@example.com\"\n    else:\n        return \"external@example.com\"\n</code></pre>"},{"location":"guides/routing/#context-aware-routing","title":"Context-Aware Routing","text":"<pre><code>def context_router(msg, response, context):\n    \"\"\"Route based on conversation history.\"\"\"\n    state = context.get(\"state\", {})\n    interaction_count = state.get(\"interaction_count\", 0)\n\n    # Long conversations need escalation\n    if interaction_count &gt; 5:\n        return RoutingResponse(\n            recipients=\"escalation@example.com\",\n            metadata={\"reason\": \"long_conversation\"}\n        )\n\n    # New conversations to onboarding\n    if interaction_count &lt;= 1:\n        return \"onboarding@example.com\"\n\n    return \"standard@example.com\"\n</code></pre>"},{"location":"guides/routing/#workflow-routing","title":"Workflow Routing","text":""},{"location":"guides/routing/#sequential-processing","title":"Sequential Processing","text":"<pre><code>def workflow_router(msg, response, context):\n    \"\"\"Route through workflow steps.\"\"\"\n\n    if \"analysis complete\" in response.lower():\n        return \"review@example.com\"\n    elif \"review approved\" in response.lower():\n        return \"execution@example.com\"\n    elif \"execution finished\" in response.lower():\n        return \"completion@example.com\"\n    else:\n        return \"analysis@example.com\"  # Start workflow\n</code></pre>"},{"location":"guides/routing/#quick-tips","title":"Quick Tips","text":"<ul> <li>Keep logic simple: Complex routing is hard to debug</li> <li>Always have a default: Don't leave messages unrouted</li> <li>Return <code>None</code>: Sends back to original sender (useful for direct conversations)</li> <li>Use metadata: Add context for debugging and tracking</li> </ul> <pre><code>def router_with_fallback(msg, response, context):\n    # Your routing logic here...\n\n    # If no specific rule matches, return to sender\n    return None  # Sends back to original sender\n</code></pre>"},{"location":"guides/routing/#next-steps","title":"Next Steps","text":"<ul> <li>Architecture - Understanding message flow</li> <li>Providers - LLM provider configuration</li> <li>Tools System - Adding tool capabilities</li> </ul>"},{"location":"guides/tools-system/","title":"Tools System","text":"<p>Enable LLM agents to execute functions and interact with external services.</p>"},{"location":"guides/tools-system/#tool-calling-flow","title":"Tool Calling Flow","text":"<pre><code>flowchart TD\n    A[User sends message] --&gt; B{Does LLM need tools?}\n    B --&gt;|No| C[Generate direct response]\n    B --&gt;|Yes| D[Decide which tool to use]\n    D --&gt; E[Determine required arguments]\n    E --&gt; F[Send tool_calls in response]\n    F --&gt; G[System executes tool]\n    G --&gt; H[Add results to context]\n    H --&gt; I[Second LLM query with results]\n    I --&gt; J[Generate final response]\n    C --&gt; K[Send response to user]\n    J --&gt; K</code></pre>"},{"location":"guides/tools-system/#overview","title":"Overview","text":"<p>The Tools System empowers LLM agents to extend beyond conversation by executing real functions. This enables agents to:</p> <ul> <li>\ud83d\udd27 Execute Python functions with dynamic parameters</li> <li>\ud83c\udf10 Access external APIs and databases  </li> <li>\ud83d\udcc1 Process files and perform calculations</li> <li>\ud83d\udd17 Integrate with third-party services</li> </ul>"},{"location":"guides/tools-system/#how-tool-calling-works","title":"How Tool Calling Works","text":"<p>When an LLM agent receives a message, it can either respond directly or decide to use tools. The process involves:</p> <ol> <li>Intelligence Decision: The LLM analyzes if it needs external data or functionality</li> <li>Tool Selection: It chooses the appropriate tool from available options</li> <li>Parameter Generation: The LLM determines what arguments the tool needs</li> <li>Execution: The system runs the tool function asynchronously</li> <li>Context Integration: Results are added back to the conversation</li> <li>Final Response: The LLM processes results and provides a complete answer</li> </ol>"},{"location":"guides/tools-system/#basic-tool-definition","title":"Basic Tool Definition","text":"<pre><code>from spade_llm import LLMTool\n\nasync def get_weather(city: str) -&gt; str:\n    \"\"\"Get weather for a city.\"\"\"\n    return f\"Weather in {city}: 22\u00b0C, sunny\"\n\nweather_tool = LLMTool(\n    name=\"get_weather\",\n    description=\"Get current weather for a city\",\n    parameters={\n        \"type\": \"object\",\n        \"properties\": {\n            \"city\": {\"type\": \"string\", \"description\": \"City name\"}\n        },\n        \"required\": [\"city\"]\n    },\n    func=get_weather\n)\n</code></pre>"},{"location":"guides/tools-system/#using-tools-with-agents","title":"Using Tools with Agents","text":"<pre><code>from spade_llm import LLMAgent, LLMProvider\n\nagent = LLMAgent(\n    jid=\"assistant@example.com\",\n    password=\"password\",\n    provider=provider,\n    tools=[weather_tool]  # Register tools\n)\n</code></pre> <p>When the LLM needs weather information, it will automatically detect the need and call the tool.</p>"},{"location":"guides/tools-system/#common-tool-categories","title":"Common Tool Categories","text":""},{"location":"guides/tools-system/#api-integration","title":"\ud83c\udf10 API Integration","text":"<p>Connect to external web services for real-time data.</p> <pre><code>import aiohttp\n\nasync def web_search(query: str) -&gt; str:\n    \"\"\"Search the web for information.\"\"\"\n    async with aiohttp.ClientSession() as session:\n        async with session.get(f\"https://api.duckduckgo.com/?q={query}&amp;format=json\") as response:\n            data = await response.json()\n            return str(data)\n\nsearch_tool = LLLTool(\n    name=\"web_search\",\n    description=\"Search the web for current information\",\n    parameters={\n        \"type\": \"object\",\n        \"properties\": {\n            \"query\": {\"type\": \"string\"}\n        },\n        \"required\": [\"query\"]\n    },\n    func=web_search\n)\n</code></pre>"},{"location":"guides/tools-system/#file-operations","title":"\ud83d\udcc1 File Operations","text":"<p>Read, write, and process files on the system.</p> <pre><code>import aiofiles\n\nasync def read_file(filepath: str) -&gt; str:\n    \"\"\"Read a text file.\"\"\"\n    try:\n        async with aiofiles.open(filepath, 'r') as f:\n            content = await f.read()\n        return f\"File content:\\n{content}\"\n    except Exception as e:\n        return f\"Error reading file: {e}\"\n\nfile_tool = LLMTool(\n    name=\"read_file\",\n    description=\"Read contents of a text file\",\n    parameters={\n        \"type\": \"object\",\n        \"properties\": {\n            \"filepath\": {\"type\": \"string\"}\n        },\n        \"required\": [\"filepath\"]\n    },\n    func=read_file\n)\n</code></pre>"},{"location":"guides/tools-system/#data-processing","title":"\ud83d\udcca Data Processing","text":"<p>Perform calculations and data analysis.</p> <pre><code>import json\n\nasync def calculate_stats(numbers: list) -&gt; str:\n    \"\"\"Calculate statistics for a list of numbers.\"\"\"\n    if not numbers:\n        return \"Error: No numbers provided\"\n\n    stats = {\n        \"count\": len(numbers),\n        \"mean\": sum(numbers) / len(numbers),\n        \"min\": min(numbers),\n        \"max\": max(numbers)\n    }\n    return json.dumps(stats, indent=2)\n\nstats_tool = LLMTool(\n    name=\"calculate_stats\",\n    description=\"Calculate basic statistics\",\n    parameters={\n        \"type\": \"object\",\n        \"properties\": {\n            \"numbers\": {\n                \"type\": \"array\",\n                \"items\": {\"type\": \"number\"}\n            }\n        },\n        \"required\": [\"numbers\"]\n    },\n    func=calculate_stats\n)\n</code></pre>"},{"location":"guides/tools-system/#langchain-integration","title":"LangChain Integration","text":"<p>Seamlessly use existing LangChain tools with SPADE_LLM:</p> <pre><code>from langchain_community.tools import DuckDuckGoSearchRun\nfrom spade_llm.tools import LangChainToolAdapter\n\n# Create LangChain tool\nsearch_lc = DuckDuckGoSearchRun()\n\n# Adapt for SPADE_LLM  \nsearch_tool = LangChainToolAdapter(search_lc)\n\n# Use with agent\nagent = LLMAgent(\n    jid=\"assistant@example.com\",\n    password=\"password\", \n    provider=provider,\n    tools=[search_tool]\n)\n</code></pre>"},{"location":"guides/tools-system/#best-practices","title":"\u2705 Best Practices","text":"<ul> <li>Single Purpose: Each tool should do one thing well</li> <li>Clear Naming: Use descriptive tool names that explain functionality</li> <li>Rich Descriptions: Help the LLM understand when and how to use tools</li> <li>Input Validation: Always validate and sanitize inputs for security</li> <li>Meaningful Errors: Return clear error messages for troubleshooting</li> <li>Async Functions: Use async/await for non-blocking execution</li> </ul>"},{"location":"guides/tools-system/#next-steps","title":"Next Steps","text":"<ul> <li>MCP Integration - Connect to external MCP servers</li> <li>Architecture - Understanding system design</li> <li>Providers - LLM provider configuration</li> </ul>"},{"location":"includes/mkdocs/","title":"Mkdocs","text":"<p>Production Use</p> <p>This feature is still in beta. While functional, it may change in future versions. Use with caution in production environments.</p> <p>Async Required</p> <p>This method is asynchronous and must be awaited when called from async code.</p> <p>Performance Tip</p> <p>For better performance, consider implementing connection pooling or caching for frequently accessed resources.</p> <p>Basic Example</p> <p>Here's a simple example to get you started:</p> <p>Compatibility</p> <p>This feature requires Python 3.10+ and SPADE 3.3.0+.</p>"},{"location":"reference/","title":"API Reference","text":"<p>Complete API documentation for SPADE_LLM components.</p>"},{"location":"reference/#core-components","title":"Core Components","text":"<ul> <li>Agent - LLMAgent and ChatAgent classes</li> <li>Behaviour - LLMBehaviour implementation  </li> <li>Providers - LLM provider interfaces</li> <li>Tools - Tool system and LLMTool class</li> <li>Context - Context and conversation management</li> <li>Routing - Message routing system</li> </ul>"},{"location":"reference/#quick-reference","title":"Quick Reference","text":""},{"location":"reference/#creating-agents","title":"Creating Agents","text":"<pre><code>from spade_llm import LLMAgent, LLMProvider\n\nprovider = LLMProvider.create_openai(api_key=\"key\", model=\"gpt-4o-mini\")\nagent = LLMAgent(jid=\"agent@server.com\", password=\"pass\", provider=provider)\n</code></pre>"},{"location":"reference/#creating-tools","title":"Creating Tools","text":"<pre><code>from spade_llm import LLMTool\n\nasync def my_function(param: str) -&gt; str:\n    return f\"Result: {param}\"\n\ntool = LLMTool(\n    name=\"my_function\",\n    description=\"Description of function\",\n    parameters={\"type\": \"object\", \"properties\": {\"param\": {\"type\": \"string\"}}, \"required\": [\"param\"]},\n    func=my_function\n)\n</code></pre>"},{"location":"reference/#message-routing","title":"Message Routing","text":"<pre><code>def router(msg, response, context):\n    if \"technical\" in response.lower():\n        return \"tech@example.com\"\n    return str(msg.sender)\n\nagent = LLMAgent(..., routing_function=router)\n</code></pre>"},{"location":"reference/#examples","title":"Examples","text":"<p>See Examples for complete working code examples.</p>"},{"location":"reference/#type-definitions","title":"Type Definitions","text":""},{"location":"reference/#common-types","title":"Common Types","text":"<pre><code># Message context\nContextMessage = Union[SystemMessage, UserMessage, AssistantMessage, ToolResultMessage]\n\n# Routing result\nRoutingResult = Union[str, List[str], RoutingResponse, None]\n\n# Tool parameters\nToolParameters = Dict[str, Any]  # JSON Schema format\n</code></pre>"},{"location":"reference/#error-handling","title":"Error Handling","text":"<p>All SPADE_LLM components use standard Python exceptions:</p> <ul> <li><code>ValueError</code> - Invalid parameters or configuration</li> <li><code>ConnectionError</code> - Network or provider connection issues  </li> <li><code>TimeoutError</code> - Operations that exceed timeout limits</li> <li><code>RuntimeError</code> - General runtime errors</li> </ul>"},{"location":"reference/#configuration","title":"Configuration","text":""},{"location":"reference/#environment-variables","title":"Environment Variables","text":"<pre><code>OPENAI_API_KEY=your-api-key\nOLLAMA_BASE_URL=http://localhost:11434/v1\nLM_STUDIO_BASE_URL=http://localhost:1234/v1\n</code></pre>"},{"location":"reference/#provider-configuration","title":"Provider Configuration","text":"<pre><code># OpenAI\nprovider = LLMProvider.create_openai(api_key=\"key\", model=\"gpt-4o-mini\")\n\n# Ollama  \nprovider = LLMProvider.create_ollama(model=\"llama3.1:8b\")\n\n# LM Studio\nprovider = LLMProvider.create_lm_studio(model=\"local-model\")\n</code></pre> <p>For detailed API documentation, see the individual component pages.</p>"},{"location":"reference/examples/","title":"Examples","text":"<p>Complete working examples for SPADE_LLM applications.</p>"},{"location":"reference/examples/#repository-examples","title":"Repository Examples","text":"<p>The examples directory contains complete working examples:</p> <ul> <li><code>multi_provider_chat_example.py</code> - Chat with different LLM providers</li> <li><code>ollama_with_tools_example.py</code> - Local models with tool calling</li> <li><code>langchain_tools_example.py</code> - LangChain tool integration</li> <li><code>valencia_multiagent_trip_planner.py</code> - Multi-agent workflow</li> <li><code>spanish_to_english_translator.py</code> - Translation agent</li> </ul>"},{"location":"reference/examples/#basic-examples","title":"Basic Examples","text":""},{"location":"reference/examples/#simple-chat-agent","title":"Simple Chat Agent","text":"<pre><code>import spade\nfrom spade_llm import LLMAgent, ChatAgent, LLMProvider\n\nasync def main():\n    # Create LLM provider\n    provider = LLMProvider.create_openai(\n        api_key=\"your-api-key\",\n        model=\"gpt-4o-mini\"\n    )\n\n    # Create LLM agent\n    llm_agent = LLMAgent(\n        jid=\"assistant@jabber.at\",\n        password=\"password1\",\n        provider=provider,\n        system_prompt=\"You are a helpful assistant\"\n    )\n\n    # Create chat interface\n    chat_agent = ChatAgent(\n        jid=\"human@jabber.at\",\n        password=\"password2\",\n        target_agent_jid=\"assistant@jabber.at\"\n    )\n\n    # Start agents\n    await llm_agent.start()\n    await chat_agent.start()\n\n    print(\"Type messages to chat. Enter 'exit' to quit.\")\n    await chat_agent.run_interactive()\n\n    # Cleanup\n    await chat_agent.stop()\n    await llm_agent.stop()\n\nif __name__ == \"__main__\":\n    spade.run(main())\n</code></pre>"},{"location":"reference/examples/#tool-enabled-agent","title":"Tool-Enabled Agent","text":"<pre><code>import spade\nfrom spade_llm import LLMAgent, LLMProvider, LLMTool\nfrom datetime import datetime\nimport requests\n\n# Tool functions\nasync def get_weather(city: str) -&gt; str:\n    \"\"\"Get weather information for a city.\"\"\"\n    # Simplified weather API call\n    try:\n        response = requests.get(f\"http://api.weatherapi.com/v1/current.json?key=YOUR_KEY&amp;q={city}\")\n        data = response.json()\n        return f\"Weather in {city}: {data['current']['temp_c']}\u00b0C, {data['current']['condition']['text']}\"\n    except:\n        return f\"Could not get weather for {city}\"\n\nasync def get_time() -&gt; str:\n    \"\"\"Get current date and time.\"\"\"\n    return datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n\nasync def main():\n    # Create tools\n    weather_tool = LLMTool(\n        name=\"get_weather\",\n        description=\"Get current weather for a city\",\n        parameters={\n            \"type\": \"object\",\n            \"properties\": {\n                \"city\": {\"type\": \"string\", \"description\": \"City name\"}\n            },\n            \"required\": [\"city\"]\n        },\n        func=get_weather\n    )\n\n    time_tool = LLMTool(\n        name=\"get_time\",\n        description=\"Get current date and time\",\n        parameters={\"type\": \"object\", \"properties\": {}, \"required\": []},\n        func=get_time\n    )\n\n    # Create provider\n    provider = LLMProvider.create_openai(\n        api_key=\"your-api-key\",\n        model=\"gpt-4o-mini\"\n    )\n\n    # Create agent with tools\n    agent = LLMAgent(\n        jid=\"assistant@jabber.at\",\n        password=\"password\",\n        provider=provider,\n        system_prompt=\"You are a helpful assistant with access to weather and time information\",\n        tools=[weather_tool, time_tool]\n    )\n\n    await agent.start()\n    print(\"Agent with tools started!\")\n\n    # Keep running\n    import asyncio\n    await asyncio.sleep(60)\n\n    await agent.stop()\n\nif __name__ == \"__main__\":\n    spade.run(main())\n</code></pre>"},{"location":"reference/examples/#multi-agent-workflow","title":"Multi-Agent Workflow","text":"<pre><code>import spade\nfrom spade_llm import LLMAgent, LLMProvider\n\n# Routing functions\ndef analyzer_router(msg, response, context):\n    \"\"\"Route analysis results to reviewer.\"\"\"\n    if \"analysis complete\" in response.lower():\n        return \"reviewer@jabber.at\"\n    return str(msg.sender)\n\ndef reviewer_router(msg, response, context):\n    \"\"\"Route review results to executor.\"\"\"\n    if \"approved\" in response.lower():\n        return \"executor@jabber.at\"\n    elif \"rejected\" in response.lower():\n        return \"analyzer@jabber.at\"  # Send back for revision\n    return str(msg.sender)\n\nasync def main():\n    provider = LLMProvider.create_openai(\n        api_key=\"your-api-key\",\n        model=\"gpt-4o-mini\"\n    )\n\n    # Analyzer agent\n    analyzer = LLMAgent(\n        jid=\"analyzer@jabber.at\",\n        password=\"password1\",\n        provider=provider,\n        system_prompt=\"You analyze requests and provide detailed analysis. End with 'Analysis complete.'\",\n        routing_function=analyzer_router\n    )\n\n    # Reviewer agent  \n    reviewer = LLMAgent(\n        jid=\"reviewer@jabber.at\",\n        password=\"password2\",\n        provider=provider,\n        system_prompt=\"You review analysis and either approve or reject. Say 'Approved' or 'Rejected'.\",\n        routing_function=reviewer_router\n    )\n\n    # Executor agent\n    executor = LLMAgent(\n        jid=\"executor@jabber.at\",\n        password=\"password3\",\n        provider=provider,\n        system_prompt=\"You execute approved plans and report completion.\"\n    )\n\n    # Start all agents\n    await analyzer.start()\n    await reviewer.start() \n    await executor.start()\n\n    print(\"Multi-agent workflow started!\")\n    print(\"Send a request to analyzer@jabber.at to start the workflow\")\n\n    # Keep running\n    import asyncio\n    await asyncio.sleep(120)\n\n    # Cleanup\n    await analyzer.stop()\n    await reviewer.stop()\n    await executor.stop()\n\nif __name__ == \"__main__\":\n    spade.run(main())\n</code></pre>"},{"location":"reference/examples/#local-model-with-ollama","title":"Local Model with Ollama","text":"<pre><code>import spade\nfrom spade_llm import LLMAgent, LLMProvider\n\nasync def main():\n    # Create Ollama provider\n    provider = LLMProvider.create_ollama(\n        model=\"llama3.1:8b\",\n        base_url=\"http://localhost:11434/v1\",\n        temperature=0.7,\n        timeout=120.0\n    )\n\n    # Create agent\n    agent = LLMAgent(\n        jid=\"local-agent@jabber.at\",\n        password=\"password\",\n        provider=provider,\n        system_prompt=\"You are a helpful assistant running on a local model\"\n    )\n\n    await agent.start()\n    print(\"Local Ollama agent started!\")\n\n    # Keep running\n    import asyncio\n    await asyncio.sleep(60)\n\n    await agent.stop()\n\nif __name__ == \"__main__\":\n    spade.run(main())\n</code></pre>"},{"location":"reference/examples/#conversation-management","title":"Conversation Management","text":"<pre><code>import spade\nfrom spade_llm import LLMAgent, LLMProvider\n\ndef conversation_ended(conversation_id: str, reason: str):\n    \"\"\"Handle conversation end.\"\"\"\n    print(f\"Conversation {conversation_id} ended: {reason}\")\n    # Save conversation, send notifications, etc.\n\nasync def main():\n    provider = LLMProvider.create_openai(\n        api_key=\"your-api-key\",\n        model=\"gpt-4o-mini\"\n    )\n\n    agent = LLMAgent(\n        jid=\"managed-agent@jabber.at\",\n        password=\"password\",\n        provider=provider,\n        system_prompt=\"You are a helpful assistant. Say 'DONE' when tasks are complete.\",\n        max_interactions_per_conversation=5,  # Limit conversation length\n        termination_markers=[\"DONE\", \"COMPLETE\", \"FINISHED\"],\n        on_conversation_end=conversation_ended\n    )\n\n    await agent.start()\n    print(\"Agent with conversation management started!\")\n\n    # Test conversation state\n    import asyncio\n    await asyncio.sleep(30)\n\n    # Check conversation states\n    # In a real application, you'd have actual conversation IDs\n    print(\"Active conversations:\", len(agent.context.get_active_conversations()))\n\n    await agent.stop()\n\nif __name__ == \"__main__\":\n    spade.run(main())\n</code></pre>"},{"location":"reference/examples/#integration-examples","title":"Integration Examples","text":""},{"location":"reference/examples/#with-langchain-tools","title":"With LangChain Tools","text":"<pre><code>from langchain_community.tools import DuckDuckGoSearchRun\nfrom spade_llm.tools import LangChainToolAdapter\n\n# Create LangChain tool\nsearch_tool_lc = DuckDuckGoSearchRun()\n\n# Adapt for SPADE_LLM\nsearch_tool = LangChainToolAdapter(search_tool_lc)\n\n# Use with agent\nagent = LLMAgent(\n    jid=\"search-agent@jabber.at\",\n    password=\"password\",\n    provider=provider,\n    system_prompt=\"You are a research assistant with web search capabilities\",\n    tools=[search_tool]\n)\n</code></pre>"},{"location":"reference/examples/#running-examples","title":"Running Examples","text":"<ol> <li>Install dependencies: <code>pip install spade_llm</code></li> <li>Set environment variables: <code>export OPENAI_API_KEY=\"your-key\"</code></li> <li>Run example: <code>python example.py</code></li> </ol>"},{"location":"reference/examples/#common-patterns","title":"Common Patterns","text":""},{"location":"reference/examples/#environment-configuration","title":"Environment Configuration","text":"<pre><code>import os\nfrom spade_llm.utils import load_env_vars\n\n# Load .env file\nload_env_vars()\n\n# Use environment variables\napi_key = os.getenv(\"OPENAI_API_KEY\")\nmodel = os.getenv(\"OPENAI_MODEL\", \"gpt-4o-mini\")\n</code></pre> <p>For more examples, check the examples directory in the SPADE_LLM repository.</p>"},{"location":"reference/api/agent/","title":"Agent API","text":"<p>API reference for SPADE_LLM agent classes.</p>"},{"location":"reference/api/agent/#llmagent","title":"LLMAgent","text":"<p>Main agent class that extends SPADE Agent with LLM capabilities.</p>"},{"location":"reference/api/agent/#constructor","title":"Constructor","text":"<pre><code>LLMAgent(\n    jid: str,\n    password: str,\n    provider: LLMProvider,\n    reply_to: Optional[str] = None,\n    routing_function: Optional[RoutingFunction] = None,\n    system_prompt: Optional[str] = None,\n    mcp_servers: Optional[List[MCPServerConfig]] = None,\n    tools: Optional[List[LLMTool]] = None,\n    termination_markers: Optional[List[str]] = None,\n    max_interactions_per_conversation: Optional[int] = None,\n    on_conversation_end: Optional[Callable[[str, str], None]] = None,\n    verify_security: bool = False\n)\n</code></pre> <p>Parameters:</p> <ul> <li><code>jid</code> - Jabber ID for the agent</li> <li><code>password</code> - Agent password  </li> <li><code>provider</code> - LLM provider instance</li> <li><code>reply_to</code> - Optional fixed reply destination</li> <li><code>routing_function</code> - Custom routing function</li> <li><code>system_prompt</code> - System instructions for LLM</li> <li><code>tools</code> - List of available tools</li> <li><code>termination_markers</code> - Conversation end markers</li> <li><code>max_interactions_per_conversation</code> - Conversation length limit</li> <li><code>on_conversation_end</code> - Callback when conversation ends</li> <li><code>verify_security</code> - Enable SSL verification</li> </ul>"},{"location":"reference/api/agent/#methods","title":"Methods","text":""},{"location":"reference/api/agent/#add_tooltool-llmtool","title":"add_tool(tool: LLMTool)","text":"<p>Add a tool to the agent.</p> <pre><code>tool = LLMTool(name=\"function\", description=\"desc\", parameters={}, func=my_func)\nagent.add_tool(tool)\n</code></pre>"},{"location":"reference/api/agent/#get_tools-listllmtool","title":"get_tools() -&gt; List[LLMTool]","text":"<p>Get all registered tools.</p> <pre><code>tools = agent.get_tools()\nprint(f\"Agent has {len(tools)} tools\")\n</code></pre>"},{"location":"reference/api/agent/#reset_conversationconversation_id-str-bool","title":"reset_conversation(conversation_id: str) -&gt; bool","text":"<p>Reset conversation limits.</p> <pre><code>success = agent.reset_conversation(\"user1_session\")\n</code></pre>"},{"location":"reference/api/agent/#get_conversation_stateconversation_id-str-optionaldictstr-any","title":"get_conversation_state(conversation_id: str) -&gt; Optional[Dict[str, Any]]","text":"<p>Get conversation state information.</p> <pre><code>state = agent.get_conversation_state(\"user1_session\")\nif state:\n    print(f\"Interactions: {state['interaction_count']}\")\n</code></pre>"},{"location":"reference/api/agent/#example","title":"Example","text":"<pre><code>from spade_llm import LLMAgent, LLMProvider\n\nprovider = LLMProvider.create_openai(api_key=\"key\", model=\"gpt-4o-mini\")\n\nagent = LLMAgent(\n    jid=\"assistant@example.com\",\n    password=\"password\",\n    provider=provider,\n    system_prompt=\"You are a helpful assistant\",\n    max_interactions_per_conversation=10\n)\n\nawait agent.start()\n</code></pre>"},{"location":"reference/api/agent/#chatagent","title":"ChatAgent","text":"<p>Interactive chat agent for human-computer communication.</p>"},{"location":"reference/api/agent/#constructor_1","title":"Constructor","text":"<pre><code>ChatAgent(\n    jid: str,\n    password: str,\n    target_agent_jid: str,\n    display_callback: Optional[Callable[[str, str], None]] = None,\n    on_message_sent: Optional[Callable[[str, str], None]] = None,\n    on_message_received: Optional[Callable[[str, str], None]] = None,\n    verbose: bool = False,\n    verify_security: bool = False\n)\n</code></pre> <p>Parameters:</p> <ul> <li><code>target_agent_jid</code> - JID of agent to communicate with</li> <li><code>display_callback</code> - Custom response display function  </li> <li><code>on_message_sent</code> - Callback after sending message</li> <li><code>on_message_received</code> - Callback after receiving response</li> <li><code>verbose</code> - Enable detailed logging</li> </ul>"},{"location":"reference/api/agent/#methods_1","title":"Methods","text":""},{"location":"reference/api/agent/#send_messagemessage-str","title":"send_message(message: str)","text":"<p>Send message to target agent.</p> <pre><code>chat_agent.send_message(\"Hello, how are you?\")\n</code></pre>"},{"location":"reference/api/agent/#send_message_asyncmessage-str","title":"send_message_async(message: str)","text":"<p>Send message asynchronously.</p> <pre><code>await chat_agent.send_message_async(\"Hello!\")\n</code></pre>"},{"location":"reference/api/agent/#wait_for_responsetimeout-float-100-bool","title":"wait_for_response(timeout: float = 10.0) -&gt; bool","text":"<p>Wait for response from target agent.</p> <pre><code>received = await chat_agent.wait_for_response(timeout=30.0)\n</code></pre>"},{"location":"reference/api/agent/#run_interactive","title":"run_interactive()","text":"<p>Start interactive chat session.</p> <pre><code>await chat_agent.run_interactive()  # Starts interactive chat\n</code></pre>"},{"location":"reference/api/agent/#example_1","title":"Example","text":"<pre><code>from spade_llm import ChatAgent\n\ndef display_response(message: str, sender: str):\n    print(f\"Response: {message}\")\n\nchat_agent = ChatAgent(\n    jid=\"human@example.com\",\n    password=\"password\",\n    target_agent_jid=\"assistant@example.com\",\n    display_callback=display_response\n)\n\nawait chat_agent.start()\nawait chat_agent.run_interactive()  # Interactive chat\nawait chat_agent.stop()\n</code></pre>"},{"location":"reference/api/agent/#agent-lifecycle","title":"Agent Lifecycle","text":""},{"location":"reference/api/agent/#starting-agents","title":"Starting Agents","text":"<pre><code>await agent.start()  # Initialize and connect\n</code></pre>"},{"location":"reference/api/agent/#stopping-agents","title":"Stopping Agents","text":"<pre><code>await agent.stop()  # Cleanup and disconnect\n</code></pre>"},{"location":"reference/api/agent/#running-with-spade","title":"Running with SPADE","text":"<pre><code>import spade\n\nasync def main():\n    agent = LLMAgent(...)\n    await agent.start()\n    # Agent runs until stopped\n    await agent.stop()\n\nif __name__ == \"__main__\":\n    spade.run(main())\n</code></pre>"},{"location":"reference/api/agent/#error-handling","title":"Error Handling","text":"<pre><code>try:\n    await agent.start()\nexcept ConnectionError:\n    print(\"Failed to connect to XMPP server\")\nexcept ValueError:\n    print(\"Invalid configuration\")\n</code></pre>"},{"location":"reference/api/agent/#best-practices","title":"Best Practices","text":"<ul> <li>Always call <code>start()</code> before using agents</li> <li>Use <code>stop()</code> for proper cleanup</li> <li>Handle connection errors gracefully</li> <li>Set appropriate conversation limits</li> <li>Use callbacks for monitoring</li> </ul>"},{"location":"reference/api/behaviour/","title":"Behaviour API","text":"<p>API reference for SPADE_LLM behaviour classes.</p>"},{"location":"reference/api/behaviour/#llmbehaviour","title":"LLMBehaviour","text":"<p>Core behaviour that handles LLM interaction loop.</p>"},{"location":"reference/api/behaviour/#constructor","title":"Constructor","text":"<pre><code>LLMBehaviour(\n    llm_provider: LLMProvider,\n    reply_to: Optional[str] = None,\n    routing_function: Optional[RoutingFunction] = None,\n    context_manager: Optional[ContextManager] = None,\n    termination_markers: Optional[List[str]] = None,\n    max_interactions_per_conversation: Optional[int] = None,\n    on_conversation_end: Optional[Callable[[str, str], None]] = None,\n    tools: Optional[List[LLMTool]] = None\n)\n</code></pre> <p>Parameters:</p> <ul> <li><code>llm_provider</code> - LLM provider instance</li> <li><code>reply_to</code> - Fixed reply destination (optional)</li> <li><code>routing_function</code> - Custom routing function</li> <li><code>context_manager</code> - Context manager instance</li> <li><code>termination_markers</code> - Conversation end markers</li> <li><code>max_interactions_per_conversation</code> - Interaction limit</li> <li><code>on_conversation_end</code> - End callback</li> <li><code>tools</code> - Available tools</li> </ul>"},{"location":"reference/api/behaviour/#methods","title":"Methods","text":""},{"location":"reference/api/behaviour/#register_tooltool-llmtool","title":"register_tool(tool: LLMTool)","text":"<p>Register a tool with the behaviour.</p> <pre><code>tool = LLMTool(name=\"func\", description=\"desc\", parameters={}, func=my_func)\nbehaviour.register_tool(tool)\n</code></pre>"},{"location":"reference/api/behaviour/#get_tools-listllmtool","title":"get_tools() -&gt; List[LLMTool]","text":"<p>Get registered tools.</p> <pre><code>tools = behaviour.get_tools()\n</code></pre>"},{"location":"reference/api/behaviour/#reset_conversationconversation_id-str-bool","title":"reset_conversation(conversation_id: str) -&gt; bool","text":"<p>Reset conversation state.</p> <pre><code>success = behaviour.reset_conversation(\"user1_session\")\n</code></pre>"},{"location":"reference/api/behaviour/#get_conversation_stateconversation_id-str-optionaldictstr-any","title":"get_conversation_state(conversation_id: str) -&gt; Optional[Dict[str, Any]]","text":"<p>Get conversation state.</p> <pre><code>state = behaviour.get_conversation_state(\"user1_session\")\n</code></pre>"},{"location":"reference/api/behaviour/#processing-loop","title":"Processing Loop","text":"<p>The behaviour automatically:</p> <ol> <li>Receives XMPP messages</li> <li>Updates conversation context</li> <li>Calls LLM provider</li> <li>Executes requested tools</li> <li>Routes responses</li> </ol>"},{"location":"reference/api/behaviour/#conversation-states","title":"Conversation States","text":"<pre><code>class ConversationState:\n    ACTIVE = \"active\"\n    COMPLETED = \"completed\"\n    ERROR = \"error\"\n    TIMEOUT = \"timeout\"\n    MAX_INTERACTIONS_REACHED = \"max_interactions_reached\"\n</code></pre>"},{"location":"reference/api/behaviour/#example","title":"Example","text":"<pre><code>from spade_llm.behaviour import LLMBehaviour\nfrom spade_llm.context import ContextManager\n\ncontext = ContextManager(system_prompt=\"You are helpful\")\nbehaviour = LLMBehaviour(\n    llm_provider=provider,\n    context_manager=context,\n    termination_markers=[\"DONE\", \"END\"],\n    max_interactions_per_conversation=10\n)\n\n# Used internally by LLMAgent\nagent.add_behaviour(behaviour)\n</code></pre>"},{"location":"reference/api/behaviour/#internal-architecture","title":"Internal Architecture","text":""},{"location":"reference/api/behaviour/#message-processing-flow","title":"Message Processing Flow","text":"<pre><code>async def run(self):\n    \"\"\"Main processing loop.\"\"\"\n    msg = await self.receive(timeout=10)\n    if not msg:\n        return\n\n    # Update context\n    self.context.add_message(msg, conversation_id)\n\n    # Process with LLM\n    await self._process_message_with_llm(msg, conversation_id)\n</code></pre>"},{"location":"reference/api/behaviour/#tool-execution-loop","title":"Tool Execution Loop","text":"<pre><code>async def _process_message_with_llm(self, msg, conversation_id):\n    \"\"\"Process message with tool execution.\"\"\"\n    max_iterations = 20\n    current_iteration = 0\n\n    while current_iteration &lt; max_iterations:\n        response = await self.provider.get_llm_response(self.context, self.tools)\n        tool_calls = response.get('tool_calls', [])\n\n        if not tool_calls:\n            # Final response\n            break\n\n        # Execute tools\n        for tool_call in tool_calls:\n            await self._execute_tool(tool_call)\n\n        current_iteration += 1\n</code></pre>"},{"location":"reference/api/behaviour/#conversation-lifecycle","title":"Conversation Lifecycle","text":"<pre><code>def _end_conversation(self, conversation_id: str, reason: str):\n    \"\"\"End conversation and cleanup.\"\"\"\n    self._active_conversations[conversation_id][\"state\"] = reason\n\n    if self.on_conversation_end:\n        self.on_conversation_end(conversation_id, reason)\n</code></pre>"},{"location":"reference/api/behaviour/#advanced-usage","title":"Advanced Usage","text":""},{"location":"reference/api/behaviour/#custom-behaviour","title":"Custom Behaviour","text":"<pre><code>class CustomLLMBehaviour(LLMBehaviour):\n    async def run(self):\n        \"\"\"Custom processing logic.\"\"\"\n        # Pre-processing\n        await self.custom_preprocessing()\n\n        # Standard processing\n        await super().run()\n\n        # Post-processing\n        await self.custom_postprocessing()\n\n    async def custom_preprocessing(self):\n        \"\"\"Custom preprocessing.\"\"\"\n        pass\n\n    async def custom_postprocessing(self):\n        \"\"\"Custom postprocessing.\"\"\"\n        pass\n</code></pre>"},{"location":"reference/api/behaviour/#direct-usage","title":"Direct Usage","text":"<pre><code># Rarely used directly - usually through LLMAgent\nfrom spade.agent import Agent\nfrom spade.template import Template\n\nclass MyAgent(Agent):\n    async def setup(self):\n        behaviour = LLMBehaviour(llm_provider=provider)\n        template = Template()\n        self.add_behaviour(behaviour, template)\n</code></pre>"},{"location":"reference/api/behaviour/#error-handling","title":"Error Handling","text":"<p>The behaviour handles various error conditions:</p> <ul> <li>Provider Errors: LLM service failures</li> <li>Tool Errors: Tool execution failures  </li> <li>Timeout Errors: Response timeouts</li> <li>Conversation Limits: Max interaction limits</li> </ul> <pre><code>try:\n    await behaviour._process_message_with_llm(msg, conv_id)\nexcept Exception as e:\n    logger.error(f\"Processing error: {e}\")\n    await behaviour._end_conversation(conv_id, ConversationState.ERROR)\n</code></pre>"},{"location":"reference/api/behaviour/#performance-considerations","title":"Performance Considerations","text":"<ul> <li>Tool Iteration Limit: Prevents infinite tool loops</li> <li>Conversation Cleanup: Removes completed conversations</li> <li>Message Deduplication: Prevents duplicate processing</li> <li>Context Management: Efficient memory usage</li> </ul>"},{"location":"reference/api/behaviour/#best-practices","title":"Best Practices","text":"<ul> <li>Let <code>LLMAgent</code> manage behaviour lifecycle</li> <li>Use appropriate termination markers</li> <li>Set reasonable interaction limits</li> <li>Handle conversation end events</li> <li>Monitor conversation states</li> </ul>"},{"location":"reference/api/context/","title":"Context API","text":"<p>API reference for conversation context management.</p>"},{"location":"reference/api/context/#contextmanager","title":"ContextManager","text":"<p>Manages conversation history and context for LLM interactions.</p>"},{"location":"reference/api/context/#constructor","title":"Constructor","text":"<pre><code>ContextManager(\n    max_tokens: int = 4096,\n    system_prompt: Optional[str] = None\n)\n</code></pre> <p>Parameters:</p> <ul> <li><code>max_tokens</code> - Maximum context size in tokens</li> <li><code>system_prompt</code> - System instructions for LLM</li> </ul>"},{"location":"reference/api/context/#methods","title":"Methods","text":""},{"location":"reference/api/context/#add_message","title":"add_message()","text":"<pre><code>def add_message(self, message: Message, conversation_id: str) -&gt; None\n</code></pre> <p>Add SPADE message to conversation context.</p> <p>Example:</p> <pre><code>context = ContextManager(system_prompt=\"You are helpful\")\ncontext.add_message(spade_message, \"user1_session\")\n</code></pre>"},{"location":"reference/api/context/#add_message_dict","title":"add_message_dict()","text":"<pre><code>def add_message_dict(self, message_dict: ContextMessage, conversation_id: str) -&gt; None\n</code></pre> <p>Add message from dictionary format.</p> <p>Example:</p> <pre><code>user_msg = {\"role\": \"user\", \"content\": \"Hello!\"}\ncontext.add_message_dict(user_msg, \"user1_session\")\n</code></pre>"},{"location":"reference/api/context/#add_assistant_message","title":"add_assistant_message()","text":"<pre><code>def add_assistant_message(self, content: str, conversation_id: Optional[str] = None) -&gt; None\n</code></pre> <p>Add assistant response to context.</p> <p>Example:</p> <pre><code>context.add_assistant_message(\"Hello! How can I help?\", \"user1_session\")\n</code></pre>"},{"location":"reference/api/context/#add_tool_result","title":"add_tool_result()","text":"<pre><code>def add_tool_result(\n    self, \n    tool_name: str, \n    result: Any, \n    tool_call_id: str, \n    conversation_id: Optional[str] = None\n) -&gt; None\n</code></pre> <p>Add tool execution result to context.</p> <p>Example:</p> <pre><code>context.add_tool_result(\n    tool_name=\"get_weather\",\n    result=\"22\u00b0C, sunny\",\n    tool_call_id=\"call_123\",\n    conversation_id=\"user1_session\"\n)\n</code></pre>"},{"location":"reference/api/context/#get_prompt","title":"get_prompt()","text":"<pre><code>def get_prompt(self, conversation_id: Optional[str] = None) -&gt; List[ContextMessage]\n</code></pre> <p>Get formatted prompt for LLM provider.</p> <p>Example:</p> <pre><code>prompt = context.get_prompt(\"user1_session\")\n# Returns list of messages formatted for LLM\n</code></pre>"},{"location":"reference/api/context/#get_conversation_history","title":"get_conversation_history()","text":"<pre><code>def get_conversation_history(self, conversation_id: Optional[str] = None) -&gt; List[ContextMessage]\n</code></pre> <p>Get raw conversation history.</p> <p>Example:</p> <pre><code>history = context.get_conversation_history(\"user1_session\")\nprint(f\"Conversation has {len(history)} messages\")\n</code></pre>"},{"location":"reference/api/context/#clear","title":"clear()","text":"<pre><code>def clear(self, conversation_id: Optional[str] = None) -&gt; None\n</code></pre> <p>Clear conversation messages.</p> <p>Example:</p> <pre><code># Clear specific conversation\ncontext.clear(\"user1_session\")\n\n# Clear all conversations\ncontext.clear(\"all\")\n</code></pre>"},{"location":"reference/api/context/#get_active_conversations","title":"get_active_conversations()","text":"<pre><code>def get_active_conversations(self) -&gt; List[str]\n</code></pre> <p>Get list of active conversation IDs.</p> <p>Example:</p> <pre><code>conversations = context.get_active_conversations()\nprint(f\"Active conversations: {conversations}\")\n</code></pre>"},{"location":"reference/api/context/#set_current_conversation","title":"set_current_conversation()","text":"<pre><code>def set_current_conversation(self, conversation_id: str) -&gt; bool\n</code></pre> <p>Set current conversation context.</p> <p>Example:</p> <pre><code>success = context.set_current_conversation(\"user1_session\")\n</code></pre>"},{"location":"reference/api/context/#example-usage","title":"Example Usage","text":"<pre><code>from spade_llm.context import ContextManager\n\n# Create context manager\ncontext = ContextManager(\n    system_prompt=\"You are a helpful coding assistant\",\n    max_tokens=2000\n)\n\n# Add conversation messages\ncontext.add_message_dict(\n    {\"role\": \"user\", \"content\": \"Help me with Python\"}, \n    \"coding_session\"\n)\n\ncontext.add_assistant_message(\n    \"I'd be happy to help with Python!\", \n    \"coding_session\"\n)\n\n# Get formatted prompt\nprompt = context.get_prompt(\"coding_session\")\n# Use with LLM provider\n</code></pre>"},{"location":"reference/api/context/#message-types","title":"Message Types","text":""},{"location":"reference/api/context/#contextmessage-types","title":"ContextMessage Types","text":"<pre><code>from spade_llm.context._types import (\n    SystemMessage,\n    UserMessage, \n    AssistantMessage,\n    ToolResultMessage\n)\n</code></pre>"},{"location":"reference/api/context/#systemmessage","title":"SystemMessage","text":"<pre><code>{\n    \"role\": \"system\",\n    \"content\": \"You are a helpful assistant\"\n}\n</code></pre>"},{"location":"reference/api/context/#usermessage","title":"UserMessage","text":"<pre><code>{\n    \"role\": \"user\",\n    \"content\": \"Hello, how are you?\",\n    \"name\": \"user@example.com\"  # Optional\n}\n</code></pre>"},{"location":"reference/api/context/#assistantmessage","title":"AssistantMessage","text":"<pre><code># Text response\n{\n    \"role\": \"assistant\",\n    \"content\": \"I'm doing well, thank you!\"\n}\n\n# With tool calls\n{\n    \"role\": \"assistant\", \n    \"content\": None,\n    \"tool_calls\": [\n        {\n            \"id\": \"call_123\",\n            \"type\": \"function\",\n            \"function\": {\n                \"name\": \"get_weather\",\n                \"arguments\": \"{\\\"city\\\": \\\"Madrid\\\"}\"\n            }\n        }\n    ]\n}\n</code></pre>"},{"location":"reference/api/context/#toolresultmessage","title":"ToolResultMessage","text":"<pre><code>{\n    \"role\": \"tool\",\n    \"content\": \"Weather in Madrid: 22\u00b0C, sunny\",\n    \"tool_call_id\": \"call_123\"\n}\n</code></pre>"},{"location":"reference/api/providers/","title":"Providers API","text":"<p>API reference for LLM provider classes.</p>"},{"location":"reference/api/providers/#llmprovider","title":"LLMProvider","text":"<p>Unified interface for different LLM services.</p>"},{"location":"reference/api/providers/#class-methods","title":"Class Methods","text":""},{"location":"reference/api/providers/#create_openai","title":"create_openai()","text":"<pre><code>LLMProvider.create_openai(\n    api_key: str,\n    model: str = \"gpt-4o-mini\",\n    temperature: float = 0.7,\n    timeout: Optional[float] = None,\n    max_tokens: Optional[int] = None\n) -&gt; LLMProvider\n</code></pre> <p>Create OpenAI provider.</p> <p>Parameters:</p> <ul> <li><code>api_key</code> - OpenAI API key</li> <li><code>model</code> - Model name (e.g., \"gpt-4o\", \"gpt-4o-mini\")</li> <li><code>temperature</code> - Sampling temperature (0.0-1.0)</li> <li><code>timeout</code> - Request timeout in seconds</li> <li><code>max_tokens</code> - Maximum tokens to generate</li> </ul> <p>Example:</p> <pre><code>provider = LLMProvider.create_openai(\n    api_key=\"sk-...\",\n    model=\"gpt-4o-mini\",\n    temperature=0.7\n)\n</code></pre>"},{"location":"reference/api/providers/#create_ollama","title":"create_ollama()","text":"<pre><code>LLMProvider.create_ollama(\n    model: str = \"llama3.1:8b\",\n    base_url: str = \"http://localhost:11434/v1\",\n    temperature: float = 0.7,\n    timeout: float = 120.0\n) -&gt; LLMProvider\n</code></pre> <p>Create Ollama provider.</p> <p>Parameters:</p> <ul> <li><code>model</code> - Model name (e.g., \"llama3.1:8b\", \"mistral:7b\")</li> <li><code>base_url</code> - Ollama server URL</li> <li><code>temperature</code> - Sampling temperature</li> <li><code>timeout</code> - Request timeout (longer for local models)</li> </ul> <p>Example:</p> <pre><code>provider = LLMProvider.create_ollama(\n    model=\"llama3.1:8b\",\n    temperature=0.8,\n    timeout=180.0\n)\n</code></pre>"},{"location":"reference/api/providers/#create_lm_studio","title":"create_lm_studio()","text":"<pre><code>LLMProvider.create_lm_studio(\n    model: str = \"local-model\",\n    base_url: str = \"http://localhost:1234/v1\",\n    temperature: float = 0.7\n) -&gt; LLMProvider\n</code></pre> <p>Create LM Studio provider.</p> <p>Example:</p> <pre><code>provider = LLMProvider.create_lm_studio(\n    model=\"Meta-Llama-3.1-8B-Instruct\",\n    base_url=\"http://localhost:1234/v1\"\n)\n</code></pre>"},{"location":"reference/api/providers/#create_vllm","title":"create_vllm()","text":"<pre><code>LLMProvider.create_vllm(\n    model: str,\n    base_url: str = \"http://localhost:8000/v1\"\n) -&gt; LLMProvider\n</code></pre> <p>Create vLLM provider.</p> <p>Example:</p> <pre><code>provider = LLMProvider.create_vllm(\n    model=\"meta-llama/Llama-2-7b-chat-hf\",\n    base_url=\"http://localhost:8000/v1\"\n)\n</code></pre>"},{"location":"reference/api/providers/#instance-methods","title":"Instance Methods","text":""},{"location":"reference/api/providers/#get_llm_response","title":"get_llm_response()","text":"<pre><code>async def get_llm_response(\n    self, \n    context: ContextManager, \n    tools: Optional[List[LLMTool]] = None\n) -&gt; Dict[str, Any]\n</code></pre> <p>Get complete response from LLM.</p> <p>Returns:</p> <pre><code>{\n    'text': Optional[str],      # Text response\n    'tool_calls': List[Dict]    # Tool calls requested\n}\n</code></pre> <p>Example:</p> <pre><code>response = await provider.get_llm_response(context, tools)\n\nif response['tool_calls']:\n    # Handle tool calls\n    for call in response['tool_calls']:\n        print(f\"Tool: {call['name']}, Args: {call['arguments']}\")\nelse:\n    # Handle text response\n    print(f\"Response: {response['text']}\")\n</code></pre>"},{"location":"reference/api/providers/#get_response-legacy","title":"get_response() (Legacy)","text":"<pre><code>async def get_response(\n    self, \n    context: ContextManager, \n    tools: Optional[List[LLMTool]] = None\n) -&gt; Optional[str]\n</code></pre> <p>Get text response only.</p> <p>Example:</p> <pre><code>text_response = await provider.get_response(context)\n</code></pre>"},{"location":"reference/api/providers/#get_tool_calls-legacy","title":"get_tool_calls() (Legacy)","text":"<pre><code>async def get_tool_calls(\n    self, \n    context: ContextManager, \n    tools: Optional[List[LLMTool]] = None\n) -&gt; List[Dict[str, Any]]\n</code></pre> <p>Get tool calls only.</p>"},{"location":"reference/api/providers/#baseprovider","title":"BaseProvider","text":"<p>Abstract base class for custom providers.</p> <pre><code>from spade_llm.providers.base_provider import LLMProvider as BaseProvider\n\nclass CustomProvider(BaseProvider):\n    async def get_llm_response(self, context, tools=None):\n        \"\"\"Implement custom LLM integration.\"\"\"\n        # Your implementation\n        return {\n            'text': \"Response from custom provider\",\n            'tool_calls': []\n        }\n</code></pre>"},{"location":"reference/api/providers/#provider-configuration","title":"Provider Configuration","text":""},{"location":"reference/api/providers/#model-formats","title":"Model Formats","text":"<pre><code>class ModelFormat(Enum):\n    OPENAI = \"openai\"    # gpt-4, gpt-3.5-turbo\n    OLLAMA = \"ollama\"    # llama3.1:8b, mistral:7b  \n    CUSTOM = \"custom\"    # custom/model-name\n</code></pre>"},{"location":"reference/api/providers/#environment-variables","title":"Environment Variables","text":"<pre><code># OpenAI\nOPENAI_API_KEY=sk-...\nOPENAI_MODEL=gpt-4o-mini\n\n# Ollama  \nOLLAMA_BASE_URL=http://localhost:11434/v1\nOLLAMA_MODEL=llama3.1:8b\n\n# LM Studio\nLM_STUDIO_BASE_URL=http://localhost:1234/v1\nLM_STUDIO_MODEL=local-model\n</code></pre>"},{"location":"reference/api/providers/#dynamic-configuration","title":"Dynamic Configuration","text":"<pre><code>import os\n\ndef create_provider_from_env():\n    provider_type = os.getenv('LLM_PROVIDER', 'openai')\n\n    if provider_type == 'openai':\n        return LLMProvider.create_openai(\n            api_key=os.getenv('OPENAI_API_KEY'),\n            model=os.getenv('OPENAI_MODEL', 'gpt-4o-mini')\n        )\n    elif provider_type == 'ollama':\n        return LLMProvider.create_ollama(\n            model=os.getenv('OLLAMA_MODEL', 'llama3.1:8b')\n        )\n\nprovider = create_provider_from_env()\n</code></pre>"},{"location":"reference/api/providers/#tool-support","title":"Tool Support","text":""},{"location":"reference/api/providers/#openai-tools","title":"OpenAI Tools","text":"<p>Native tool calling support:</p> <pre><code># Tools automatically formatted for OpenAI\nresponse = await provider.get_llm_response(context, tools)\n</code></pre>"},{"location":"reference/api/providers/#ollama-tools","title":"Ollama Tools","text":"<p>Limited to compatible models:</p> <pre><code># Check model compatibility\ntool_compatible_models = [\n    \"llama3.1:8b\", \"llama3.1:70b\", \"mistral:7b\"\n]\n\nif model in tool_compatible_models:\n    # Use tools\n    response = await provider.get_llm_response(context, tools)\n</code></pre>"},{"location":"reference/api/providers/#error-handling","title":"Error Handling","text":"<pre><code>from openai import OpenAIError\n\ntry:\n    response = await provider.get_llm_response(context)\nexcept OpenAIError as e:\n    print(f\"OpenAI API error: {e}\")\nexcept ConnectionError as e:\n    print(f\"Connection error: {e}\")\nexcept TimeoutError as e:\n    print(f\"Request timeout: {e}\")\n</code></pre>"},{"location":"reference/api/providers/#provider-comparison","title":"Provider Comparison","text":"Feature OpenAI Ollama LM Studio vLLM Setup Easy Medium Easy Hard Quality Excellent Good Good Good Speed Fast Slow Slow Fast Cost Paid Free Free Free Privacy Low High High High Tools Full Limited Limited Limited"},{"location":"reference/api/providers/#best-practices","title":"Best Practices","text":""},{"location":"reference/api/providers/#provider-selection","title":"Provider Selection","text":"<pre><code>def choose_provider(use_case: str):\n    \"\"\"Choose provider based on use case.\"\"\"\n    if use_case == \"development\":\n        return LLMProvider.create_ollama(model=\"llama3.1:1b\")  # Fast\n    elif use_case == \"production\":\n        return LLMProvider.create_openai(model=\"gpt-4o-mini\")   # Reliable\n    elif use_case == \"privacy\":\n        return LLMProvider.create_ollama(model=\"llama3.1:8b\")  # Local\n</code></pre>"},{"location":"reference/api/providers/#error-recovery","title":"Error Recovery","text":"<pre><code>async def robust_llm_call(providers: List[LLMProvider], context):\n    \"\"\"Try multiple providers with fallback.\"\"\"\n    for provider in providers:\n        try:\n            return await provider.get_llm_response(context)\n        except Exception as e:\n            print(f\"Provider failed: {e}\")\n            continue\n\n    raise Exception(\"All providers failed\")\n</code></pre>"},{"location":"reference/api/providers/#performance-monitoring","title":"Performance Monitoring","text":"<pre><code>import time\n\nasync def timed_call(provider, context):\n    \"\"\"Monitor provider performance.\"\"\"\n    start = time.time()\n    try:\n        response = await provider.get_llm_response(context)\n        duration = time.time() - start\n        print(f\"Provider response time: {duration:.2f}s\")\n        return response\n    except Exception as e:\n        duration = time.time() - start\n        print(f\"Provider failed after {duration:.2f}s: {e}\")\n        raise\n</code></pre>"},{"location":"reference/api/routing/","title":"Routing API","text":"<p>API reference for message routing system.</p>"},{"location":"reference/api/routing/#routingfunction","title":"RoutingFunction","text":"<p>Type definition for routing functions.</p> <pre><code>RoutingFunction = Callable[[Message, str, Dict[str, Any]], Union[str, RoutingResponse]]\n</code></pre> <p>Parameters:</p> <ul> <li><code>msg</code> - Original SPADE message</li> <li><code>response</code> - LLM response text</li> <li><code>context</code> - Conversation context and metadata</li> </ul> <p>Returns:</p> <ul> <li><code>str</code> - Single recipient JID</li> <li><code>List[str]</code> - Multiple recipient JIDs  </li> <li><code>RoutingResponse</code> - Advanced routing</li> <li><code>None</code> - Send to original sender</li> </ul>"},{"location":"reference/api/routing/#basic-routing-function","title":"Basic Routing Function","text":"<pre><code>def simple_router(msg, response, context):\n    \"\"\"Route based on response content.\"\"\"\n    if \"technical\" in response.lower():\n        return \"tech-support@example.com\"\n    elif \"billing\" in response.lower():\n        return \"billing@example.com\"\n    else:\n        return str(msg.sender)  # Reply to sender\n</code></pre>"},{"location":"reference/api/routing/#context-information","title":"Context Information","text":"<pre><code>context = {\n    \"conversation_id\": \"user1_agent1\",\n    \"state\": {\n        \"state\": \"active\",\n        \"interaction_count\": 5,\n        \"start_time\": 1642678800.0,\n        \"last_activity\": 1642678900.0\n    }\n}\n</code></pre>"},{"location":"reference/api/routing/#routingresponse","title":"RoutingResponse","text":"<p>Advanced routing with transformations and metadata.</p>"},{"location":"reference/api/routing/#constructor","title":"Constructor","text":"<pre><code>@dataclass\nclass RoutingResponse:\n    recipients: Union[str, List[str]]\n    transform: Optional[Callable[[str], str]] = None\n    metadata: Optional[Dict[str, Any]] = None\n</code></pre> <p>Parameters:</p> <ul> <li><code>recipients</code> - Destination JID(s)</li> <li><code>transform</code> - Function to modify response before sending</li> <li><code>metadata</code> - Additional message metadata</li> </ul>"},{"location":"reference/api/routing/#example","title":"Example","text":"<pre><code>from spade_llm.routing import RoutingResponse\n\ndef advanced_router(msg, response, context):\n    \"\"\"Advanced routing with transformation.\"\"\"\n\n    def add_signature(text):\n        return f\"{text}\\n\\n--\\nProcessed by AI Assistant\"\n\n    if \"urgent\" in response.lower():\n        return RoutingResponse(\n            recipients=\"emergency@example.com\",\n            transform=add_signature,\n            metadata={\n                \"priority\": \"high\",\n                \"category\": \"urgent\",\n                \"original_sender\": str(msg.sender)\n            }\n        )\n\n    return str(msg.sender)\n</code></pre>"},{"location":"reference/api/routing/#routing-patterns","title":"Routing Patterns","text":""},{"location":"reference/api/routing/#content-based-routing","title":"Content-Based Routing","text":"<pre><code>def content_router(msg, response, context):\n    \"\"\"Route based on response keywords.\"\"\"\n    response_lower = response.lower()\n\n    routing_map = {\n        \"tech-support@example.com\": [\"error\", \"bug\", \"technical\", \"debug\"],\n        \"sales@example.com\": [\"price\", \"cost\", \"purchase\", \"buy\"],\n        \"billing@example.com\": [\"payment\", \"invoice\", \"billing\"],\n        \"urgent@example.com\": [\"urgent\", \"emergency\", \"critical\"]\n    }\n\n    for recipient, keywords in routing_map.items():\n        if any(keyword in response_lower for keyword in keywords):\n            return recipient\n\n    return \"general@example.com\"\n</code></pre>"},{"location":"reference/api/routing/#sender-based-routing","title":"Sender-Based Routing","text":"<pre><code>def sender_router(msg, response, context):\n    \"\"\"Route based on message sender.\"\"\"\n    sender = str(msg.sender)\n    sender_domain = sender.split('@')[1]\n\n    # Internal vs external routing\n    if sender_domain == \"company.com\":\n        return \"internal-support@example.com\"\n    else:\n        return \"external-support@example.com\"\n</code></pre>"},{"location":"reference/api/routing/#context-aware-routing","title":"Context-Aware Routing","text":"<pre><code>def context_router(msg, response, context):\n    \"\"\"Route based on conversation context.\"\"\"\n    state = context.get(\"state\", {})\n    interaction_count = state.get(\"interaction_count\", 0)\n\n    # Long conversations need escalation\n    if interaction_count &gt; 10:\n        return RoutingResponse(\n            recipients=\"escalation@example.com\",\n            metadata={\n                \"reason\": \"long_conversation\",\n                \"interaction_count\": interaction_count\n            }\n        )\n\n    return \"standard@example.com\"\n</code></pre>"},{"location":"reference/api/routing/#multi-recipient-routing","title":"Multi-Recipient Routing","text":"<pre><code>def broadcast_router(msg, response, context):\n    \"\"\"Route to multiple recipients.\"\"\"\n    recipients = [\"primary@example.com\"]\n\n    # Add recipients based on content\n    if \"error\" in response.lower():\n        recipients.append(\"monitoring@example.com\")\n\n    if \"sales\" in response.lower():\n        recipients.append(\"sales-team@example.com\")\n\n    return RoutingResponse(\n        recipients=recipients,\n        metadata={\n            \"broadcast\": True,\n            \"primary\": \"primary@example.com\"\n        }\n    )\n</code></pre>"},{"location":"reference/api/routing/#best-practices","title":"Best Practices","text":""},{"location":"reference/api/routing/#routing-design","title":"Routing Design","text":"<ul> <li>Keep logic simple - Complex routing is hard to debug</li> <li>Use meaningful destinations - Clear JID naming</li> <li>Handle edge cases - Provide fallback routing</li> <li>Document routing rules - Clear rule descriptions</li> <li>Test thoroughly - Test all routing paths</li> </ul>"},{"location":"reference/api/tools/","title":"Tools API","text":"<p>API reference for the SPADE_LLM tools system.</p>"},{"location":"reference/api/tools/#llmtool","title":"LLMTool","text":"<p>Core tool class for defining executable functions.</p>"},{"location":"reference/api/tools/#constructor","title":"Constructor","text":"<pre><code>LLMTool(\n    name: str,\n    description: str,\n    parameters: Dict[str, Any],\n    func: Callable[..., Any]\n)\n</code></pre> <p>Parameters:</p> <ul> <li><code>name</code> - Unique tool identifier</li> <li><code>description</code> - Tool description for LLM understanding</li> <li><code>parameters</code> - JSON Schema parameter definition</li> <li><code>func</code> - Python function to execute</li> </ul>"},{"location":"reference/api/tools/#methods","title":"Methods","text":""},{"location":"reference/api/tools/#execute","title":"execute()","text":"<pre><code>async def execute(self, **kwargs) -&gt; Any\n</code></pre> <p>Execute tool with provided arguments.</p> <p>Example:</p> <pre><code>result = await tool.execute(city=\"Madrid\", units=\"celsius\")\n</code></pre>"},{"location":"reference/api/tools/#to_dict","title":"to_dict()","text":"<pre><code>def to_dict(self) -&gt; Dict[str, Any]\n</code></pre> <p>Convert tool to dictionary representation.</p>"},{"location":"reference/api/tools/#to_openai_tool","title":"to_openai_tool()","text":"<pre><code>def to_openai_tool(self) -&gt; Dict[str, Any]\n</code></pre> <p>Convert to OpenAI tool format.</p>"},{"location":"reference/api/tools/#example","title":"Example","text":"<pre><code>from spade_llm import LLMTool\n\nasync def get_weather(city: str, units: str = \"celsius\") -&gt; str:\n    \"\"\"Get weather for a city.\"\"\"\n    return f\"Weather in {city}: 22\u00b0C, sunny\"\n\nweather_tool = LLMTool(\n    name=\"get_weather\",\n    description=\"Get current weather information for a city\",\n    parameters={\n        \"type\": \"object\",\n        \"properties\": {\n            \"city\": {\n                \"type\": \"string\",\n                \"description\": \"Name of the city\"\n            },\n            \"units\": {\n                \"type\": \"string\",\n                \"enum\": [\"celsius\", \"fahrenheit\"],\n                \"default\": \"celsius\"\n            }\n        },\n        \"required\": [\"city\"]\n    },\n    func=get_weather\n)\n</code></pre>"},{"location":"reference/api/tools/#parameter-schema","title":"Parameter Schema","text":"<p>Tools use JSON Schema for parameter validation:</p>"},{"location":"reference/api/tools/#basic-types","title":"Basic Types","text":"<pre><code># String parameter\n\"city\": {\n    \"type\": \"string\",\n    \"description\": \"City name\"\n}\n\n# Number parameter  \n\"temperature\": {\n    \"type\": \"number\",\n    \"minimum\": -100,\n    \"maximum\": 100\n}\n\n# Boolean parameter\n\"include_forecast\": {\n    \"type\": \"boolean\",\n    \"default\": False\n}\n\n# Array parameter\n\"cities\": {\n    \"type\": \"array\",\n    \"items\": {\"type\": \"string\"},\n    \"maxItems\": 10\n}\n</code></pre>"},{"location":"reference/api/tools/#complex-schema","title":"Complex Schema","text":"<pre><code>parameters = {\n    \"type\": \"object\",\n    \"properties\": {\n        \"query\": {\n            \"type\": \"string\",\n            \"description\": \"Search query\"\n        },\n        \"filters\": {\n            \"type\": \"object\",\n            \"properties\": {\n                \"date_range\": {\n                    \"type\": \"object\",\n                    \"properties\": {\n                        \"start\": {\"type\": \"string\", \"format\": \"date\"},\n                        \"end\": {\"type\": \"string\", \"format\": \"date\"}\n                    }\n                },\n                \"category\": {\n                    \"type\": \"string\",\n                    \"enum\": [\"news\", \"blogs\", \"academic\"]\n                }\n            }\n        },\n        \"max_results\": {\n            \"type\": \"integer\",\n            \"minimum\": 1,\n            \"maximum\": 100,\n            \"default\": 10\n        }\n    },\n    \"required\": [\"query\"]\n}\n</code></pre>"},{"location":"reference/api/tools/#langchain-integration","title":"LangChain Integration","text":""},{"location":"reference/api/tools/#langchaintooladapter","title":"LangChainToolAdapter","text":"<pre><code>from spade_llm.tools import LangChainToolAdapter\nfrom langchain_community.tools import DuckDuckGoSearchRun\n\n# Create LangChain tool\nsearch_lc = DuckDuckGoSearchRun()\n\n# Adapt for SPADE_LLM\nsearch_tool = LangChainToolAdapter(search_lc)\n\n# Use with agent\nagent = LLMAgent(\n    jid=\"agent@example.com\",\n    password=\"password\",\n    provider=provider,\n    tools=[search_tool]\n)\n</code></pre>"},{"location":"reference/api/tools/#best-practices","title":"Best Practices","text":""},{"location":"reference/api/tools/#tool-design","title":"Tool Design","text":"<ul> <li>Single Purpose: Each tool should do one thing well</li> <li>Clear Names: Use descriptive tool names</li> <li>Good Descriptions: Help LLM understand when to use tools</li> <li>Validate Inputs: Always validate and sanitize parameters</li> <li>Handle Errors: Return meaningful error messages</li> <li>Use Async: Enable concurrent execution</li> </ul>"}]}